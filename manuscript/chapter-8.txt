# Concurrent Programming Principles

This chapter presents the following concurrent programming principles:

- Threading principle
- Thread safety principle
- Publish/subscribe shared state change principle

## Threading Principle

> ***Modern cloud-native microservices should primarily scale out by adding more processes, not scale up by adding more threads. Use threading only when it is needed or is a good optimization.***

When developing modern cloud-native software, microservices should be stateless and automatically scale
horizontally (scaling out and in via adding and removing processes). The role of threading in modern cloud-native microservices is not as prominent as earlier when software
consisted of monoliths running on bare metal servers, mainly capable of scaling up or down. Nowadays, you should use threading if it is a good optimization or otherwise needed.
Apart from microservices, if you have a library, standalone application, or a client software component, the situation is different, and you can use threading.

Suppose we have a software system with an event-driven architecture. Multiple microservices communicate with each other using asynchronous messaging. Each microservice instance has only a single thread that
consumes messages from a message broker and processes them. If the message broker's message queue for a microservice starts growing too long, the microservice should scale out by adding a new instance.
When the load for the microservice diminishes, it can scale in by removing an instance. There is no need to use threading at all.

We could use threading in the data exporter microservice if the input consumer and the output producer were synchronous. The reason for using threads is optimization. If we had everything in a single thread and the microservice was performing network I/O (either input or output-related), the microservice would have
nothing to execute because it is waiting for some network I/O to complete. Using threads, we can optimize the execution of the microservice so that it potentially has something
to do when waiting for an I/O operation to complete.

Many modern input consumers and output producers are available as asynchronous implementations. If we use an asynchronous consumer and producer in the data exporter microservice, we can eliminate threading because network I/O will not block the execution of the main thread anymore. As a rule of thumb, consider using asynchronous code first, and if it is not possible or feasible, only then consider threading. Many Python libraries exist that support asynchronous operations instead of synchronous ones. Good examples are FastAPI, [databases](https://github.com/encode/databases), and [aiofiles](https://github.com/Tinche/aiofiles/).

{aside}
Python uses the Linux-native asynchronous I/O facility (libaio) or POSIX asynchronous I/O (AIO) interface that allows applications to
initiate I/O operations that are performed
asynchronously (i.e., in the background). The application can
choose how it is notified of the completion of the I/O operation: by delivery of a signal, by instantiation of a
thread, or no notification at all.
{/aside}

You might need a microservice to execute housekeeping tasks on a specific schedule in the background. Instead of using threading and implementing the housekeeping functionality
in the microservice, consider implementing it in a separate microservice to ensure that the *single responsibility principle* is followed.
For example, you can configure the housekeeping microservice to run regularly using a Kubernetes CronJob.

Threading also brings complexity to a microservice because the microservice must ensure thread safety. You will be in big trouble if you forget to implement thread safety.
Threading and synchronization-related bugs are hard to find. Thread safety is a topic that is discussed later in this chapter.
Threading also brings complexity when deploying a microservice because the number of vCPUs requested by the microservice depends on the number of threads used.

### Parallel Executors

Parallel executors simplify concurrency by hiding the creation of multiple sub-processes.
In Python, you can create parallel executors by using multiple processes. Below is an example using a pool of 4 sub-processes:

```python
import multiprocessing
import os


def print_stdout(number: int) -> None:
    print (f'{number} {os.getpid()}')


if __name__ == '__main__':
    numbers = [1, 2, 3, 4]
    pool = multiprocessing.Pool(4)
    pool.map(print_stdout, numbers)
```

The output of the above code could be, for example:

```
1 97672
2 97671
4 97672
3 97670
```

## Thread Safety Principle

> ***If you are using threads, you must ensure thread safety. Thread safety means that only one thread can access shared data simultaneously to avoid race conditions.***

Do not assume thread safety if you use a ready-made data structure or library. You must consult the documentation to see whether thread safety is guaranteed.
If thread safety is not mentioned in the documentation, it can't be assumed. The best way to communicate thread safety to developers is to name things so that
thread safety is explicit. For example, you could create a thread-safe collection library and have a class named `ThreadSafeList` to indicate the class is thread-safe.

The primary way in Python to ensure thread safety is to use a lock. Python does not have atomic variables.

### Use Locking for Mutual Exclusion (Mutex)

Python has a `Lock` class in the `threading` module.
The class implements primitive lock objects to achieve mutual exclusion. Once a thread has acquired a lock,
subsequent attempts to acquire it block until it is released. Any thread can release it.

Let's implement a thread-safe counter using a lock object:

{title: "ThreadSafeCounter.py"}
```python
from threading import Lock

class ThreadSafeCounter:
    def __init__(self):
        self.__lock = Lock()
        self.__counter = 0

    def increment(self) -> None:
        with self.__lock:
            self.__counter += 1

    @property
    def value(self) -> int:
       with self.__lock:
           return self.__counter
```

Python also contains a `Lock` class in the `multiprocessing` module, and it can be used to synchronize multiple sub-processes
in a similar fashion.

### Atomic Variables

Python does not have atomic variables, but you can define your own atomic variable class using locking. Below is
an example of an `AtomicInt` class that uses a lock.

```python
from threading import Lock

class AtomicInt():
    def __init__(self, value: int):
        self.__value = value
        self.__lock = Lock()

    def increment(self, amount: int) -> int:
        with self.__lock:
            self.__value += amount
            return self.__value

    def decrement(self, amount: int) -> int:
        with self.__lock:
            self.__value -= amount
            return self.__value

    @property
    def value(self) -> int:
        with self.__lock:
            return self.__value

    @value.setter
    def value(self, new_value: int):
        with self.__lock:
            self.__value = new_value
```

All the operations on `my_int` below can be done safely from different threads:

```python
my_int = AtomicInt(0)

my_int.increment(1)
my_int.decrement(2)
print(my_int.value) # Prints -1
```

### Concurrent Collections

Multiple threads can use concurrent collections without any additional synchronization.
Below is a partial example of a thread-safe list:

```python
from threading import Lock
from typing import Generic, TypeVar

T = TypeVar('T')


class ThreadSafeList(Generic[T]):
    def __init__(self):
        self.__list: list[T] = []
        self.__lock = Lock()

    def append(self, value: T) -> None:
         with self.__lock:
            self.__list.append(value)

    def pop(self, index: int) -> T:
        with self.__lock:
            return self.__list.pop(index)

    def get(self, index: int) -> T:
        with self.__lock:
            return self.__list[index]

    # Implement rest of wanted methods
```

## Publish/Subscribe Shared State Change Principle

> ***Use condition objects to publish and wait for a change to a shared state.***

Condition objects are useful when you have a queue, and there is a producer and consumer for the queue in different
threads. The producer thread can inform the consumer thread when a new item is placed in the queue, and the consumer
thread waits for an item to be available in the queue. If you did not have a condition object, you would have to implement
this using a lock and a sleep in the consumer. This is not optimal because you don't know the optimal sleep duration. You can sleep too long or too short. Below is an example of using a condition object.

```python
from threading import Condition, Lock
from typing import Final, Generic, TypeVar

T = TypeVar('T')


class ThreadSafeQueue(Generic[T]):
    def __init__(self):
        self.__items: Final[list[T]] = []
        self.__item_waiter: Final = Condition()
        self.__lock: Final = Lock()

    def append(self, item: T) -> None:
        with self.__lock:
            self.__items.append(item)

    def pop_front(self) -> T:
        with self.__lock:
            return self.__items.pop(0)

    def has_item(self) -> bool:
        return len(self.__items) > 0

    @property
    def item_waiter(self):
        return self.__item_waiter


class MsgQueueProducer(Generic[T]):
    def __init__(self, queue: ThreadSafeQueue[T]):
        self.__queue = queue

    def produce(self, item: T) -> None:
        with self.__queue.item_waiter:
            self.__queue.append(item)
            self.__queue.item_waiter.notify()


class MsgQueueConsumer(Generic[T]):
    def __init__(self, queue: ThreadSafeQueue[T]):
        self.__queue = queue

    def consume(self) -> T:
        with self.__queue.item_waiter:
            self.__queue.item_waiter.wait_for(self.__queue.has_item)
            return self.__queue.pop_front()
```