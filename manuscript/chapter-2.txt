# Object-Oriented Design Principles

This chapter describes principles related to object-oriented design. The following principles are discussed:

- Object-oriented programming concepts
- Programming paradigms
- Why is object-oriented programming hard?
- SOLID principles
- Clean microservice design (architecture) principle
- Vertical slice design (architecture) principle
- Class organization principle
- Package, class and function sizing principle
- Uniform naming principle
- Encapsulation principle
- Prefer composition over inheritance principle
- Tactical domain-driven design principle
- Use the design patterns principle
- Tell, don't ask principle
- Law of Demeter
- Avoid primitive type obsession principle
- Dependency injection principle
- Avoid duplication principle

We start the chapter by defining object-oriented programming (OOP) concepts and discussing different programming paradigms: object-oriented, imperative, and functional. We also analyze why OOP can be hard to master even though the concepts and fundamental principles are not so difficult to grasp.

{aside}
In the coming code samples, I don't use the `Final` type hint for local variables, even though most of the local variables I create are constants. I made this decision to improve the readability of the examples. For production code,
I recommend you add the `Final` type hints for local variables that should not be modified. By default, local variables
should not be modified after the initial assignment. You should usually introduce a new variable instead of modifying
an existing one. This rule applies, of course, to the function parameter variables, too. You shouldn't modify them in the function body.

```python
from typing import Final


# Mutable variable
value = get_value()
value = get_another_value()

# Constant
value2: Final = get_value()

# Type checker reports an error
value2 = get_another_value()
```

For all you language designers, variables should be constant by default, and if you want a mutable variable, that
should be denoted by a keyword like `Mutable`. This is how Python should look like:

```python
from typing import Mutable


# Constant
value = get_value()

# Type checker reports an error
value = get_another_value()

# Mutable
value2: Mutable = get_value()
value2 = get_another_value()
```
{/aside}

## Object-Oriented Programming Concepts

The following are the basic concepts related to OOP:

- Classes/Objects
    - Attributes and methods
    - Composition (= when attributes are other classes)
- Encapsulation
- Abstraction
- Inheritance
- Interfaces
    - Interface evolution
- Polymorphism
    - Dynamic dispatch (late binding)

Let's discuss each of the concepts next.

### Classes/Objects

A class is a user-defined data type that acts as the blueprint for individual objects (instances of the class). An object is created
using the class's `__init__` method, which sets the object's initial state. A class consists of *attributes* and *methods*, which can be either *class* or *instance* attributes/methods. Instance attributes define the
state of an object. Instance methods act on instance attributes, i.e., they are used to query and modify the state of an object.
Class attributes belong to the class, and class methods act on class attributes.

An object can represent either a concrete or abstract entity in the real world. For example, a circle and an employee object represent
real-world entities, while an object representing an open file (a file handle) is an abstract entity. Objects can also be hybrid, representing something concrete and abstract.

Attributes of an object can contain other objects to create object hierarchies. This is called *object composition*, handled in more detail in the *prefer composition over inheritance principle* section.

In pure object-oriented languages like Java, you must always create a class where you can put functions. Even if you have only
class methods and no attributes, you must create a class in Java to host the class methods (static methods). In Python, you don't
have to create classes for hosting functions; just put the functions into a single module or create a package (directory) and put each function
in a separate module. Putting functions into classes has many benefits (e.g., dependency injection), which is why putting functions into classes is often a good idea.

### Encapsulation

Encapsulation makes changing the internal state of an object directly outside of the object impossible. The idea of encapsulation
is that the object's state is internal to the object and can be changed externally only by the object's public methods. Encapsulation contributes to better
security and avoidance of data corruption. Unfortunately, the Python language does not support encapsulation, but some conventions can be used to simulate encapsulation. More about that in the *encapsulation principle* section.

### Abstraction

Objects only reveal relevant internal mechanisms to other objects, hiding any unnecessary implementation code.
Callers of the object methods don't need to know the object's internal workings. They adhere only to the public API of the object.
This makes it possible to change the implementation details without affecting any external code.

### Inheritance

Inheritance allows classes to be arranged in a hierarchy representing *is-a* relationships. For example, the `Employee` class might inherit from the `Person` class because an employee is also a person.
All the attributes and methods in the parent (super) class also appear in the child (sub) class with the same names.
For example, class `Person` might define attributes `name` and `birth_date`.
These will also be available in the `Employee` class. Child class can add methods and attributes. Child class can also
override a method in the parent class. For example, the `Employee` might add attributes `employer` and `salary`. This technique allows
easy re-use of the same functionality and data definitions, mirroring real-world relationships intuitively.

Python also supports multiple inheritance, where a child class can have multiple parent classes. The problem with multiple inheritance is that
the child class can inherit different versions of a method with the same name. By default, multiple inheritance should be avoided whenever
possible. Some languages, like Java, don't support multiple inheritance at all. In Python, inheriting from multiple so-called *mixin* classes can
also be problematic because two mixin classes can also have clashing method names. Inheritance will cram additional functionality into a
child class, making the class large and possibly not having a single responsibility. A better way to add functionality to a class is to compose
the class of multiple other classes (the mixins). In that way, there is no need to worry about possible clashing of method names.

Multiple inheritance is always allowed for interfaces. Because Python does not have interfaces, you can use multiple inheritance
with an [abstract base class ](https://docs.python.org/3/library/abc.html)(ABC) or [protocol](https://peps.python.org/pep-0544/) serving as an interface. More about interfaces in the next section.

{aside}
Python 3.12 introduced the `@override` decorator that can be used for methods that are supposed to override a base class
method. A similar kind of keyword or annotation is available in many other languages. Because the examples in this book are written for Python
3.11, I am not using the `@override` decorator, but you should take it into use and specify it for each method that is supposed to override
a base class (or protocol) method. Using the decorator will reveal cases where you accidentally misspell a method name so that it does not override a base class method.
{/aside}

### Interface

An interface specifies a contract that classes that implement the interface must obey. Interfaces are used to implement polymorphic behavior,
which will be described in the next section. An interface consists of one or more methods that classes must implement. Python does not
have interfaces but has *abstract base classes* (ABCs) and *protocols*. Both of these can be used to create an interface.
The ABC syntax is more verbose than protocol syntax because you must always denote a method in an ABC with the `@abstractmethod` decorator.
You cannot instantiate an interface. It is just a contract specification.

Below are two interfaces implemented inheriting from the `ABC` and one class that implements both interfaces:

```python
from abc import ABC, abstractmethod


class Drawable(ABC):
    @abstractmethod
    def draw(self) -> None:
        pass


class Clickable(ABC):
    @abstractmethod
    def click(self) -> None:
        pass


class Button(Drawable, Clickable):
    def draw(self) -> None:
        print("Button drawn")

    def click(self) -> None:
        print("Button clicked")


button = Button()
button.draw()
button.click()

# Output:
# Button drawn
# Button clicked
```

You can also combine the usage of ABCs and protocols. However, sticking to one way of defining interfaces is good practice.
Below is an example where the `Window` class implements two interfaces, one that is defined extending from the `ABC`
(in the above code listing) and one that is defined as extending from the `Protocol`:

```python
from typing import Protocol


class Draggable(Protocol):
    def drag_to(self, x: int, y: int) -> None:
        pass


class Window(Drawable, Draggable):
    def draw(self) -> None:
        print("Window drawn")

    def drag_to(self, x: int, y: int) -> None:
        print(f"Window dragged to ({x}, {y})")


window = Window()
window.draw()
window.drag_to(200, 300)

# Output:
# Window drawn
# Window dragged to (200, 300)
```

{aside}
For the rest of the book, I will interchangeably use the terms *interface* and *protocol*.
{/aside}

#### Interface evolution

After an interface has been defined and is used by the implementing classes, and you would like to add
method(s) to the interface, you might have to provide a default implementation in your interface because the classes that
currently implement your interface don't implement the methods you are about to add to the interface. This is
true in cases where the implementing classes are something you cannot or don't want to modify.

Let's imagine you have a `Message` interface with `get_data` and `get_length_in_bytes` methods, and you have classes
implementing the `Message` interface, but you cannot modify the classes.
You want to add `set_queued_at_instant` and `get_queued_at_instant` methods to the interface.
You can add the methods to the interface but must provide a default implementation, like raising an error
indicating the method is not implemented.

```python
from typing import Protocol


class Message(Protocol):
    def get_data(self) -> bytearray:
        # ...

    def get_length_in_bytes(self) -> int:
        # ...

    def set_queued_at_instant(self, timestamp_in_ms: int) -> None:
        raise NotImplementedError()

    def get_queued_at_instant(self) -> int:
        raise NotImplementedError()
```

### Polymorphism

Polymorphism means that methods are polymorphic when the actual method to be called is decided during the runtime.
For this reason, polymorphism is also called *late binding* (to a particular method) or *dynamic dispatch*. Polymorphic behavior
is easily implemented using an interface variable. You can assign any object that implements the interface to the interface variable.
When you call a method on the interface variable, that actual method to be called is decided based on
what type of object is currently assigned to the interface variable. Below is an example of polymorphic behavior:

```python
drawable: Drawable = Button()
drawable.draw()

# Output:
# Button drawn

drawable = Window()
drawable.draw()

# Output:
# Window drawn
```

Polymorphic behavior is also exhibited when you have a variable of the parent class type and assign a child class object
to the variable, like in the below example:

```python
class IconButton(Button):
    def draw(self) -> None:
        print("Button with icon drawn")


button: Button = Button()
button.draw()

# Output:
# Button drawn

button = IconButton()
button.draw()

# Output:
# Button with icon drawn
```

## Programming Paradigms

The most popular programming languages, including Python, are multi-paradigm programming languages. Multi-paradigm languages
support the following programming paradigms:

- Imperative programming
- Object-oriented programming
- Functional programming

### Imperative Programming

Imperative programming is a programming paradigm that focuses on providing a sequence of explicit instructions or statements
for the computer to follow to solve a problem or achieve a desired outcome. The program consists of a series of commands
that modify the program state, typically using mutable variables and assignments. Imperative programming emphasizes
how to achieve a result step by step, specifying the control flow and state changes explicitly. Typical imperative programming constructs
are variable assignments, state mutations, match-case statements, if-statements, and different kinds of loops (for, while).
Below is a code sample using imperative programming:

```python
from typing import Final

numbers: Final = [1, 2, 3, 4, 5]

doubled_even_numbers: Final = []

for number in numbers:
    if number % 2 == 0:
        doubled_even_numbers.append(number**2)

print(doubled_even_numbers)

# Output:
# [4, 16]
```

In the above example, although the `doubled_even_numbers` variable is declared as `Final`, it is still a mutable list and
we mutate the list inside the `for` loop.

### Functional Programming

Functional programming is a programming paradigm that treats computation as the evaluation of mathematical functions and avoids
changing state and mutable data. It emphasizes the use of immutable data and the composition of functions to solve problems.
In functional programming, functions are first-class citizens, meaning they can be assigned to variables, passed as arguments
to other functions, and returned as results. This enables the creation of higher-order functions and promotes modularity,
code re-usability, and concise expression of complex operations in a declarative way. Functional programming avoids side effects,
favoring [pure functions](https://en.wikipedia.org/wiki/Pure_function) that consistently produce the same output for a given input without causing side effects, making programs easier to reason about and test.

Unlike in imperative programming, in functional programming, you don't tell the computer *how* to do something but declare
*what* you want, e.g., I want to filter even numbers from a sequence and calculate their squares.

{aside}
In mathematics and computer science, a [higher-order function](https://en.wikipedia.org/wiki/Higher-order_function) (HOF)
is a function that does at least one of the following:
    1. Takes one or more functions as arguments
    2. Returns a function as its result.
{/aside}

Below is functional code that uses Python's list comprehension:

```python
from typing import Final

numbers: Final = [1, 2, 3, 4, 5]

print([number**2 for number in numbers if number %2 == 0])

# Output:
# [4, 16]
```

As you can see, the above code is much safer, shorter, and more straightforward than the earlier imperative code. There are no variable assignments or state modifications.

Let's implement the above code using `map` and `filter` functions:

```python
from typing import Final

numbers: Final = [1, 2, 3, 4, 5]

is_even: Final = lambda number: number % 2 == 0
doubled: Final = lambda number: number**2

print(list(map(doubled, filter(is_even, numbers))))

# Output:
# [4, 16]
```

In the above example, we assigned a *lambda* (a single statement anonymous function) to a variable. This practice is not according to PEP 8. We should use
`def` to define a function instead:

```python
from typing import Final

numbers: Final = [1, 2, 3, 4, 5]

def is_even(number: int):
    return number % 2 == 0

def doubled(number: int):
    return number**2

print(list(map(doubled, filter(is_even, numbers))))

# Output:
# [4, 16]
```

Both the `is_even` and `doubled` are pure functions because they return the same output for the same input without
any side effects.
The above expression could be easier to read. Let's use a variable (= constant) to store an intermediate value:

```python
even_numbers: Final = filter(is_even, numbers)
print(list(map(doubled, even_numbers)))

# Output:
# [4, 16]
```

There is another way to implement the above code using a composition of functions. We can define reusable functions and
compose more specific ones from general-purpose ones. Below is an example of function composition using the
`compose` function from the [toolz](https://toolz.readthedocs.io/en/latest/api.html) library. The example also uses the `partial`
function from the `functools` module to create
partially applied functions. For example, the `filter_even` function is a partially applied `filter` function where
the first parameter is bound to the `is_even` function. Similarly, the `map_doubled` function is a partially
applied `map` function where the first parameter is bound to the `doubled` function. The `compose` function composes two
or more functions in the following way: `compose(f, g)(x)` is the same as `f(g(x))` and `compose(f, g, h)(x)` is same as `f(g(h(x)))`
and so on. You can compose as many functions as you need/want.

```python
from functools import partial
from typing import Final

from toolz import compose

numbers: Final = [1, 2, 3, 4, 5]


def is_even(number: int):
    return number % 2 == 0


def doubled(number: int):
    return number**2


filter_even: Final = partial(filter, is_even)
map_doubled: Final = partial(map, doubled)
doubled_even: Final = compose(list, map_doubled, filter_even)
print(doubled_even(numbers))

# Output:
# [4, 16]
```

In the above example, all the following functions can be made re-usable and put into a library:

- `is_even`
- `doubled`
- `filter_even`
- `map_doubled`

Modern code should favor functional programming over imperative programming when possible. As compared to functional programming,
imperative programming comes with the following disadvantages:

1. *Mutable state*: Imperative programming relies heavily on mutable state, where variables can be modified throughout the program's execution. This can lead to subtle bugs and make the program harder to reason about, as the state can change unpredictably. In functional programming, immutability is emphasized, reducing the complexity of state management and making programs more reliable.
2. *Side effects*: Imperative programming often involves side effects, where functions or operations modify the state or interact with the external world. Side effects make the code harder to test, reason about, and debug. On the other hand, functional programming encourages pure functions with no side effects, making the code more modular, reusable, and testable.
3. *Concurrency and parallelism*: Imperative programming can be challenging to parallelize and reason about in concurrent scenarios. Since mutable state can be modified by multiple threads or processes, race conditions and synchronization issues can occur. Functional programming, with its emphasis on immutability and pure functions, simplifies concurrency and parallelism by eliminating shared mutable state.
4. *Lack of referential transparency*: Imperative programming tends to rely on assignments and statements that modify variables in place. This can lead to code that is difficult to reason about due to implicit dependencies and hidden interactions between different parts of the code. In functional programming, [referential transparency](https://en.wikipedia.org/wiki/Referential_transparency) is a key principle where expressions can be replaced with their values without changing the program's behavior. This property allows for easier understanding, debugging, and optimization.

Pure imperative programming also quickly leads to code duplication, lack of modularity, and abstraction issues. These are issues
that can be solved using object-oriented programming.

## Multi-Paradigm Programming Principle

> ***You should not use a single programming paradigm only.***

To best utilize both object-oriented and functional programming (FP) when developing software, you can
leverage the strengths of each paradigm in different parts of your codebase. Use domain-driven design (DDD) and object-oriented design to design
the application: interfaces and classes. Implement classes by encapsulating related behavior and (possibly mutable, but aim for immutable) state in the classes.
Apply OOP principles like *SOLID principles* and  *design patterns*. These principles and patterns make code modular and
easily extensible without accidentally breaking existing code. Use FP as much as possible when implementing class and instance methods.
Embrace functional composition by creating pure functions that take immutable data as input and always produce the same output for the same input without side effects.
Use higher-order functions to compose functions and build complex operations from simpler ones. For example, utilize higher-order functions in OOP by passing functions as arguments to methods
or using them as callbacks. This allows for greater flexibility and modularity, enabling functional-style operations within an OOP framework.
Also, remember to use functional programming libraries, either the standard or 3rd party libraries. Consider using functional techniques for error handling,
such as *Either* or *Maybe/Optional* types. This helps you manage errors without exceptions, promoting more predictable and robust code. This is
because function signatures don't tell if they can raise an error. You must remember to consult the documentation and check if a function can raise an error.

Aim for immutability within your codebase, regardless of the paradigm. Immutable data reduces complexity, avoids shared mutable state,
and facilitates reasoning about your code. Favor creating new objects or data structures instead of modifying existing ones.

## Why is Object-Oriented Programming Hard?

The basic concepts of OOP are not complex to understand, so why is it hard to master OOP?
Below are listed things that can make OOP hard:

- You cannot rush into coding. You must have patience and perform object-oriented design (OOD) first
- You cannot get the OOD right on the first try. You need to have discipline and time reserved for refactoring.
- The difference between object composition and inheritance is not correctly understood, and inheritance is used in place of object composition, making the OOD flawed
- SOLID principles are not understood or followed
    - It can be challenging to create optimal-sized classes and functions with a single responsibility
        - For example, you might have a single-responsibility class, but the class is too big. You must realize that you must split the class into smaller classes the original class is composed of. Each of these smaller classes has a single responsibility on a lower level of abstraction compared to the original class
    - Understanding and following the open-closed principle can be challenging
      - The idea of the open-closed principle is to avoid modifying existing code and thus avoid breaking any existing working code. For example, If you have a collection class and need a thread-safe collection class, don't modify the existing one, e.g., by adding a constructor flag to tell if a collection should be thread-safe. Instead, create a totally new class for thread-safe collections.
    - Liskov's substitution principle is not as simple as it looks
      - For example, suppose you have a base class `Circle` with a `draw` method. If you derive a `FilledCircle` class from the `Circle` class, you must implement the `draw` function so that it first calls the base class method. In some cases, it is possible to override the base class method with the derived class method
    - Interface segregation is usually left undone if it is not immediately needed. This might hinder the extensibility of the codebase in the future
    - In many texts, the dependency inversion principle is explained in complicated terms. The dependency inversion principle generally means programming against interfaces instead of concrete class types.
- You don't understand the value of dependency injection and are not using it
  - Dependency injection is a requirement for effectively utilizing some other principles, like the open-closed principle
  - Dependency injection makes unit testing a breeze because you can create mock implementations and inject them into the tested code
- You don't know/understand design patterns and don't know when and how to use them
  - Familiarize yourself with the design patterns
  - Some design patterns are more useful than others. You use some patterns basically in every codebase, and some patterns you rarely use
  - Many design patterns help make code more modular and extensible and help avoid modifying existing code. Modifying existing code is always a risk. You can introduce bugs in already working code. These bugs are sometimes very subtle and hard to discover.
  - Learning the design patterns takes time. It can take years to master them, and mastery is only achieved by repeatedly using them in real-life codebases.

Mastering OOD and OOP is a life-long process. You are never 100% ready. The best way to become better in OOD and OOP, as in any other thing
in your life, is practicing. I have been practicing OOD and OOP for 29 years and am still improving and learning something new regularly.
Start a non-trivial (hobby/work) project and try to make the code 100% clean. Whenever you think you are ready
with it, leave the project for some time and later come back to the project, and you might be surprised to notice that there are several things
still needing improvement!

## SOLID Principles

All five [SOLID principles](https://en.wikipedia.org/wiki/SOLID) are covered in this section. The *dependency inversion principle* is generalized
as a *program against interfaces principle*. The five SOLID principles are the following:

- Single responsibility principle
- Open-closed principle
- Liskov's substitution principle
- Interface segregation principle
- Dependency inversion principle (Generalization: program against interfaces principle)

### Single Responsibility Principle

> ***Classes should have one responsibility: representing a thing or providing a single functionality.***
> ***Functions should do one thing only.***

Single responsibility should be at a particular abstraction level. A class or function can become too large if the abstraction level is too high. Then, split the class or function into multiple classes or functions on a lower
level of abstraction. The single responsibility principle is akin to the [separation of concerns](https://en.wikipedia.org/wiki/Separation_of_concerns) principle.
In the *separation of concerns* principle, you divide a software component into distinct "sections". A section can be any size, e.g., a subdomain (package),
module, class, or function.

Suppose you need to implement configuration reading and parsing for your software component. Reading and parsing are two different
concerns and should be implemented in separate "sections", which in practice means implementing them in different class hierarchies: a `ConfigReader` interface
with various implementation classes, like `FileSystemConfigReader`, `DatabaseConfigReader`, `RestApiConfigReader`, and a `ConfigParser` interface with various implementation
classes like `JsonConfigParser`, `YamlConfigParser`, `TomlConfigParser`, `XmlConfigParser`, etc.

Single responsibility helps to achieve *high cohesion*, which
is the target of good design (another target is *low coupling*, which we will discuss later). If your class or function has multiple distinct responsibilities (and reasons for change),
then the class or function does not have high cohesion. Cohesion, in a class, for example, means the level that class methods belong together (i.e., change for the same reason).
If you have a class that performs user authentication and configuration parsing, you have a class with two distinct responsibilities at the same abstraction level. That class is against the *single responsibility principle*,
and has lower cohesion because it has two reasons to change. One great sign of a class possibly having multiple responsibilities is that it can be hard to figure out a good name for the class, or if you could put an *and* word in the class name, like `UserAuthenticatorAndConfigParser`.
*High cohesion* and *low coupling* are both part of the [GRASP](https://en.wikipedia.org/wiki/GRASP_(object-oriented_design)) principles.

Another great example of the separation of concerns principle is the *clean microservice design (or architecture) principle* where
you separate the microservice's business logic from the microservice's input and output. In this way, it is easy to make the microservice
support various inputs and outputs without modifying the business logic "section". The clean microservice design (or architecture) principle
is discussed later in this chapter.

Let's get back to the single responsibility principle. Each class should have a single dedicated purpose. A class can represent a single thing, like
a bank account (`Account` class) or an employee (`Employee` class), or provide a single functionality
like parsing a configuration file (`ConfigFileParser` class) or calculating tax (`TaxCalculator` class).

We should not create a class representing a bank account and an employee. It is simply wrong.
Of course, an employee can *have* a bank account. But that is a different thing. It is called object composition.
In object composition, an `Employee` class object contains an `Account` class object. The `Employee` class still represents one thing:
An employee (who can have a bank account). Object composition is covered in more detail later in this chapter.

At the function level, each function should perform a single task. The function name should describe what task the function
performs, meaning each function name should contain a verb. The function name should not contain the word *and*
because it can mean that the function is doing more than one thing or you haven't named the function on a correct abstraction level.
You should not name a function according to the steps it performs (e.g., do_this_and_that_and_then_some_third_thing) but
instead, use wording on a higher level of abstraction.

When a class represents something, it can contain multiple methods. For example, an `Account` class can have
methods like `deposit` and `withdraw`. It is still a single responsibility if these methods are simple enough
and if there are not too many methods in the class.

Below is a real-life code example where the *and* word is used in the function name:

```python
def delete_page_and_all_references(page: Page):
    delete_page(page)
    registry.delete_reference(page.name)
    config_keys.delete_key(page.name.make_key())
```

In the above example, the function does two things: delete a page and remove all the references to that page.
But if we look at the code inside the function, we can realize that it is also doing a third thing: deleting a page key from configuration keys.
So should the function be named `delete_page_and_all_references_and_config_key`? It does not sound reasonable.
The problem with the function name is that it is at the same level of abstraction as the function statements.
The function name should be at a higher level of abstraction than the statements inside the function.

How should we then name the function? I cannot say for sure because I don't know the context of the function.
We could name the function just `delete`. This would tell
the function caller that a page will be deleted. The caller does not need to know all the actions
related to deleting a page. The caller just wants a page to be deleted. The function implementation should fulfill that
request and do the needed housekeeping tasks, like removing all the references to the page being deleted and so on.

Let's consider another example with [React Hooks](https://react.dev/reference/react/hooks). React Hooks has a function named `useEffect`, which can be used to enqueue
functions to be run after component rendering. The `useEffect` function can be used to run some code
after the initial render (after the component mount), after every render, or conditionally. This is quite a
responsibility for a single function. Also, the function's name does not reveal its purpose; it sounds abstract. The word *effect*
comes from the fact that this function is used to enqueue other functions with side effects to be run. The term [side effect](https://en.wikipedia.org/wiki/Side_effect_(computer_science))
might be familiar to functional programmers. It indicates that a function is not pure because it causes side effects.

Below is an example of a React functional component:

{format: jsx, type: code}
![MyComponent.jsx](resources/chapter2/code/MyComponent.jsx)

In the above example, the `useEffect` call makes calls to functions `startFetchData` and `subscribeToDataUpdates`
to happen after the initial render because of the supplied empty array for dependencies
(the second parameter to the `useEffect` function). The cleanup function returned from the function supplied to `useEffect`
will be called before the effect will be rerun or when the component is unmounted and in this case, only on unmount
because the effect will only run once after the initial render.

Let's imagine how we could improve the `useEffect` function. We could split the rather abstract-sounding `useEffect` method into multiple methods on a lower level
of abstraction. The functionality related to mounting and
unmounting could be separated into two different functions: `afterMount` and `beforeUnmount`. Then, we could change the above example
to the following piece of code:

```jsx
export default function MyComponent() {
  function startFetchData() {
    // ...
  }

  function subscribeToDataUpdate() {
    // ...
  }

  function unsubscribeFromDataUpdate() {
    // ...
  }

  afterMount(startFetchData, subscribeToDataUpdates);
  beforeUnmount(unsubscribeFromDataUpdates)

  // JSX to render
  return ...;
}
```

The above example is cleaner and much easier for a reader to understand than the original example. There are no
multiple levels of nested functions. You don't have to return a function to be executed on component unmount, and you don't
have to supply an array of dependencies.

Let's have another example of a React functional component:

```jsx
import { useEffect, useState } from "react";

export default function ButtonClickCounter() {
  const [clickCount, setClickCount] = useState(0);

  useEffect(() => {
    function updateClickCountInDocumentTitle() {
      document.title = `Click count: ${clickCount}`;
    }

    updateClickCountInDocumentTitle();
  });
}
```

In the above example, the effect is called after every render (because no dependencies array is supplied
for the `useEffect` function). Nothing in the above code clearly states what will be executed and when. We
still use the same `useEffect` function, but now it behaves differently than in the previous example. It
seems like the `useEffect` function is doing multiple things. How to solve this? Let's think hypothetically again.
We could extract functionality from the `useEffect` function and introduce yet another new function called `afterEveryRender` that can be called when we want something to happen after every render:

```react
export default function ButtonClickCounter() {
  const [clickCount, setClickCount] = useState(0);

  afterEveryRender(function updateClickCountInDocumentTitle() {
    document.title = `Click count: ${clickCount}`;
  });
}
```

The intentions of the above React functional component are pretty clear: It will update the click count in the
document title after every render.

Let's optimize our example so that the click count update happens only if the click count has changed:

```react
import { useEffect, useState } from "react";

export default function ButtonClickCounter() {
  const [clickCount, setClickCount] = useState(0);

  useEffect(() => {
    function updateClickCountInDocumentTitle() {
      document.title = `Click count: ${clickCount}`;
    }

    updateClickCountInDocumentTitle();
  }, [clickCount]);
}
```

Notice how `clickCount` is now added to the dependencies array of the `useEffect` function. This means
the effect is not executed after every render but only when the click count is changed.

Let's imagine how we could improve the above example. We could once again extract functionality from the `useEffect` function and introduce a new function that handles dependencies:
`afterEveryRenderIfChanged`. Our hypothetical example would now look like this:

```react
export default function ButtonClickCounter() {
  const [clickCount, setClickCount] = useState(0);

  afterEveryRenderIfChanged(
    [clickCount],
    function updateClickCountInDocumentTitle() {
      document.title = `Click count: ${clickCount}`;
  });
}
```

Making functions do a single thing at an appropriate level of abstraction also helped make the code more readable.
Regarding the original examples, a reader must look at the end of the `useEffect` function call to figure out in what
circumstances the effect function will be called. Understanding and remembering the difference between
a missing and empty dependencies array is cognitively challenging.

> ***Good code is such that it does not make the code reader think. At best, the code should read like beautifully written prose.***

In the above example, we can read like prose: *after every render if changed click count, update click count in document title*.

One idea behind the single responsibility principle is that it enables software development using the *open-closed principle*
described in the next section. When you follow the single responsibility principle and need
to add functionality, you add it to a new class, which means you don't need to modify an existing
class. You should avoid modifying existing code but extend it by adding new classes, each with a single responsibility.
Modifying existing code always poses a risk of breaking something that works.

### Open-Closed Principle

> ***Software code should be open for extension and closed for modification. Functionality in existing classes should not be modified, but new classes that implement a new or existing interface or extend an existing class should be introduced.***

Any time you find yourself modifying some method in an existing class, you should consider if
this principle could be followed and if the modification could be avoided. Every time you modify an existing class, you can
introduce a bug in the working code. The idea of this principle is to leave the working code untouched so it does not get accidentally broken.
The *open-closed principle* is called *protected variations* in the GRASP principles. The protected variation principle protects existing
classes from variations in other classes. For example, if you have a `ConfigReader` class that needs to parse the configuration, but the configuration format can vary. The `ConfigReader` class is protected from variations by introducing a `ConfigParser` interface for which various implementations can be provided. The `ConfigReader` depends only on the `ConfigParser` interface and does not need to know what
particular parsing implementation is actually used. There could be a `JsonConfigParser` class for parsing the configuration in JSON format, and later, a `YamlConfigParser` class could be introduced to parse the configuration in YAML format.
The `JsonConfigParser` class is also protected from variations because possible variations in configuration parsing can be introduced in new classes instead of modifying
an existing class.

Let's have an example where this principle is *not* followed. We have the following existing and working code:

```python
from typing import Protocol


class Shape(Protocol):
    # ...


class RectangleShape(Shape):
    def __init__(self, width: int, height: int):
        self.__width = width
        self.__height = height

    @property
    def width(self) -> int:
       return self.__width

    @property
    def height(self) -> int:
       return self.__height

    @width.setter
    def width(self, width: int) -> None:
        self.__width = width

    @height.setter
    def height(self, height: int) -> None:
        self.__height = height
```

Suppose we get an assignment to introduce support for square shapes. Let's try to modify the existing
`RectangleShape` class, because a square is also a rectangle:

```python
class RectangleShape(Shape):
    # Constructor for creating rectangles
    def __init__(self, width: int, height: int):
        self.__width = width
        self.__height = height

    # Factory method for creating squares
    @classmethod
    def create_square(cls, side_length: int) -> 'RectangleShape':
        return cls(side_length, side_length)

    @property
    def width(self) -> int:
        return self.__width

    @property
    def height(self) -> int:
        return self.__height

    @width.setter
    def width(self, width: int) -> None:
        if self.__height == self.__width:
            self.__height = width
        self.__width = width

    @height.setter
    def height(self, height: int) -> None:
        if self.__height == self.__width:
            self.__width = height
        self.__height = height
```

We needed to add a factory method for creating squares and modify two methods in the class. Everything works okay when we run tests.
But we have introduced a subtle bug in the code: If we create a rectangle
with an equal height and width, the rectangle becomes a square, which is probably not what is wanted. This is
a bug that can be hard to find in unit tests. This example showed that modifying an existing class can be problematic.
We modified an existing class and accidentally broke it.

A better solution to introduce support for square shapes is to use the *open-closed principle* and create a new class
that implements the `Shape` protocol. Then, we don't have to modify any existing class, and there is no risk of accidentally
breaking something in the existing code. Below is the new `SquareShape` class:

```python
class SquareShape(Shape):
    def __init__(self, side_length: int):
        self.__side_length = side_length

    @property
    def side_length(self) -> int:
        return self.__side_length

    @side_length.setter
    def side_length(self, side_length: int) -> None:
        self.__side_length = side_length
```

An existing class can be safely modified by adding a new method in the following cases:

1) The added method is a pure function, i.e., it always returns the same value for the same arguments and does not have side effects, e.g., it does not modify the object's state.
2) The added method is read-only and tread-safe, i.e., it does not modify the object's state and accesses the object's state in a thread-safe manner in the case of multithreaded code. An example of a read-only method in the `Shape` class would be a method that calculates a shape's area.
3) Class is immutable, i.e., the added method (or any other method) cannot modify the object's state

There are a couple of cases where the modification of existing code is needed. One example is factories. When you
introduce a new class, you need to modify the related factory to be able to create an instance of that new class.
For example, if we had a `ShapeFactory` class, we would need to modify it to support the creation of `SquareShape` objects.
Fortunately, this modification is simple: Just adding a new case branch. The probability of introducing a bug is very low.
Factories are discussed later in this chapter.

Another case is adding a new enum constant. You typically need to modify existing code to handle the new enum constant.
If you forget to add the handling of the new enum constant somewhere in the existing code, typically, a bug will arise. For this reason,
You should always safeguard match-case statements with a `_` case that raises an exception and safeguard if/elif structures with an else branch that raises an exception.
You can also enable your static code analysis tool to report an issue if a match statement's `_` case or
an else branch is missing from an if/elif structure. Also, some static code analysis tools can report an issue if you miss handling
an enum constant in a match-case statement.

Here is an example of safeguarding an if/elif structure:

```python
from enum import Enum
from typing import Protocol, Final


class FilterType(Enum):
    INCLUDE = 1
    EXCLUDE = 2


class Filter(Protocol):
    def is_filtered_out(self) -> bool:
        pass


class FilterImpl(Filter):
    def __init__(self, filter_type: FilterType):
        self.__filter_type: Final = filter_type

    def is_filtered_out(self) -> bool:
        if self.__filter_type == FilterType.INCLUDE:
            # ...
        elif self.__filter_type == FilterType.EXCLUDE:
            # ...
        else:
            # Safeguarding
            raise ValueError('Invalid filter type')
```

Safeguarding might be needed for a [literal type union](https://peps.python.org/pep-0586/) also:

```python
from typing import Literal

FilterType = Literal['include', 'exclude']

filter_type: FilterType = # ...

if filter_type == 'include':
    # ...
elif filter_type == 'exclude':
    # ...
else:
    # Safeguarding
    raise ValueError('Invalid filter type')
```

In the future, if a new literal is added to the `FilterType` type and you forget to update the if-statement, you
get an exception raised instead of silently passing through the if-statement without any action.

We can notice from the above examples that if/elif structures could be avoided
with a better object-oriented design. For instance, we could create a `Filter` protocol and two separate classes,
`IncludeFilter` and `ExcludeFilter`. The classes implement the `Filter` protocol. Using object-oriented design allows us to eliminate the `FilterType` enum and the if/elif structure.
This is known as the *replace conditionals with polymorphism* refactoring technique. Refactoring is discussed more
in the next chapter. Below is the above example refactored to be more object-oriented:

```python
from typing import Protocol


class Filter(Protocol):
    def is_filtered_out(self) -> bool:
        pass


class IncludeFilter(Filter):
    # ...

    def is_filtered_out(self) -> bool:
        # ...


class ExcludeFilter(Filter):
    # ...

    def is_filtered_out(self) -> bool:
        # ...
```

### Liskov's Substitution Principle

> ***Objects of a superclass should be replaceable with objects of its subclasses without breaking the application. I.e.,***
> ***objects of subclasses behave the same way as the objects of the superclass.***

Following *Liskov's substitution principle* guarantees semantic interoperability of types in a type hierarchy.

Let's have an example with a `RectangleShape` class and a derived `SquareShape` class:

```python
from typing import Protocol


class Shape(Protocol):
    def draw(self) -> None:
        pass


class RectangleShape(Shape):
    def __init__(self, width: int, height: int):
        self.__width = width;
        self.__height = height

    def draw(self):
        # ...

    @property
    def width(self) -> int:
        return self.__width

    @property
    def height(self) -> int:
        return self.__height

    @width.setter
    def width(self, width: int) -> None:
         self.__width = width

    @height.setter
    def height(self, height: int) -> None:
         self.__height = height


class SquareShape(RectangleShape):
    def __init__(self, side_length: int):
        super().__init__(side_length, side_length)

    @RectangleShape.width.setter
    def width(self, width: int) -> None:
        RectangleShape.width.fset(self, width)
        RectangleShape.height.fset(self, width)

    @RectangleShape.height.setter
    def height(self, height: int) -> None:
        RectangleShape.width.fset(self, height)
        RectangleShape.height.fset(self, height)
```

The above example does not follow Liskov's substitution principle because you cannot set a square's width and height separately.
This means that a square is not a rectangle from an object-oriented point of view. Of course, mathematically, a square is a rectangle. But when
considering the above public API of the `RectangleShape` class, we can conclude that a square is not a rectangle because
a square cannot fully implement the API of the `RectangleShape` class.
We cannot substitute a square object for a rectangle object. What we need to do is to implement the `SquareShape` class without
deriving from the `RectangleShape` class:

```python
class SquareShape(Shape):
    def __init__(self, side_length: int):
        self.__side_length = side_length

    def draw(self):
        # ...

    @property
    def side_length(self) -> int:
        return self.__side_length

    @side_length.setter
    def side_length(self, side_length: int) -> None:
        self.__side_length = side_length
```

Let's have another example where we have the following two classes:

```python
class Dog:
    def bark(self):
        # ...

    def walk(self):
        # ...

    def eat(self):
        # ...

    # ...


class RoboticDog(Dog):
    def bark(self):
        # Use super class method
        # get_sound()

    def walk(self):
        # Use super class method
        # get_speed()

    def eat(self):
        # Robotic dog cannot eat
        raise NotImplementedError()
```

The above example does not follow *Liskov's substitution principle*, because a `RoboticDog` object cannot be used
in place of a `Dog` object. The `RoboticDog` object raises an error if you call the `eat` method. There are two
solutions to the problem:

1) Abstract away
2) Composition over inheritance

Let's abstract the `eat` method to something common to both a dog and a robotic dog. We could change the
`eat` method to a `recharge` method:

```python
class Dog:
    def bark(self):
        # ...

    def walk(self):
        # ...

    def recharge(self):
        # Eat

    # ...


class RoboticDog(Dog):
    def bark(self):
        # ...

    def walk(self):
        # ...

    def recharge(self):
        # Charge the robot
```

The other solution is to use composition over inheritance:

```python
class RoboticDog:
    def __init__(self, dog: Dog):
        self.__dog = dog

    def bark(self):
        # Use self.__dog.get_sound()

    def walk(self):
        # Use self.__dog.get_speed()
```

Liskov's substitution principle requires the following:

- A subclass must implement the superclass API and retain (or, in some cases, replace) the functionality of the superclass.
- A superclass should not have protected attributes because it allows subclasses to modify the state of the superclass, which can lead to incorrect behavior in the superclass.

Below is an example where a subclass extends the behavior of a superclass in the `do_something` method. The functionality of the superclass is retained in the subclass
making a subclass object substitutable for a superclass object.

```python
class SuperClass:
    # ...

    def do_something(self):
        # ...


class SubClass(SuperClass):
    # ...

    def do_something(self):
        super().do_something()

        # Some additional behaviour...
```

Let's have a concrete example of using the above strategy. We have the following `CircleShape` class defined:

```python
from typing import Protocol


class Shape(Protocol):
    def draw(self) -> None:
        pass


class CircleShape(Shape):
    def draw(self) -> None:
        # Draw the circle stroke here
```

Next, we introduce a class for filled circles:

```python
class FilledCircleShape(CircleShape):
    def draw(self) -> None:
        super().draw() # Draws the circle stroke
        # Fill the circle here...
```

The `FilledCircleShape` class fulfills the requirements of Liskov's substitution principle. We can use an instance of the
`FilledCircleShape` class everywhere where an instance of the `CircleShape` class is wanted. The `FilledCircleShape` class
does all that the `CircleShape` class does, plus adds some behavior (= fills the circle).

You can also completely replace the superclass functionality in a subclass:

```python
from collections import UserList


class ReverseList(UserList):
    def __iter__(self):
        return ReverseListIterator(self)
```

The above subclass implements the superclass API and retains its behavior: The `iterator` method still returns an iterator.
It just returns a different iterator compared to the superclass.

### Interface Segregation and Multiple Inheritance Principle

> ***No class should depend on other classes' methods it does not use.***

When following the *interface segregation principle*, you split larger interfaces into smaller interfaces so that no one should depend on something it does not use.
Let's have an example where we have the following classes:

```python
from typing import Protocol


class ProtocolA(Protocol):
    def method1(self):
        pass

    def method2(self):
        pass

    def method3(self):
        pass

    def method4(self):
        pass

    def method5(self):
        pass


class ClassB():
    __someAttribute: ProtocolA

    # Depends on ProtocolA but
    # uses only the following methods:
    # method1, method2, method3


class ClassC():
    __someAttribute: ProtocolA

    # Depends on ProtocolA and
    # uses all methods from the ProtocolA
```

The `ClassB` is depending on `method4` and `method5` even if it does not use them. We need to apply the *interface segregation principle* and segregate a smaller interface from the `ProtocolA`:

```python
from typing import Protocol


class ProtocolA1(Protocol):
    def method1(self):
        pass

    def method2(self):
        pass

    def method3(self):
        pass


class ProtocolA(ProtocolA1):
    def method4(self):
        pass

    def method5(self):
        pass


class ClassB():
    __someAttribute: ProtocolA1

    # Depends on ProtocolA1
    # and uses all methods of it


class ClassC():
    __someAttribute: ProtocolA

    # Depends on ProtocolA
    # and uses all methods of it
```

*Interface segregation principle* is a way to reduce coupling in your software component. A software component's design
is considered the most optimal when it has *low coupling* between classes and *high cohesion* in classes. In the above example, the `ClassB`
only depends on the three methods provided by the `ProtocolA1`, not all the five methods provided by the `ProtocolA`.

Next, we will have examples of an extreme case of the *interface segregation principle*: Segregating larger interfaces to microinterfaces with a single capability/behavior and constructing larger interfaces by
inheriting multiple microinterfaces. We will use the Python-specific term *protocol* instead of *interface* for the rest of this section.
Let's have an example with several automobile classes:

```python
from typing import Protocol

from Location import Location


class Automobile(Protocol):
    def drive(self, start: Location, destination: Location) -> None:
        pass

    def carry_cargo(
        self,
        volume_in_cubic_meters: float,
        weight_in_kgs: float
    ) -> None:
        pass


class PassengerCar(Automobile):
    # Implement drive and carry_cargo


class Van(Automobile):
    # Implement drive and carry_cargo


class Truck(Automobile):
    # Implement drive and carry_cargo


class ExcavatingAutomobile(Automobile):
    def excavate(self) -> None:
        pass


class Excavator(ExcavatingAutomobile):
    # Implement drive, carry_cargo and excavate
```

Notice how the `Automobile` protocol has two methods declared. This can limit our software if we
later want to introduce other vehicles that could be just driven but unable to carry cargo. We should
segregate two microprotocols from the `Automobile` protocol in an early phase. A microprotocol defines a single capability
or behavior. After segregation, we will have the following two microprotocols:

```python
from typing import Protocol

from Location import Location


class Drivable(Protocol):
    def drive(self, start: Location, destination: Location) -> None:
        pass


class CargoCarrying(Protocol):
    def carry_cargo(
        self,
        volume_in_cubic_meters: float,
        weight_in_kgs: float
    ) -> None:
        pass
```

Now that we have two protocols, we can use these protocols separately in our codebase. For example, we can have
a list of drivable objects or a list of objects that can carry cargo. We still want to have a protocol
for automobiles, though. We can use *protocol multiple inheritance* to redefine the `Automobile` protocol to extend
the two microprotocols:

```python
class Automobile(Drivable, CargoCarrying):
    pass
```

If we look at the `ExcavatingAutomobile` protocol, we can notice that it extends the `Automobile` protocol and
adds excavating behavior. Once again, we have a problem if we want an excavating machine that is not
auto-mobile. The excavating behavior should be segregated into its own microprotocol:

```python
class Excavating(Protocol):
    def excavate(self) -> None:
        pass
```

We can once again use the protocol multiple inheritance to redefine the `ExcavatingAutomobile` protocol as follows:

```python
class ExcavatingAutomobile(Excavating, Automobile):
    pass
```

The `ExcavatingAutomobile` protocol now extends three microprotocols: `Excavating`, `Drivable`,
and `CargoCarrying`. Where-ever you need an excavating, drivable, or cargo-carrying object in your codebase,
you can use an instance of the `Excavator` class there.

Let's have another example with a generic collection protocol. We should be able to
traverse a collection and also be able to compare two collections for equality. First, we define a generic `Iterator` protocol for iterators. It has two methods, as described below:

```python
from typing import Protocol, TypeVar

T = TypeVar('T')


class Iterator(Protocol[T]):
    def has_next_elem(self) -> bool:
        pass

    def get_next_elem(self) -> T:
        pass
```

Next, we can define the collection protocol:

```python
class Collection(Protocol[T]):
    def create_iterator(self) -> Iterator[T]:
        pass

    def equals(self, another_collection: 'Collection[T]') -> bool:
        pass
```

`Collection` is a protocol with two unrelated methods. Let's segregate those methods
into two microprotocols: `Iterable` and `Equatable`. The `Iterable` protocol is for objects that you can iterate over.
It has one method for creating new iterators. The `Equatable` protocol's `equals` method is more generic than
the `equals` method in the above `Collection` protocol. You can equate an `Equatable[T]` object with
another object of type `T`:

```python
class Iterable(Protocol[T]):
    def create_iterator(self) -> Iterator[T]:
        pass


class Equatable(Protocol[T]):
    def equals(self, another_object: T) -> bool:
        pass
```

We can use protocol multiple inheritance to redefine the `Collection` protocol as follows:

```python
class Collection(Iterable[T], Equatable['Collection[T]']):
    pass
```

We can implement the `equals` method by iterating elements in two collections and checking if the elements are equal:

```python
from abc import abstractmethod


class AbstractCollection(Collection[T]):
    @abstractmethod
    def create_iterator(self) -> Iterator[T]:
        pass

    def equals(self, another_collection: Collection[T]):
        iterator = self.create_iterator()
        another_iterator = another_collection.create_iterator()

        collections_are_equal = self.__are_equal(
            iterator, another_iterator
        )

        return (
            False
            if another_iterator.has_next_elem()
            else collections_are_equal
        )

    @staticmethod
    def __are_equal(iterator: Iterator[T], another_iterator: Iterator[T]):
        while iterator.has_next_elem():
            if another_iterator.has_next_elem():
                if (
                    iterator.get_next_elem()
                    != another_iterator.get_next_elem()
                ):
                    return False
            else:
                return False
        return True
```

Collections can also be compared. Let's introduce support for such collections. First, we define a generic `Comparable` protocol for comparing an object with another object:

```python
from typing import Protocol, Literal

ComparisonResult = Literal['isLessThan', 'areEqual', 'isGreaterThan', 'unspecified']


class Comparable(Protocol[T]):
    def compare(self, another_object: T) -> ComparisonResult:
        pass
```

Now, we can introduce a comparable collection protocol that allows comparing two collections
of the same type:

```python
class ComparableCollection(Comparable[Collection[T]], Collection[T]):
    pass
```

Let's define a generic sorting algorithm for collections whose elements are comparable:

```python
U = TypeVar('U', bound=ComparableCollection)

def sort(collection: U) -> U:
    # ...
```

Let's create two protocols, `Inserting` and `InsertingIterable` for classes whose instances elements can be inserted into:

```python
class Inserting(Protocol[T]):
    def insert(self, element: T) -> None:
        pass


class InsertingIterable(Inserting[T], Iterable[T]):
    pass
```

Let's redefine the `Collection` protocol to extend the `InsertingIterable` protocol because a collection is iterable,
and you can insert elements into a collection.

```python
class Collection(InsertingIterable[T]):
    pass
```

Next, we introduce two generic algorithms for collections: `map` and `filter`. We can realize that those algorithms work
with more abstract objects than collections. We benefit from protocol segregation because instead of
the `Collection` protocol, we can use the `Iterable` and
`InsertingIterable` protocols to create generic `map` and `filter` algorithms. Later, it is possible to introduce
additional non-collection iterable objects that can utilize the algorithms. Below are the implementations of the
`map` and `filter` functions:

```python
from collections.abc import Callable
from typing import TypeVar

T = TypeVar('T')
U = TypeVar('U')


def map(
    source: Iterable[T],
    mapped: Callable[[T], U],
    destination: InsertingIterable[U],
) -> InsertingIterable[U]:
    source_iterator = source.create_iterator()
    while source_iterator.has_next_elem():
        source_element = source_iterator.get_next_elem()
        destination.insert(mapped(source_element))
    return destination


def filter(
    source: Iterable[T],
    is_included: Callable[[T], bool],
    destination: InsertingIterable[T],
) -> InsertingIterable[T]:
    source_iterator = source.create_iterator()
    while source_iterator.has_next_elem():
        source_element = source_iterator.get_next_elem()
        if is_included(source_element):
            destination.insert(source_element)
    return destination
```

Let's define the following concrete collection classes:

```python
class List(Collection[T]):
    def __init__(self, *args: T):
        # ...

    # ...


class Stack(Collection[T]):
    # ...


class Set(Collection[T]):
    # ...
```

Now, we can use the `map` and `filter` algorithms with the above-defined collection classes:

```python
numbers = List(1, 2, 3, 3, 3, 50, 60)
is_less_than_10 = lambda number: number < 10
unique_less_than_10_numbers = filter(numbers, is_less_than_10, Set())

doubled = lambda number: 2 * number
stack_of_doubled_numbers = map(numbers, doubled, Stack())
```

Let's create an asynchronous version of the `map` algorithm:

```python
from collections.abc import Callable
from typing import Protocol, TypeVar

T = TypeVar('T')
U = TypeVar('U')


class Closeable(Protocol):
    def close(self) -> None:
        pass


class MaybeInserting(Protocol[T]):
    class InsertError(Exception):
        pass

    async def try_insert(self, value: T) -> None:
        pass


class CloseableMaybeInserting(Closeable, MaybeInserting[T]):
    pass


class MapError(Exception):
    pass


async def try_map(
    source: Iterable[T],
    mapped: Callable[[T], U],
    destination: CloseableMaybeInserting[U],
) -> None:
    source_iterator = source.create_iterator()
    try:
        while source_iterator.has_next_elem():
            source_element = source_iterator.get_next_elem()
            await destination.try_insert(mapped(source_element))
    except destination.InsertError as error:
        raise MapError(error)
    finally:
        destination.close()
```

Let's create a `FileLineInserter` class that implements the `CloseableMaybeInserting` protocol using the [aiofiles](https://github.com/Tinche/aiofiles) library:

```python
from typing import Final

from aiofiles import open


class FileLineInserter(CloseableMaybeInserting[T]):
    def __init__(self, file_path_name: str):
        self.__file = None
        self.__file_path_name: Final = file_path_name

    async def try_insert(self, value: T):
        if self.__file is None:
            self.__file = await open(self.__file_path_name, mode='w')
        line = str(value) + '\n'
        await self.__file.write(line)

    def close(self):
        self.__file.close()
```

Let's use the above-defined `try_map` algorithm and the `FileLineInserter` class to write doubled numbers (one number per line) to
a file named *file.txt*:

```python
from asyncio import run

numbers = [1, 2, 3, 2, 1, 50, 60]
doubled = lambda number: 2 * number


async def my_func():
    try:
        await try_map(numbers, doubled, FileLineInserter('file.txt'))
    except MapError as error:
        print(str(error))


run(my_func())
```

Python's standard library utilizes interface segregation and multiple interface inheritance in an exemplary
way. For example, the Python standard library defines the below listed abstract base classes (or interfaces) that implement a single
method only. I.e., they are microinterfaces.

| Abstract base class |  Method        |
| --------------------|----------------|
| `Container`         | `__contains__` |
| `Hashable`          | `__hash__`     |
| `Iterable`          | `__iter__`     |
| `Sized`             | `__len__`      |
| `Callable`          | `__call__`     |
| `Awaitable`         | `__await__`    |
| `AsyncIterable`     | `__aiter__`    |


Python standard library also contains the below abstract base classes that inherit from multiple (micro)interfaces:

| Abstract base class |  Inherits from                    |
|---------------------|-----------------------------------|
| `Collection`        | `Sized`, `Iterable`, `Container`  |
| `Sequence`          | `Collection`, `Reversible`        |


### Program Against Interfaces Principle (Generalized Dependency Inversion Principle)

> ***Do not write programs where internal dependencies are concrete object typesinstead, program against interfaces.***
> ***An exception to this rule is data classes with no behavior (not counting simple getters/setters)***.

An interface is used to define an abstract base type. Various implementations that implement the interface can be introduced.
When you want to change the behavior of a program, you create a new class that implements an interface
and then use an instance of that class. In this way, you can practice the *open-closed principle*.
You can think of this principle as a prerequisite for using the *open-closed principle* effectively.
The *program against interfaces principle* was presented by the *Gang of Four* in their book *Design Patterns* and
can be seen as a generalization of the *dependency inversion principle* from the SOLID principles:

> The *dependency inversion principle* is a methodology for loosely coupling software classes. When following the principle,
> the conventional dependency relationships from high-level classes to low-level classes are reversed, thus making the high-level
> classes independent of the low-level implementation details.

The *dependency inversion principle* states:

1) High-level classes should not import anything from low-level classes
2) Abstractions (= interfaces) should not depend on concrete implementations (classes)
3) Concrete implementations (classes) should depend on abstractions (= interfaces)

![Dependency Inversion Principle](resources/chapter2/images/dep_inv_principle.png)

*Dependency inversion principle* is the primary way to reduce coupling in a software component's code (the other way being the
*interface segregation principle*). When you use the principle, your classes are not coupled to any concrete implementation but to an interface promising its implementors to offer certain functionality. For example, if your software component needs a collection to store and retrieve items and occasionally get the item count, you should define an interface for the wanted functionality:

```python
from typing import Protocol


class Collection(Protocol):
    def add(self, item):
        pass

    def remove(self, item):
        pass

    def length(self):
        pass
```

Being coupled to the above `Collection` interface is much weaker and less coupling than being coupled to a concrete implementation
like a stack or linked list.

An interface is always an abstract type and cannot be instantiated. Below is an example of an interface:

```python
from typing import Protocol


class Shape(Protocol):
    def draw(self) -> None:
        pass

    def calculate_area(self) -> float:
        pass
```

The name of an interface describes something abstract, which you cannot create an object of.
In the above example, `Shape` is something abstract. You cannot create an instance of `Shape` and then
draw it or calculate its area because you don't know what shape it is. But when a class implements an
interface, a concrete object of the class representing the interface can be created. Below is an example of three different classes
that implement the `Shape` interface:

```python
from math import pi
from typing import Final


class CircleShape(Shape):
    def __init__(self, radius: int):
        self.__radius: Final = radius

    def draw(self) -> None:
        # ...

    def calculate_area(self) -> float:
        return pi * self.__radius**2


class RectangleShape(Shape):
    def __init__(self, width: int, height: int):
        self.__width: Final = width
        self.__height: Final = height

    def draw(self) -> None:
        # ...

    def calculate_area(self) -> float:
        return self.__width * self.__height


class SquareShape(RectangleShape):
    def __init__(self, side_length: int):
        super().__init__(side_length, side_length)
```

We should program against the `Shape` interface when using shapes in code. In the below example, we make
a high-level class `Canvas` dependent on the `Shape` interface, not on any of the low-level classes (`CircleShape`,
`RectangleShape` or `SquareShape`). Now, the high-level `Canvas` class and all the low-level shape classes depend on abstraction only,
the `Shape` interface. We can also notice that the high-level class `Canvas` does not import anything from the
low-level classes. Also, the abstraction `Shape` does not depend on concrete implementations (classes).

```
from typing import Final


class Canvas:
    def __init__(self):
        self.__shapes: Final[list[Shape]] = []

    def add(self, shape: Shape) -> None:
        self.__shapes.append(shape)

    def draw_shapes(self) -> None:
        for shape in self.__shapes:
            shape.draw()
```

A `Canvas` object can contain any shape and draw any shape. It can handle any of
the currently defined concrete shapes and any new ones defined in the future.

If you did not program against interfaces and did not use the dependency inversion principle, your `Canvas` class would
look like the following:

```
from typing import Final


class Circle:
    def draw(self) -> None:
        # ...


class Rectangle:
    def draw(self) -> None:
        # ...


class Square:
    def draw(self) -> None:
        # ...


class Canvas:
  def __init__(self):
      self.__shapes: Final[list[Circle | Rectangle | Square]] = []

  def add(self, shape: Circle | Rectangle | Square) -> None:
      self.__shapes.append(shape)

  def draw_shapes(self) -> None:
      for shape in self.__shapes:
          shape.draw()
```

The above high-level `Canvas` class is coupled with all the low-level classes (`Circle`, `Rectangle`, and `Square`).
The type annotations in the `Canvas` class must be modified if a new shape type is needed. If something changes in the public API
of any low-level class, the `Canvas` class needs to be modified accordingly. In the above example, we implicitly specify
the protocol for the `draw` method: it does not take arguments and returns `None`. Relying on implicit protocols is not
a good solution, especially in non-trivial applications. It is better to program against interfaces and make protocols explicit.

Let's have another example. If you have read books or articles about object-oriented design, you may have encountered
something similar as is presented in the below example:

```python
class Dog:
    def walk(self) -> None:
        # ...

    def bark(self) -> None:
        # ...


class Fish:
    def swim(self) -> None:
        # ...


class Bird:
    def fly(self) -> None:
        # ...

    def sing(self) -> None:
        # ...
```

Three concrete implementations are defined above, but no interface is defined. Let's say we are making
a game that has different animals. The first thing to do when coding the game is to remember to program
against interfaces and thus introduce an `Animal` protocol that we can use as an abstract base type. Let's try to create
the `Animal` protocol based on the above concrete implementations:

```python
from typing import Protocol


class Animal(Protocol):
    def walk(self) -> None:
        pass

    def bark(self) -> None:
        pass

    def swim(self) -> None:
        pass

    def fly(self) -> None:
        pass

    def sing(self) -> None:
        pass


class Dog(Animal):
    def walk(self) -> None:
        # ...

    def bark(self) -> None:
        # ...

    def swim(self) -> None:
        raise NotImplementedError()

    def fly(self) -> None:
        raise NotImplementedError()

    def sing(self) -> None:
        raise NotImplementedError()


# Fish class ...
# Bird class ...
```

The above approach is wrong. We declare that the `Dog` class implements the `Animal` protocol, but
it does not do that. It implements only methods `walk` and `bark` while other methods throw an exception. We should be
able to supply any concrete animal implementation where an animal is required. But it is impossible because
if we have a `Dog` object, we cannot safely call `swim`, `fly`, or `sing` methods because they will always raise an error.

The problem is that we defined the concrete classes before defining the interface. That approach is wrong.
We should specify the interface first and then the concrete implementations. What we did above was the other way around.

When defining an interface, we should remember that we are defining an abstract base type, so we must think in abstract terms.
We must consider what we want the animals to do in the game. If we look
at the methods `walk`, `fly`, and `swim`, they are all concrete actions. But what is the abstract action common to these
three concrete actions? It is *move*. Walking, flying, and swimming are all ways
of moving. Similarly, if we look at the `bark` and `sing` methods, they are also concrete actions. What is the abstract action
common to these two concrete actions? It is *make sound*. And barking and
singing are both ways to make a sound. When we use these abstract actions, our `Animal` protocol becomes the following:

```python
from typing import Protocol


class Animal(Protocol):
    def move(self) -> None:
        pass

    def make_sound(self) -> None:
        pass
```

We can now redefine the animal classes to implement the new `Animal` protocol:

```python
class Dog(Animal):
    def move(self) -> None:
        # Walk ...

    def make_sound(self) -> None:
        # Bark ...


class Fish(Animal):
    def move(self) -> None:
        # Swim ...

    def make_sound(self) -> None:
        # Intentionally no operation
        # (Fishes typically don't make sounds)
        pass


class Bird(Animal):
    def move(self) -> None:
        # Fly ...

    def make_sound(self) -> None:
        # Sing ...
```

Now, we have the correct object-oriented design and can program against the `Animal` interface. We can call the `move` method
when we want an animal to move and the `make_sound` method when we want an animal to make a sound.

After realizing that some birds don't fly at all, we can easily enhance our design. We can introduce two different implementations:

```python
from abc import abstractmethod


class AbstractBird(Animal):
    @abstractmethod
    def move(self) -> None:
        pass

    def make_sound(self) -> None:
        # Sing ...


class FlyingBird(AbstractBird):
    def move(self) -> None:
        # Fly ...


class NonFlyingBird(AbstractBird):
    def move(self) -> None:
        # Walk ...
```

We might also later realize that not all birds sing but make different sounds. Ducks quack, for example.
Instead of using inheritance as was done above, an even better alternative is to use *object composition*.
We compose the `Bird` class of behavioral classes for moving and making sounds. This is called the *strategy pattern*,
and is discussed later in this chapter. We can give different moving and sound-making strategies for bird objects upon construction.

```python
class Mover(Protocol):
    def move(self) -> None:
        pass


class SoundMaker(Protocol):
    def make_sound(self) -> None:
        pass


class Bird(Animal):
    def __init__(self, mover: Mover, sound_maker: SoundMaker):
        self.__mover = mover
        self.__sound_maker = sound_maker

    def move(self):
        self.__mover.move()

    def make_sound(self):
        self.__sound_maker.make_sound()
```

I don't advocate adding a design pattern name to code entity names, but for demonstration purposes, we could make an exception here, and I can show
how the code would look when making the *strategy pattern* explicit:

```python
class MovingStrategy(Protocol):
    def move(self) -> None:
        pass


class SoundMakingStrategy(Protocol):
    def make_sound(self) -> None:
        pass


class Bird(Animal):
    def __init__(
        self,
        moving_strategy: MovingStrategy,
        sound_making_strategy: SoundMakingStrategy,
    ):
        self.__moving_strategy = moving_strategy
        self.__sound_making_strategy = sound_making_strategy

    def move(self):
        self.__moving_strategy.move()

    def make_sound(self):
        self.__sound_making_strategy.make_sound()
```

Now, we can create birds with various behaviors for moving and making sounds. We can use the *factory pattern* to
create different birds. The *factory pattern* is described in more detail later in this chapter. Let's introduce
three different moving and sound-making behaviors and a factory to make three kinds of birds: goldfinches, ostriches,
and domestic ducks.

```python
from enum import Enum


class Flyer(Mover):
    def move(self) -> None:
        # fly


class Runner(Mover):
    def move(self) -> None:
        # run


class Walker(Mover):
    def move(self) -> None:
        # walk


class GoldfinchSoundMaker(SoundMaker):
    def make_sound(self) -> None:
        # Sing goldfinch specific songs


class OstrichSoundMaker(SoundMaker):
    def make_sound(self) -> None:
        # Make ostrich specific sounds like whistles,
        # hoots, hisses, growls, and deep booming growls
        # that sound like the roar of a lion


class Quacker(SoundMaker):
    def make_sound(self) -> None:
        # quack


class BirdType(Enum):
    GOLDFINCH = 1
    OSTRICH = 2
    DOMESTIC_DUCK = 3


class BirdFactory:
    def create_bird(self, bird_type: BirdType) -> Bird:
        match bird_type:
            case BirdType.GOLDFINCH:
                return Bird(Flyer(), GoldfinchSoundMaker())
            case BirdType.OSTRICH:
                return Bird(Runner(), OstrichSoundMaker())
            case BirdType.DOMESTIC_DUCK:
                return Bird(Walker(), Quacker())
            case _:
                raise ValueError('Unsupported bird type')
```

## Clean Microservice Design Principle

> ***The clean microservice design promotes object-oriented design with the separation of concerns achieved***
> ***by dividing software into layers using the dependency inversion principle (programming against interfaces).***

Clean microservice design focuses on creating a microservice core (the model, business logic) that is devoid of technological concerns, pushing those to an outer input/output interface adaptation layer
that includes, e.g., the persistence mechanism (an output interface adapter) and controller (an input interface adapter), which can be considered as technological details that
have nothing to do with the microservice core. The benefit of this approach is that you can modify technological details without affecting the microservice core. Microservice's input comes from its clients and
output is anything external the microservice needs to access to fulfill requests from the input.

*Robert C. Martin a.k.a. Uncle Bob* uses the term *clean architecture* in his book *Clean Architecture* for this same principle. I do not use
the term *architecture* because I have reserved that term to designate the design of something larger (i.e., a software system) than a single service. Clean microservice design focuses on designing a single (micro)service conducting OOD in a particular fashion.

Clean microservice design is a relatively simple concept. If you have used the single responsibility principle, divided a microservice into layers, and been programming against interfaces (using the dependency inversion
principle), you may have applied the clean microservice design without knowing it. The clean microservice design principle also employs the *adapter pattern* from design patterns discussed later in this chapter. The adapter pattern is used in input and output interface adapters. We can create separate adapter classes for various input sources and output destinations. Many resources (books, websites, etc.) can explain the clean architecture in rather complex terms.

{aside}
If you are interested, there are similar concepts to *clean architecture* called [hexagonal architecture](https://en.wikipedia.org/wiki/Hexagonal_architecture_(software)) and *onion architecture*.
They have the same basic idea of separating technology-related code from the business logic code, making it easy to change technological aspects of the
software without modifications to the business logic part. They can use different terms, and in hexagonal architecture, you
have co-centric hexagons instead of circles like in clean architecture, but the basic idea in all of them is the same.
{/aside}

Clean microservice design comes with the following benefits for the service:

- Not tied to any single framework
- Not tied to any single API technology like REST or GraphQL
- Unit testable
- Not tied to a specific client (works with web, desktop, console, and mobile clients)
- Not tied to a specific database or other storage technology
- Not dependent on any specific external service implementation

A clean API microservice design consists of the following layers:

- Input and output interface adapters (e.g., controllers, repositories, etc.)
- Use cases (i.e., the features the microservice exposes outside)
- (Business) Entities

Use cases and entities together form the *core* or *model* of the service, also called the *business logic*.
The outermost layer of the use case layer usually contains application service classes that implement the use cases. For example, one method
in a service class implements one use case. The use case layer can contain other layers of software needed to implement the
use cases. The service classes serve as a facade to those other layers. The service classes (i.e., the facade) are meant to be used by the *input
interface adapters*, i.e., the *controllers*. A controller is a common term used to describe an input interface adapter. The controller should delegate to application service classes. It coordinates and controls the activity but should not do much work itself.
*Controller* is a pattern from GRASP principles. Similarly, a repository
is a common term used to describe an output interface adapter that stores information, usually in persistent storage.

![Clean Microservice Design](resources/chapter2/images/clean_arch.png)

The direction of dependencies in the above diagrams is shown with arrows. We can see that the microservice's clients depend
on the input interface adapter or *controller* we create. The controller depends on the use cases. The use case layer depends on (business) entities.
The purpose of the use case layer is to orchestrate operations on the (business) entities. In the above figure,
the parts of software that tend to change most often are located at the outer layers (e.g., controller technology like REST, GraphQL, and database)
The most stable part of the software is located at the center (entities).

Let's have an example of an entity: a bank account.
We know it is something that doesn't change often. It has a couple of key attributes: owner, account number, interest rate, and balance (and probably some other attributes),
but what a bank account is or does has remained the same for tens of years. But we cannot say the same for API technologies or database technologies.
Those are things that change at a much faster pace compared to bank accounts.
Because of the direction of dependencies,
changes in the outer layers do not affect the inner layers. The clean microservice design allows for easy API technology
and database change, e.g., from REST to gRPC or SQL to NoSQL database. All these changes can be made
without affecting the business logic (use case and entities layers).

Put entity-related business rules into entity classes. A `BankAccount` entity class should have
a method for withdrawing money from the account. That method should enforce a business rule: Withdrawal is possible only if the account has enough funds.
Don't put the withdrawal functionality into a service class and use `BankAccount`'s `get_balance` and `set_balance` accessors to perform the withdrawal
procedure in the service class. That would be against the *tell, don't ask principle* discussed later in this chapter. Also, don't allow service classes to access sub-entities (and their methods) a `BankAccount` possibly contains. That would be against DDD principles and the *law of demeter* (discussed later in this chapter). DDD states that you should access an aggregate (`BankAccount`)  by its root only, not directly accessing any sub-entities.

Services orchestrate operations on one or more entities, e.g., a `BacklogItemService` can
have a `setSprint` method that will fetch a `Sprint` entity and pass it to the `BacklogItem` entity's `setSprint` method, which verifies
if the sprint for the backlog item can be set (only non-closed sprint can be set, i.e., current or future, not past sprints; a new sprint for a closed backlog item cannot be set. The closed backlog item has to remain in the sprint where it was closed.)

Let's assume that the `BacklogItem` entity class is large and the `setSprint` would contain a lot of validation code, making it even larger. In that case, you should extract a new class for backlog item sprint validation business rules, and the `BacklogItem` class should be composed of that new behavioral class: `BacklogItemSprintValidator`. In the `BacklogItem` class's `setSprint` method, the validation can be done with the following call: `sprintValidator.validate(newSprint, this)`.

### Real-Life Example

Let's have a real-life example of creating an API microservice called *order-service*, which handles orders in an
e-commerce software system. First, we define our input interface adapter, which is a REST API controller using [FastAPI](https://fastapi.tiangolo.com/):

{format: python}
![controllers/RestOrderController.py](resources/chapter2/code/orderservice/controllers/RestOrderController.py)

The API offered by the microservice depends on the controller, as seen in the earlier diagram. The API is currently a REST API, but
we could create and use a GraphQL controller. Then, our API, which depends on the controller, is a GraphQL API.
You can create a controller for any client-server technology, like gRPC or WebSocket. You can even create a controller for standard input (stdin) or command line interface (CLI). A CLI controller reads command(s) from the command line arguments supplied to the microservice. Remember that you can have multiple controllers active in the same microservice. Your microservice could be used by frontend clients using a REST controller, or it could be used on the command line using its CLI controller.

As an example, below is a partial implementation of a GraphQL controller using FastAPI and [Strawberry](https://strawberry.rocks/) GraphQL library:

{format: python}
![controllers/GraphQlOrderController.py](resources/chapter2/code/orderservice/controllers/GraphQlOrderController.py)

The `RestOrderController` and `GraphQlOrderController` classes depend on the `OrderService` interface, which is
part of the use case layer. Notice that the controllers do not rely on a concrete implementation of the use
cases but depend on an interface according to the *dependency inversion principle*. Below is the definition for
the `OrderService` protocol:

{format: python}
![services/OrderService.py](resources/chapter2/code/orderservice/services/OrderService.py)

The below `OrderServiceImpl` class implements the `OrderService` protocol:

{format: python}
![services/OrderServiceImpl.py](resources/chapter2/code/orderservice/services/OrderServiceImpl.py)

The `OrderServiceImpl` class has a dependency on an order repository. This dependency is also inverted. The `OrderServiceImpl` class depends only on the `OrderRepository` interface. The order repository is used to orchestrate the persistence of order entities.
Note that there is no direct dependency on a database. The term *repository* is abstract. It only means a place where data (entities) are stored. So the
repository can be implemented by a relational database, NoSQL database, file system, in-memory cache, message queue, or another microservice, to name a few.

Below is the `OrderRepository` protocol:

{format: python}
![repositories/OrderRepository.py](resources/chapter2/code/orderservice/repositories/OrderRepository.py)

The `OrderRepository` interface depends only on the `Order` entity class. You can introduce an *output interface adapter* class
that implements the `OrderRepository` interface. An output interface adapter adapts a particular concrete output destination (e.g., a database) to the `OrderRepository` interface.
Entity classes do not depend on anything except other entities to create hierarchical entities (aggregates). For example, the `Order`
entity consists of `OrderItem` entities. Let's introduce an `OrderRepository` *output interface adapter* class for an SQL database:

{format: python}
![repositories/SqlOrderRepository.py](resources/chapter2/code/orderservice/repositories/SqlOrderRepository.py)

The above class requires the used database service to be configured in an environment variable named `DATABASE_URL`.
For a local MySQL database, you could use:

```bash
# Assuming username:password to be root:password
export DATABASE_URL=mysql://root:password@localhost:3306/orderservice
```

{aside}
If you are interested in the implementation of the `to_entity_dict` method, please check Appendix A.
{/aside}

Changing the database to MongoDB can be done by creating a new output interface adapter class that implements the `OrderRepository` interface. We will implement a MongoDB repository in the coming *database principles* chapter.

![Clean Microservice Design for Order Service](resources/chapter2/images/04-05.png)

When implementing a clean microservice design, everything is wired together using configuration and dependency injection.
Dependency injection is configured using the [dependency-injector](https://python-dependency-injector.ets-labs.org/) library and by defining a `DiContainer` class:

{format: python}
![DiContainer.py](resources/chapter2/code/orderservice/DiContainer.py)

If we want to change something in our microservice, we can create a new class and use that new class in the `DiContainer`.
We could create a new repository class for a different type of database, create a new service class that implements
part of services locally and part of them remotely, or introduce a new controller for gRPC, for example. All these changes
would be according to the *open-closed principle* because we are not modifying any existing classes (except for the `DiContainer` of course)
but we are extending our application by introducing new classes that implement existing interfaces.

In the *app.py* file, we create an instance of the DI container, create the FastAPI app, define an error handler for mapping
business errors to HTTP responses and finally wire the wanted controller to the FastAPI app:

{format: python}
![app.py](resources/chapter2/code/orderservice/app.py)

{aside}
Please check Appendix A if you are interested in the rest of the code (i.e., DTOs, entities, GraphQL schema (= types), and errors. When defining an identifier field for an entity/DTO, you should define it as a string. This will allow passing large integer values (64-bit or larger) to JavaScript clients. You can also use non-integer identifiers that have a string representation, like MongoDB object ids or UUIDs.
Let's say you have a repository interface implemented by a file system repository. In that case, you must
have a way to generate entity ids. You can use UUIDs or store the latest used id in the file system and use that value as the basis
when getting a new id value.
{/aside}

The dependency injection container is the only place in a microservice that contains references to concrete implementations.
The *dependency injection principle* is discussed more in a later section of this chapter. The dependency inversion principle
and dependency injection principle usually go hand in hand. Dependency injection is used for wiring interface dependencies
so that those become dependencies on concrete implementations, as shown in the figure below.

![Fig 3.4 Dependency Injection](resources/chapter2/images/di.png)

Let's add a feature where the shopping cart is emptied when an order is created:

{format: python}
![services/ShoppingCartOrderService.py](resources/chapter2/code/orderservice/services/ShoppingCartOrderService.py)

As you can see from the above code, the `ShoppingCartOrderService` class does not depend on any concrete implementation of the
shopping cart service. We can create an *output interface adapter* class that is a concrete implementation of the `ShoppingCartService` interface.
For example, that interface adapter class connects to a particular external shopping cart service via a REST API. Once again,
the dependency injector will inject a concrete `ShoppingCartService` implementation to an instance of the `ShoppingCartOrderService` class.

Note that the above `create_order` method is not production quality because it lacks a transaction.

Now we have seen examples of the following benefits of clean microservice design:

- Not tied to any single API technology like REST or GraphQL
- Not tied to a specific client (works with web, desktop, console, and mobile clients)
- Not tied to a specific database
- Not dependent on any specific external service implementation

Let's showcase the final benefit:

- Not tied to any single framework

Let's change the used web framework from FastAPI to [Flask](https://flask.palletsprojects.com/). What we need to do is to create a Flask-specific version of the `RestOrderController`:

{format: python}
![controllers/FlaskRestOrderController.py](resources/chapter2/code/orderservice/controllers/FlaskRestOrderController.py)

And finally, we shall create a Flask-specific application file *app_flask.py*:

{format: python}
![app_flask.py](resources/chapter2/code/orderservice/app_flask.py)

We were able to change the used web framework by introducing two new small modules. We did not touch any existing modules. We can be confident that we did not break any existing functionality. We were once again successfully applying the *open-closed principle*
to our codebase.

It should be noted that the clean microservice design principle applies to other microservices with input or output, not just API microservices. We will have an example of this later in this chapter.

## Vertical Slice Design Principle

*Vertical slice design* is also known as *vertical slice architecture*. Vertical slice design divides individual features into separate vertical slices.
In the source code repository, this means a separate directory for each vertical slice (feature).
You can also group these single feature directories under a domain directory. This makes navigating the code-base a breeze and finding a particular feature easy.
The main benefit of vertical slice design is that you can follow the open-closed principle when adding new features. There is no need to modify existing code and no risk of breaking existing functionality.
You can implement a vertical slice (a feature) as you wish, which is a clear benefit when different features have divergent requirements.
So, a vertical slice can contain a single class or multiple classes that form a layered design. Vertical slice design does not enforce any specific way.
However, using some design principles for a vertical slice implementation is good practice. I suggest to use the *clean microservice design*.
You can use vertical slice design and clean microservice design principles together. They are not mutually exclusive principles.

If your microservice is a small API, e.g., just containing a handful of simple CRUD operations, you don't necessarily get much benefit from vertical slicing. If you are not using the vertical slice design, adding a new CRUD operation requires changes to existing classes. These changes include adding new methods to mostly immutable or stateless classes,
which can be considered extensions instead of modifications and are according to the open-closed principle. Adding a method
to an immutable or stateless class rarely breaks existing functionality in
existing methods. This is why I haven't used vertical slicing in the examples in this book.

For example, suppose you have an API
providing CRUD operations on a resource and want to use vertical slicing. In that case, you implement each CRUD operation separately
in different service/controller/repository classes. Here is an example of the source code directory layout for the *order-service* using
vertical slice design:

```
order-service
 src
     order
         common
            dtos
               InputOrder.py
               OutputOrder.py
            entities
                Order.py
         create
            CreateOrderRepository.py
            CreateOrderService.py
            CreateOrderServiceImpl.py
            RestCreateOrderController.py
            SqlCreateOrderRepository.py
         get
            GetOrderRepository.py
            GetOrderService.py
            GetOrderServiceImpl.py
            RestGetOrderController.py
            SqlGetOrderRepository.py
         update
            UpdateOrderRepository.py
            UpdateOrderService.py
            UpdateOrderServiceImpl.py
            RestUpdateOrderController.py
            SqlUpdateOrderRepository.py
         delete
             DeleteOrderRepository.py
             DeleteOrderService.py
             DeleteOrderServiceImpl.py
             RestDeleteOrderController.py
             SqlDeleteOrderRepository.py
```

## Class Organization Principle

A class should be organized in the following manner:

- Attributes first, methods after them
- Method order: constructor first, then public, then protected, and private last
- Order attributes and public methods by importance or logic, and if there is no logic or importance difference, order alphabetically
- Define public getters/properties/setters after other public methods
- Order private methods in the same order they are used in the public methods

An example of the *logic* is a `Rectangle` class with `width` and `height` attributes. You should give the `width` (x-axis length) before
`height` (y-axis length) because coordinates are given `x` first and then `y`.

Let's have an example with a `Circle` class.
It has the following attributes:

- `origin` (vital because you cannot draw a circle without knowing its origin)
- `radius` (vital because you cannot draw a circle without knowing its radius)
- `stroke_color` (this is a must attribute even though a default value could be used)
- `fill_color` (this is an optional attribute; a default value of `None` could be used)

Attributes `origin` and `radius` should be given in that order because you need to know the *origin* before you can start
drawing the circle.

The `Circle` class has the following methods:

- draw (most used method, should be the first)
- calculate_area (calculateXXX methods should be in alphabetical order because there is no logic or importance difference between methods)
- calculate_perimeter
- getters/properties (in the same order as respective attributes)
- setters (in the same order as respective attributes)

Our `Circle` class would like the following:

```python
from Point import Point


class Circle:
    def __init__(
        self,
        origin: Point,
        radius: float,
        stroke_color: str,
        fill_color: str = None
    ):
        self.__origin = origin
        self.__radius = radius
        self.__stroke_color = stroke_color
        self.__fill_color = fill_color

    def draw(self) -> None:
        # ...

    def calculate_area(self) -> float:
        # ...

    def calculate_perimeter(self) -> float:
        # ...

    @property
    def origin(self) -> Point:
        return self.__origin

    @property
    def radius(self) -> float:
        return self.__radius

    @property
    def stroke_color(self) -> str:
        return self.__stroke_color

    @property
    def fill_color(self) -> str:
        return self.__fill_color

    @origin.setter
    def origin(self, origin: Point) -> None:
        self.__origin = origin

    @radius.setter
    def radius(self, radius: float) -> None:
        self.__radius = radius

    @stroke_color.setter
    def stroke_color(self, stroke_color: str) -> None:
        self.__stroke_color = stroke_color

    @fill_color.setter
    def fill_color(self, fill_color: str) -> None:
        self.__fill_color = fill_color
```

## Package, Class and Function Sizing Principle

> ***Use the maximum number of items (5-9) that can be stored in the short-term memory as the maximum size for a package, class, or function.***

A package (or directory) should have a maximum of 5-9 modules. This allows you to find a specific module in the package quickly.
If you have a package with many modules, it can be hard to find a specific module because they are listed in alphabetical order, and
you cannot apply any logical order to them. I usually create packages that may only contain 2 or 3 modules. For example, I could have
a *config* directory that has a *Config.py* module, and under the *config* directory, I could have a *parser* subdirectory that has the
following modules: *ConfigParser.py*, *JsonConfigParser.py* and *YamlConfigParser.py*.

A class should have a maximum of 5-9 attributes and 5-9 methods. If your class has more than 5-9 attributes, extract attributes to a new class. Let's say you have the following class:

```python
class User:
    id: str
    first_name: str
    last_name: str
    street_address: str
    zip_code: str
    city: str
    country: str
    phone_number: str
    email_address: str
    gender: Gender
    username: str
    password: str
```

The above class can be refactored to use *value objects* to reduce the number of attributes:

```python
class Name:
    first_name: str
    last_name: str

class Address:
    street_address: str
    zip_code: str
    city: str
    country: str

class ContactInfo:
    phone_number: str
    email_address: str

class Credentials:
    username: str
    password: str

class User:
    id: str
    name: Name,
    address: Address,
    contact_info: ContactInfo
    gender: Gender
    credentials: Credentials
```

If you have too many methods, use the *extract class* refactoring technique explained
in the next chapter. As a rule of thumb, consider refactoring the class smaller if your class has more than 100-150 lines of code (not counting the import statements).
Here is an example. If you have a software component with 10,000 lines of code, you should have a new class for at least every approx. 200 lines of code, meaning that the total number of classes in the software component should be at least 50, in practice, even more.

A function should have a maximum of 5-9 statements. If you have a longer function, extract a function or functions that the original function calls.
If you have a public method that calls several private methods, keep the number of those private methods small, preferably only one or two.
This is because if you write a unit test for the public method, testing becomes more complex the more private methods also need to be tested indirectly.
More about this topic also in the coming *testing principles* chapter.

A function should have a maximum of 5-9 parameters. You should prefer limiting the maximum number of parameters closer to 5 instead of 9.
You can reduce the number of parameters by using the *introduce parameter object* refactoring technique explained in the next chapter.

## Uniform Naming Principle

> ***Use a uniform way to name interfaces, classes, and functions.***.

This section presents conventions for uniformly naming interfaces, classes, and functions.

### Naming Interfaces and Classes

> ***Classes represent a thing or an actor. They should be named consistently so the class name ends with a noun. An interface represents an abstract thing, actor, or capability. Interfaces representing a thing or an actor should be named like classes but using an abstract noun. Interfaces representing a capability should be named according to the capability.***

When an interface represents an abstract thing, name it according to that abstract thing. For example, if you have a drawing application with
various geometrical objects, name the geometrical object interface `Shape`. It is a simple abstract noun.
Names should always be the shortest, most descriptive ones. There is no reason to name the geometrical object interface
as `GeometricalObject` or `GeometricalShape`, if we can use simply `Shape`.

When an interface represents an abstract actor, name it according to that abstract actor.
The name of an interface should be derived from the functionality it provides. For example, suppose there is a `parse_config` method in the interface. In that case, the interface should be named `ConfigParser`, and if an interface has a `validate_object` method,
the interface should be named `ObjectValidator`. Don't use mismatching name combinations like a `ConfigReader` interface with a `parse_config` method or an `ObjectValidator` interface with a `validate_data` method.

When an interface represents a capability, name it according to that capability. Capability is something that
a concrete class is capable of doing. For example, a class could be sortable, iterable, comparable, equitable,
etc. Name the respective interfaces according to the capability: `Sortable`, `Iterable`, `Comparable`, and `Equitable`.
The name of an interface representing a capability usually ends with *able* or *ing*.

Don't name interfaces starting with the *I* prefix (or any other prefix or postfix). Instead, use an *Impl* postfix for
class names to distinguish a class from an interface, but only when needed. For example, if you have an interface named `ConfigParser` and you have a concrete class implementing the interface and parsing configuration in JSON format, name the class `JsonConfigParser`, not `JsonConfigParserImpl`, because the `Impl` prefix is not needed to distinguish between the interface and implementing class. Remember that you should be programming against
interfaces, and if every interface has its name prefixed with an *I*, it just adds unnecessary noise to the code.

Some examples of class names representing a thing are: `Account`, `Order`, `RectangleShape`, and `CircleShape`. In a class
inheritance hierarchy, the names of classes usually refine the interface name or the base class name. For example,
if there is an `InputMessage` interface, then there can be different concrete implementations (= classes) of the `InputMessage` interface. They can represent an input message from different sources, like `KafkaInputMessage` and `HttpInputMessage`.
And there could be different subclasses for different data formats:  `AvroBinaryKafkaInputMessage` or `JsonHttpInputMessage`.

The interface or base class name should be retained in the class or subclass name. Class names should follow the pattern:
`<class-purpose>` + `<interface-name>` or `<sub-class-purpose>` + `<super-class-name>`, e.g.,
`Kafka` + `InputMessage` = `KafkaInputMessage` and `AvroBinary` + `KafkaInputMessage` = `AvroBinaryKafkaInputMessage`.
Name abstract classes with the prefix `Abstract`.

If an interface or class name is over 20-30 characters long, consider abbreviating one or more words in the name.
The reason for this is to keep the code readable. Very long names are harder to read and slow a developer down. (Remember that
code is more often read than written).

Only use abbreviations that are commonly used and understandable for other developers. If a word does not have a good
abbreviation, don't abbreviate. For example, in the class name `AvroBinaryKafkaInputMessage`, we can only abbreviate
the `Message` to `Msg`. There are no well-established abbreviations for other words in the class name. Abbreviating `Binary` to `Bin`
is questionable because `Bin` could also mean a *bin*. Don't abbreviate a word if you benefit only one or two characters.
For example, there is no reason to abbreviate `Account` to `Accnt`.

Instead of abbreviating, you can shorten a name by dropping one or more words from it, provided readers can still easily understand it. For example, if you have classes `InternalMessage`, `InternalMessageSchema` and
`InternalMessageField`, you could shorten the last two class names to `InternalSchema` and `InternalField`.
This is because these two classes are mainly used in conjunction with the `InternalMessage` class: An `InternalMessage` object has
a schema and one or more fields. You can also use nested classes: `InternalMessage.Schema` and `InternalMessage.Field`. The problem with nested classes is that they can make the module too large.

If you have related classes and one or more class names require shortening, you should shorten all related class names
to keep the naming uniform. For example, if you have two classes, `ConfigurationParser` and `JsonConfigurationParser`, you
should shorten the names of both classes, not only the one longer than 19 characters. The new class names would be `ConfigParser` and `JsonConfigParser`.

If an interface or class name is less than 20 characters long, there is usually no need to shorten it.

Don't add a design pattern name to a class name if it does not bring any real benefit. For example, suppose we have
a `DataStore` interface, a `DataStoreImpl` class, and a class wrapping
a `DataStore` instance using the *proxy pattern* to add caching functionality to the wrapped data store. We should not name the
caching class `CachingProxyDataStore` or `CachingDataStoreProxy`. The word *proxy* does not add significant value,
so the class should be named simply `CachingDataStore`. That name clearly tells it is a question about a data store
with caching functionality. A seasoned developer notices from the `CachingDataStore` name that the class uses the *proxy pattern*. And if not,
looking at the class implementation will finally reveal it.

### Naming Functions

> ***Functions should do one thing, and the name of a function should describe what the function does. The function name usually contains a verb that indicates what the function does. The function name often starts with a verb, but exceptions exist. If a function returns a value, try to name the function so that the function name describes what it returns.***

The general rule is to name a function so that the purpose of the function is clear. A good function name should not
make you think. If a function name is *20 or more characters long*, consider abbreviating one or more words in the name.
The reason for this is to keep the code readable. Very long names are harder to read and slow a developer down. (Remember that
code is more often read than written). Only use abbreviations that are widely used and understandable for other developers.
If a word does not have a good abbreviation, don't abbreviate.

Below is an example of a protocol containing two methods named with simple verbs only. It is not necessary to name the methods as
`start_thread` and `stop_thread` because the methods are already part of the `Thread` interface, and it is self-evident what the `start` method
starts and what the `end` method ends. You don't need to repeat the class name in the method name.

```python
from typing import Protocol


class Thread(Protocol):
    def start(self) -> None:
        pass

    def stop(self) -> None:
        pass
```

Many languages offer streams that can be written to, like the standard output stream. Streams are
usually buffered, and the actual writing to the stream does not happen immediately. For example, the below
statement does not necessarily write to the stream immediately. It buffers the text to be written later
when the buffer is flushed to the stream. This can happen when the buffer is full, some
time has elapsed since the last flush, or when the stream is closed.

```python
stream.write(...)
```

The above statement is misleading and could be corrected by renaming the function to describe what it actually does:

```python
stream.write_on_flush(...)
```

The above function name immediately tells a developer that writing happens only on flush, and the developer can consult
the function documentation to determine when the flushing happens.

You can introduce a convenience method to perform a write with an immediate flush:

```python
# Instead of this:
stream.write_on_flush(...)
stream.flush()

# User can do this:
stream.write_with_flush(...)
```

Many times, the function's action is associated with a target, for example:

```python
from typing import Protocol


class ConfigParser(Protocol):
    def try_parse_config(self, config_json: str) -> Config:
        pass
```

When a function's action has a target, it is useful to name the function using the following pattern: `<action-verb>` +
`<action-target>`, for example, `parse` + `config` = `parse_config`.

We can drop the action target from the function name if the function's first parameter describes the action target.
It is not wrong to keep the action target in the function name, though. But if it can be dropped, it usually
makes the function call statements read better. In the below example, the word "config" appears repeated: `try_parse_config(config_json)`,
which makes the function call statement read a bit clumsy.

```python
config = config_parser.try_parse_config(config_json)
```

We can drop the action target from the function name:

```python
class ConfigParser(Protocol):
    def try_parse(self, config_json: str) -> Config:
        pass
```

As shown below, this change makes the code read better, presuming we use a descriptive variable name. And we should,
of course, always use a descriptive variable name.

```python
config = config_parser.try_parse(config_json);
```

Here is another example:

```python
from typing import Protocol, TypeVar

T = TypeVar('T')


class Vector(Protocol[T]):
    # OK
    def push_back(self, value: T) -> None:
        pass

    # Not ideal,
    # word "value" repeated
    def push_back_value(self, value: T) -> None:
        pass
```

Let's imagine we have the following function:

```python
class KafkaAdminClient:
    @staticmethod
    def create(self, topic: str) -> None:
        # ...
```

The above function name should be used only when a topic is the only thing a Kafka admin client can create because
Python does not support method overloading.

There are two correct ways to call the method:

```python
KafkaAdminClient.create(topic='xyz')
```

or

```python
topic = "xyz"
KafkaAdminClient.create(topic)
```

#### Preposition in Function Name

> ***Use a preposition in a function name when needed to clarify the function's purpose.***

You don't need to add a preposition to a function name if the preposition can be assumed (i.e., the preposition is implicit).
In many cases, only one preposition can be assumed. If you have a function named `wait`, the preposition `for` can be assumed,
and if you have a function named `subscribe`, the preposition `to` can be assumed. We don't need to use function names
`wait_for` and `subscribe_to`.

Suppose a function is named `laugh(person: Person)`. Now, we have to add a preposition because none can be assumed.
We should name the function either `laugh_with(person: Person)` or `laugh_at(person: Person)`.

Let's analyze the Python's list methods and see how well they are named:

`list.append(item)`
: This tells clearly where the value is put in the list (at the end). This reads well.

`list.remove(item)`
: This method removes only the first item found in the list. Therefore, it should be named `remove_first(item)`. This method can also raise an exception. We should communicate that in the method signature. Let's use a *try* prefix: `list.try_remove_first(item)`. The next chapter will discuss exception handling and *try* prefix more.

`list.pop(index)`
: This reads well. We can easily assume an `at` preposition after the `pop` word.

`list.index(item)`
: The word *index* is not a verb here. We could add a correct verb and inform the user that the first index of the found item is returned. This method can also raise an exception. We should communicate that in the method signature (try-prefix). This method could be renamed to `list.try_find_first_index(item)`.

`list.count(item)`
: This reads well. We can easily assume an `of` preposition after the `count` word.

`list.sort()`
: This is perfect. It informs that the list is sorted in place. If the method returned a new sorted list, it should be called `list.sorted()`

`list.reverse()`
: This is perfect. It informs that the list is reversed in place. If the method returned a new reversed list, it should be called `list.reversed()`

`list.copy()`
: The word *copy* is strongly associated with copying from one place to another. I would rename this method to `list.clone()`

#### Naming Method Pairs

Methods in a class can come in pairs. A typical example is a pair of getter and setter methods. Name the methods logically when you define
a method pair in a class. The methods in a method pair often do two opposite things, like
getting or setting a value. If you are unsure how to name one of the methods, try to find an antonym for a word.
For example, if you have a method whose name starts with "create" and are unsure how to name the method for the opposite action,
try a Google search: "create antonym".

Here is a non-comprehensive list of some method names that come in pairs:

- get/put (e.g., when accessing a non-sequence collection like set or map)
- read/write
- add/remove
- store/retrieve
- open/close
- load/save
- initialize/destroy
- create/destroy
- insert/delete
- start/stop
- pause/resume
- start/finish
- increase/decrease
- increment/decrement
- construct/destruct
- encrypt/decrypt
- encode/decode
- obtain/relinquish
- acquire/release
- reserve/release
- startup/shutdown
- login/logout
- begin/end
- launch/terminate
- publish/subscribe
- join/detach
- &lt;something&gt;/un&lt;something&gt;, e.g., assign/unassign, install/uninstall, subscribe/unsubscribe, follow/unfollow
- &lt;something&gt;/de&lt;something&gt;, e.g., serialize/deserialize, allocate/deallocate
- &lt;something&gt;/dis&lt;something&gt;, e.g., connect/disconnect

Let's have a couple of examples from real life. The `apt` tool in Debian/Ubuntu-based Linux has an `install` command to
install a package, but the command for uninstalling a package is `remove`. It should be `uninstall`. The Kubernetes package
manager Helm has this correct. It has an `install` command to install a Helm release and an `uninstall` command to uninstall it.

#### Naming Boolean Functions (Predicates)

> ***The naming of boolean functions (predicates) should be such that when reading the function call statement,***
> ***it reads as a boolean statement that can be true or false.***

In this section, we consider naming functions that are predicates and return a boolean value. Here, I don't mean functions that
return true or false based on the success of the executed action, but cases where the function call is used
to evaluate a statement as true or false. The naming of boolean functions should be such that when
reading the function call statement, it makes a statement that can be true or false. Below are some examples:

```python
class Response:
    def has_error(self) -> bool:
        # ...


class String:
    def is_empty(self) -> bool:
        # ...

    def starts_with(self, another_string: str) -> bool:
        # ...

    def ends_with(self, another_string: str) -> bool:
        # ...

    def contains(self, another_string: str) -> bool:
        # ...


# Here we have a statement: response has error? True or false?
if response.has_error():
    # ...

# Here we have a statement: line is empty? True or false?
line: String = file_reader.read_line()
if line.is_empty():
    # ...

# Here we have statement: line starts with a space character?
# True or false?
if line.starts_with(' '):
    # ...

# Here we have statement: line ends with a semicolon?
# True or false?
if line.ends_with(";"):
    # ...


class Thread:
    def should_terminate(self) -> bool:
        # ...

    def is_paused(self) -> bool:
        # ...

    def can_resume_execution() -> bool:
        # ...

    def run(self) -> None:
        # ...

        # Here we have statement: self should terminate?
        # True or false?
        if self.should_terminate():
            return


        # Here we have statement: self is paused and
        # self can resume execution? True or false?
        if self.is_paused() and self.can_resume_execution():
            # ...

        # ...
```

A boolean returning function is correctly named when you call the function in code and can read that function call statement
in plain English. Below is an example of incorrect and correct naming:

```python
class Thread:
    # Incorrect naming
    def stopped(self) -> bool:
        # ...

    # Correct naming
    def is_stopped(self) -> bool:
        # ...

if thread.stopped():
    # Here we have: if thread stopped
    # This is not a statement with a true or false answer.
    # It is a second conditional form,
    # asking what would happen if thread stopped.

    # ...


# Here we have statement: if thread is stopped
# True or false?
if thread.is_stopped():
    # ...
```

From the above examples, we can notice that many names of boolean-returning functions start with either
*is* or *has* and follows the below pattern:

- is + &lt;adjective&gt;, e.g. is_open, is_running or is_paused
- has + &lt;noun&gt;

Also, these two forms can be relatively common:

- should + &lt;verb&gt;
- can + &lt;verb&gt;

But as we saw with the `starts_with`, `ends_with`, and `contains` functions, a boolean returning function name can start
with any verb in third-person singular form (i.e., ending with an *s*). If you have a collection class, its boolean method names
should have a verb in the plural form, for example: `numbers.include(...)` instead of `numbers.includes(...)`. Name your collection variables
always in plural form (e.g., `numbers` instead of `number_list`). We will discuss the uniform naming principles for variables in the next
chapter.

Do not include the *does* word in a function name, like *does_start_with*, *does_end_with*, or *does_contain*.
Adding the *does* word doesn't add any real value to the name, and such function names are awkward to read when used in code, for example:

```python
line = text_file_reader.read_line()

# "If line does start with" sounds awkward
if line.does_start_with(' '):
    # ...
```

When you want to use the past tense in a function name, use a *did* prefix in the function name, for example:

```python
class DatabaseOperation:
    def execute(self) -> None:
        # ...

    # Method name not OK. This is a second conditional form
    # if db_operation.started_transaction(): ...
    def started_transaction(self) -> bool:
        # ...

    # Method name OK, no confusion possible
    def did_start_transaction(self) -> bool:
        # ...
```

#### Naming Builder Methods

A builder class is used to create builder objects that build a new object of a particular type. If you want to
construct a URL, a *UrlBuilder* class can be used for that purpose. Builder class methods add properties to the
built object. For this reason, it is recommended to name builder class methods starting with the verb *add*. The method that
finally builds the wanted object should be named simply *build* or *build + &lt;build-target&gt;*, for example, `build_url`.
I prefer the longer form to remind the reader what is being built. Below is an example of naming the methods in a builder class:

```python
class UrlBuilder:
    def add_scheme(self, scheme: str) -> 'UrlBuilder':
        # ...
        return self

    def add_host(self, host: str) -> 'UrlBuilder':
        # ...
        return self

    def add_port(self, port: int) -> 'UrlBuilder':
        # ...
        return self

    def add_path(self, path: str) -> 'UrlBuilder':
        # ...
        return self

    def add_query(self, query: str) -> 'UrlBuilder':
        # ...
        return self

    def build_url(self) -> Url:
        # ...


url = (
    UrlBuilder().add_scheme('https://').add_host('google.com').build_url()
)
```

#### Naming Methods with Implicit Verbs

Factory method names usually start with the verb *create*. Factory methods can be named so that the *create* verb is implicit, for example:

```python
Optional.of(value)
Optional.empty() # Not optimal, 'empty' can be confused as a verb
Either.with_left(value)
Either.with_right(value)
SalesItem.from_dto(input_sales_item)
```

The explicit versions of the above method names would be:

```python
Optional.create(value)
Optional.create_empty()
Either.create_with_left(value)
Either.create_with_right(value)
SalesItem.create_from_dto(input_sales_item)
```

Similarly, conversion methods can be named so that the *convert* verb is implicit. Conversion methods without a verb
usually start with the *to* preposition, for example:

```python
numeric_value.to_string()
dict_value.to_json()
```

The explicitly named versions of the above methods would be:

```python
numeric_value.convert_to_string()
dict_value.convert_to_json()
```

I recommend using method names with implicit verbs sparingly and only in circumstances where
the implicit verb is self-evident and does not force a developer to think.

#### Naming Lifecycle Methods

Lifecycle methods are called on certain occasions only. Lifecycle method names should answer the question:
When or "on what occasion" will this method be called? Examples of good names for lifecycle methods are: `on_init`, `on_error`,
`on_success`, `after_mount`, `before_unmount`. For example, in React, there are lifecycle methods in class components called `componentDidMount`,
`componentDidUpdate` and `componentWillUnmount`. There is no reason to repeat the class name in the lifecycle method names. Better
names would have been: `afterMount`, `afterUpdate`, and `beforeUnmount`.

#### Naming Function Parameters

Naming rules for function parameters are mostly the same as for variables. *Uniform naming principle* for variables
is described in the next chapter in more detail.

There are some exceptions, like naming object parameters. When a function parameter is an object, the name of the object class can be left out from
the parameter name when the parameter name and the function name implicitly describe the class of the parameter. This exception is acceptable because the function parameter type
can always be easily checked by looking at the function signature.
This should be easily done with a glance because a function should be short (a maximum of 5-9 statements).
Below is an example of naming object type parameters:

```python
# Word 'Location' repeated, not optimal, but allowed
def drive(start_location: Location, destination_location: Location) -> None:
    # ...

# Better way
# When we think about 'drive' and 'start' or 'destination',
# we can assume that 'start' and 'destination' mean locations
def drive(start: Location, destination: Location) -> None:
    # ...
```

Some programming languages like Swift allow adding so-called *external names* to function parameters. Using external names
can make a function call statement read better, as shown below:

```swift
func drive(from start: Location, to destination: Location) {
  // ...
}

func send(
  message: String,
  from sender: Person,
  to recipient: Person
) {
  // ...
}

let start = new Location(...);
let destination = new Location(...);
drive(from: start, to: destination);

let message = "Some message";
let sender = new Person(...);
let recipient = new Person(...);
send(message, from: sender, to: recipient);
```

Always make the function call expression such that it has maximum readability: e.g., `copy(source, target)` not `copy(target, source)`
or `write(line, file)` not `write(file, line)` or `decode(input_message, internal_message)` and `encode(internal_field, output_message)`. The examples contain implicit articles and prepositions. You can
easily imagine the missing articles and prepositions, e.g., *copy from a source to a target*, *write a line to a file* or *encode an internal field to an output message*.

## Encapsulation Principle

> ***A class should encapsulate its state so that access to the state happens only via public methods.***

Encapsulation is achieved by declaring class attributes private. Python does not have attribute access modifiers. You should use a naming convention:

> ***Use an attribute name prefixed with `__` (double underscore) to denote a private attribute and a `_` prefix to denote a protected attribute.***

The problem with the above naming convention is that it hinders the readability of the code. Still, I am using that convention throughout
the book. In my opinion, Python's lack of the `protected` and `private` keywords, i.e., proper encapsulation, is a major problem.

You can create getter and setter methods (or properties) if you need
the state to be modifiable outside the class. However, encapsulation is best ensured if you don't need to create getter
and setter methods for the class attributes. Do not automatically implement or generate getter and setter methods for every class.
Only create those accessor methods if needed, like when the class represents a modifiable data structure. An automatically generated getter can break
the encapsulation of a class if the getter returns modifiable internal state, like a list.
Only generate setter methods for attributes that need to be modified outside the class. If you have a class with many getters,
you might be guilty of *feature envy* code smell, where other objects query your object for its internal state and perform operations
based on that state. You should follow the *tell, don't ask principle* (discussed later in this chapter) by removing the getters from your class and implementing the operation in your class.

### Immutable Objects

The best way to ensure the encapsulation of an object's state is to make the object immutable. This means that once the object
is created, its state cannot be modified afterward. Immutability ensures you cannot accidentally or intentionally modify the object's state.
Modifying the object's state outside the object can be a source of bugs.

When creating an immutable object, you give the needed parameters for the object in the constructor, and after that, those properties
cannot be modified (You don't create any setters for the class). If you need to modify an immutable object, the only way is to create a new object with different values given in the constructor.
The drawback of this approach is that a performance penalty is introduced when
creating new objects as compared to modifying existing objects' attributes only. But in many cases, this penalty is negligible
compared to the benefits of immutability. For example, strings are immutable in Python. Once you create a string, you cannot modify it.
You can only create new strings.

Immutability also requires that getters and other methods returning a value may not return a modifiable attribute, like a list.
If you return a list from a method, that list could be modified by adding or removing elements without
the "owning" object being aware of that.

### Don't Leak Modifiable Internal State Outside an Object Principle

Beware when you return values from methods. It is possible that a method accidentally returns some internal
state of the object that can be modified later by the method caller. Returning modifiable state from a method breaks the encapsulation.

You can safely return an object's internal state when it has a primitive or so-called value type. Those include
bool, int, and float. You can also safely return an immutable object, like a string. But you cannot safely return a mutable collection, for example.

There are two ways to protect against leaking internal state outside an object:

1) Return a copy of the modifiable internal state
2) Return an unmodifiable version of the modifiable internal state

Regarding the first approach, when a copy is returned, the caller can use it as they like.
Changes made to the copied object don't affect the original object. I am primarily talking about making a shallow copy.
In many cases, a shallow copy is enough. For example, a list of primitive values, immutable strings, or immutable objects does
not require a deep copy of the list. But you should make a deep copy when needed.

The copying approach can cause a performance penalty, but in many cases, that penalty is insignificant.
You can easily create a copy of a list:

```python
values = [1, 2, 3, 4, 5]
values2 = values.copy()
```

The second approach requires you to create an unmodifiable version of a modifiable object and return
that unmodifiable object. You can create an unmodifiable version of a class by yourself. Below is an example:

```python
from typing import Protocol, TypeVar

T = TypeVar('T')


class MyList(Protocol[T]):
    def append(self, item: T) -> None:
        # ...

    def get_item(self, index: int) -> T | None:
        # ...


class UnmodifiableMyList(MyList[T]):
    def __init__(self, list_: MyList[T]):
        self.__list = list_

    def append(self, item: T) -> None:
        raise NotImplementedError()

    def get_item(self, index: int) -> T | None:
        return self.__list.get_item(index)
```

In the above example, the unmodifiable list class takes another list (a modifiable list) as a constructor argument. It
only implements the `MyList` protocol methods that don't attempt to modify the wrapped list. In this case, it implements only the
`get_item` method that delegates to the respective method in the `MyList` class. The methods of the `UnmodifiableMyList` class that attempt to modify
the wrapped list should raise an error. The `UnmodifiableMyList` class utilizes the *proxy pattern* (discussed later in this chapter) by wrapping an object of the `MyList` class
and partially allowing access to the `MyList` class methods.

Unmodifiable and immutable objects are slightly different. No one can modify an immutable object, but when you return an
unmodifiable object from a method, the owning class can still modify that object, and modifications are visible to
everyone who has earlier received an unmodifiable version of the object. If this is something undesirable, you should use
a copy instead.

### Don't Assign From a Method Parameter to a Modifiable Attribute

If a class receives modifiable objects as constructor or method arguments, it is typically best practice not to
directly assign those arguments to the internal state. If assigned directly, the class can purposely or accidentally modify those argument objects, which is probably not what the constructor or method caller
expects.

There are two ways to handle this situation:

1) Store a copy of the modifiable argument object to the object's internal state
2) Store an unmodifiable version of the modifiable argument object to the object's internal state. Note that the original modifiable object's owner can modify the object, which is reflected in the unmodifiable object, too. If this is something unwanted, use a copy instead.

Below is an example of the second approach:

```python
class MyClass:
    def __init__(self, values: MyList[int]):
        self.__values = UnmodifiableMyList(values)
```

## Prefer Composition Over Inheritance Principle

> ***In object-oriented design, like in real life, objects are constructed by constructing larger objects from smaller objects.***
> ***This is called object composition. Prefer object composition over inheritance.***

This principle is presented in the *Design Patterns* book by the *Gang of Four*. An example of composition is a car object composed of an engine and transmission object (to name a few).
Objects are rarely "composed" by deriving from another object, i.e., using inheritance. But first, let's try to specify
classes that implement the below `Car` protocol using inheritance and see where it leads us:

```python
from typing import Protocol


class Car(Protocol):
    def drive(self, start: Location, destination: Location) -> None:
        pass


class CombustionEngineCar(Car):
    def drive(self, start: Location, destination: Location) -> None:
        # ...


class ElectricEngineCar(Car):
    def drive(self, start: Location, destination: Location) -> None:
        # ...


class ManualTransmissionCombustionEngineCar(CombustionEngineCar):
    def drive(self, start: Location, destination: Location) -> None:
        # ...


class AutomaticTransmissionCombustionEngineCar(CombustionEngineCar):
    def drive(self, start: Location, destination: Location) -> None:
       # ...
```

If we wanted to add other components to a car, like a two or four-wheel drive, the number of classes
needed would increase by three. If we wanted to add a design property (sedan, hatchback, wagon, or SUV) to a car,
the number of needed classes would explode, and the class names would become ridiculously long, like `HatchbackFourWheelDriveAutomaticTransmissionCombustionEngineCar`.
We can notice that inheritance is not the correct way to build more complex classes here.

Class inheritance creates an *is-a* relationship between a superclass and its subclasses. Object composition creates a *has-a*
relationship. We can claim that `ManualTransmissionCombustionEngineCar` *is a* kind of `CombustionEngineCar`, so basically, we
are not doing anything wrong here, one might think. But when designing classes, you should first determine if object composition
could be used: is there a *has-a* relationship? Can you declare a class as an attribute of another class?
If the answer is yes, then composition should be used instead of inheritance.

All the above things related to a car are actually properties of a car. A car *has an* engine.
A car *has a* transmission. It *has a* two or four-wheel drive and design. We can turn the inheritance-based solution into a composition-based solution:

```python
from typing import Protocol


class Drivable(Protocol):
    def drive(self, start: Location, destination: Location) -> None:
        pass


class Engine(Protocol):
    # Methods like start, stop ...


class CombustionEngine(Engine):
    # Methods like start, stop ...


class ElectricEngine(Engine):
    # Methods like start, stop ...


class Transmission(Protocol):
    # Methods like shift_gear ...


class AutomaticTransmission(Transmission):
    # Methods like shift_gear ...


class ManualTransmission(Transmission):
    # Methods like shift_gear ...


# Define DriveType here...
# Define Design here...


class Car(Drivable):
    def __init__(
        self,
        engine: Engine,
        transmission: Transmission,
        drive_type: DriveType,
        design: Design
    ):
        self.__engine = engine
        self.__transmission = transmission
        self.__drive_type = drive_type
        self.__design = design

  def drive(self, start: Location, destination: Location) -> None:
        # To implement functionality, delegate to
        # component classes, for example:

        # self.__engine.start()
        # self.__transmission.shift_gear(...)
        #  ...
        # self.__engine.stop()
```

Let's have a more realistic example with different chart types. At first, this sounds
like a case where inheritance could be used: We have some abstract base charts that different concrete charts extend, for example:

```python
from abc import abstractmethod
from typing import Protocol


class Chart(Protocol):
    def render_view(self) -> None:
        pass

    def update_data(self, ...) -> None:
        pass


class AbstractChart(Chart):
    @abstractmethod
    def render_view(self) -> None:
        pass

    @abstractmethod
    def update_data(self, ...) -> None:
        pass

    # Implement some common functionality
    # shared by all chart types


class AbstractXAxisChart(AbstractChart):
    @abstractmethod
    def render_view(self) -> None:
        pass

    def update_data(self, ...) -> None:
        # This is common for all x-axis charts,
        # like ColumnChart, LineChart and AreaChart


class ColumnChart(AbstractXAxisChart):
    def render_view(self) -> None:
        # Render column chart using library XYZ


# LineChart class definition here...
# AreaChart class definition here...


class AbstractNonAxisChart(AbstractChart):
    @abstractmethod
    def render_view(self) -> None:
        pass

    def update_data(self, ...) -> None:
        # This is common for all non-x-axis charts,
        # like PieChart and DonutChart


class PieChart(AbstractNonAxisChart):
    def render_view(self) -> None:
        # Render pie chart using library XYZ


class DonutChart(PieChart):
    def render_view(self) -> None:
        # Render donut chart using library XYZ
```

The above class hierarchy looks manageable: there should not be too many subclasses that need to be defined. We can, of course,
think of new chart types, like a geographical map or data table for which we could add subclasses. One problem with
a deep class hierarchy arises when you need to change or
correct something related to a particular chart type. Let's say you want to change or correct some behavior
related to a pie chart. You will first check the `PieChart` class to see if the behavior is defined there. If
you can't find what you are looking for, navigate to the base class of the `PieChart` class (`AbstractNonAxisChart`) and look there.
You might need to continue this navigation until you reach the base class where the behavior you want to
change or correct is located. When you are incredibly familiar with the codebase, you might be able to locate the
correct subclass on the first try. But in general, this is not a straightforward task and is a major drawback of deep class hierarchies.

Using class inheritance can
introduce class hierarchies where some classes have significantly more methods than other classes.
For example, in the chart inheritance chain, the `AbstractChart` class probably has significantly more methods than
classes at the end of the inheritance chain. This class size difference creates an imbalance between classes, making it hard to reason about
what functionality each class provides.

Even if the above class hierarchy looks okay at first sight, there is one problem. We have
hardcoded what kind of chart view we are rendering. We use the *XYZ* chart library and render `XYZChart` views. Let's say we would like to introduce another chart library called *ABC*. We want to use both chart libraries
in parallel so that the open-source version of our data visualization application uses the *XYZ* chart library, which is open
source. The paid version of our application uses the commercial *ABC* chart library.
When using class inheritance, we must create new classes for each concrete chart type for the *ABC* chart library.
So, we would have two classes for each concrete chart type, like here for the pie chart:

```python
class XyzPieChart(AbstractXyzNonAxisChart):
    def render_view(self) -> None:
        # Render pie chart using XYZ library


class AbcPieChart(AbstractAbcNonAxisChart):
    def render_view(self) -> None:
        # Render pie chart using ABC library
```

Implementing the above functionalities using composition instead of inheritance has several benefits:

- It is more apparent what behavior each class contains
- There is no significant size imbalance between classes, where some classes are huge and others relatively small
- You can split chart behaviors into classes as you find fit, and is in accordance with the *single responsibility principle*

In the below example, we have split some chart behavior into two types of classes: chart view renderers and chart data factories:

```python
from enum import Enum
from typing import Protocol


class Chart(Protocol):
    def render_view(self) -> None:
        pass

    def update_data(self, ...) -> None:
        pass


# Define ChartData class...
# Define ChartOptions class...


class ChartViewRenderer(Protocol):
    def render_view(self, data: ChartData, options: ChartOptions) -> None:
        pass


class ChartDataFactory(Protocol):
    def create_data(self, ...) -> ChartData:
        pass


class ChartImpl(Chart):
    def __init__ (
        self,
        view_renderer: ChartViewRenderer,
        data_factory: ChartDataFactory,
        options: ChartOptions
    ):
        self.__view_renderer = view_renderer
        self.__data_factory = data_factory
        self.__options = options
        self.__data = None,

    def render_view(self) -> None:
        self.__view_renderer.render_view(self.__data, self.__options)

    def update_data(self, ...) -> None:
        self.__data = self.__data_factory.create_data(...)


class XyzPieChartViewRenderer(ChartViewRenderer):
    def render_view(self, data: ChartData, options: ChartOptions) -> None:
        # Render pie chart with XYZ library


class AbcPieChartViewRenderer(ChartViewRenderer):
    def render_view(self, data: ChartData, options: ChartOptions) -> None:
        # Render pie chart with ABC library


# Define AbcColumnChartViewRenderer class...
# Define XyzColumnChartViewRenderer class...
# Define XAxisChartDataFactory class ...
# Define NonAxisChartDataFactory class ...


class ChartType(Enum):
    COLUMN = 1
    PIE = 2


class ChartFactory(Protocol):
    def create_chart(self, chart_type: ChartType) -> Chart:
        pass


class AbcChartFactory(ChartFactory):
    def create_chart(self, chart_type: ChartType) -> Chart:
        match chart_type:
            case ChartType.COLUMN:
                return ChartImpl(AbcColumnChartViewRenderer(),
                                 XAxisChartDataFactory())
            case ChartType.PIE:
                return ChartImpl(AbcPieChartViewRenderer(),
                                NonAxisChartDataFactory())
            case _:
                raise ValueError('Invalid chart type')


class XyzChartFactory(ChartFactory):
    def create_chart(self, chart_type: ChartType) -> Chart:
        match chart_type:
            case ChartType.COLUMN:
                return ChartImpl(XyzColumnChartViewRenderer(),
                                 XAxisChartDataFactory())
            case ChartType.PIE:
                return ChartImpl(XyzPieChartViewRenderer(),
                                NonAxisChartDataFactory())
            case _:
                raise ValueError('Invalid chart type')
```

The `XyzPieChartViewRenderer` and `AbcPieChartViewRenderer` classes use the *adapter pattern* as they convert
the supplied data and options to an implementation-specific (ABC or XYZ chart library) interface.

We can easily add more functionality by composing the `ChartImpl` class of more classes. There could be, for example, a title
formatter, tooltip formatter class, y/x-axis label formatter, and event handler classes.

```python
class ChartImpl(Chart):
    def __init__ (
        self,
        view_renderer: ChartViewRenderer,
        data_factory: ChartDataFactory,
        title_formatter: ChartTitleFormatter,
        tooltip_formatter: ChartTooltipFormatter,
        x_axis_label_formatter: ChartXAxisLabelFormatter,
        event_handler: ChartEventHandler,
        options: ChartOptions
    ):
        # ...

    # Chart methods ...


class AbcChartFactory(ChartFactory):
    def create_chart(self, chart_type: ChartType) -> Chart:
        match chart_type:
            case ChartType.COLUMN:
                return ChartImpl(AbcColumnChartViewRenderer(),
                                 XAxisChartDataFactory(),
                                 ChartTitleFormatterImpl(),
                                 XAxisChartTooltipFormatter(),
                                 ChartXAxisLabelFormatterImpl(),
                                 ColumnChartEventHandler())

            case ChartType.PIE:
                return ChartImpl(AbcColumnChartViewRenderer(),
                                NonAxisChartDataFactory(),
                                ChartTitleFormatterImpl(),
                                NonAxisChartTooltipFormatter(),
                                NullXAxisLabelFormatter(),
                                NonAxisChartEventHandler())

            case _:
                raise ValueError('Invalid chart type')
```

## Tactical Domain-Driven Design Principle

> ***Domain-driven design (DDD) is a software design approach where software is modeled to match the language of the problem domain that the software tries to solve. DDD is hierarchical. The top-level domain can be divided into subdomains.***

We continue here where we left with strategical DDD in the last chapter. Strategic DDD was about dividing a software system to subdomains and bounded contexts (microservices).
Tactical DDD is about implementing a single bounded context.
Tactical DDD means that the structure of a bounded context and the names appearing in the code (interface, class, function, and variable names)
should match the domain's vocabulary and the ubiquitous language. For example, names like *Account*, *withdraw*, *deposit*,
*make_payment* should be used in a *payment-service* software component.

### Tactical DDD Concepts

Tactical domain-driven design recognizes multiple concepts:

- Entities
- Value Objects
- Aggregates
- Aggregate Roots
- Factories
- Repositories
- Services
- Events

#### Entities

An entity is a domain object that has an identity. Usually, this is indicated by the entity class having some *id* attribute. Examples of entities are an *employee* and a *bank account*. An employee object has an employee id, and a bank account has a number that identifies the bank account. Entities can contain methods that operate on the attributes
of the entity. For example, a bank account entity can have methods *withdraw* and *deposit* that operate on the *balance*
attribute of the entity.

#### Value Objects

Value objects are domain objects that don't have an identity. Examples of value objects are an address or a price object.
The price object can have two attributes: *amount* and *currency*, but it does not have an identity. Similarly an address
object can have the following attributes: *street address*, *postal code*, *city* and *country*.

#### Aggregates

Aggregates are entities composed of other entities and value objects. For example, an *order* entity can have one or more *order item* entities.
Regarding object-oriented design, this is the same as object composition. Each aggregate has a root (entity). The figure below shows a `SalesItem` aggregate. A `SalesItem` entity is an aggregate and aggregate root. It can contain one or more images of the sales item, and it consists of a `Price` value object, which has two attributes: price and currency.

![Aggregate Example](resources/chapter2/images/aggregate.png)

#### Aggregate Roots

Aggregate roots are domain objects that don't have any parent objects. An *order* entity is an aggregate root when it does
not have a parent entity. But an *order item* entity is not an aggregate root when it belongs to an *order*. Aggregate roots
serve as facade objects. Operations should be performed on the aggregate root objects, not directly accessing the
objects behind the facade (e.g., not directly accessing the individual order items, but performing operations on order objects). For example, if you have an aggregate car object containing wheels, you don't operate the wheels outside of the car object. The car
object provides a facade like a *turn* method, and the car object internally operates the wheels, making the car object
an aggregate root. More about the *facade pattern* in a later section of this chapter.

Let's have an example of aggregate roots in a microservice architecture. Suppose we have a bank account, an aggregate root containing transaction entities. The bank account and transaction entities can be handled in different microservices (*account-service*
and *account-transaction-service*), but only the *account-service* can  directly access and modify the transaction entities using
the *account-transaction-service*. Our bounded context is the *account-service*. The role and benefit of an aggregate root are the following:

- The aggregate root protects against invariant violation. For example, no other service should directly remove or add transactions using the *account-transaction-service*. That would break the invariant that the sum of transactions should be the same as the balance of the account maintained by the *account-service*.
- The aggregate root simplifies (database/distributed) transactions. Your microservice can access the *account-service* and let it manage the distributed transactions between the *account-service* and *account-transaction-service*. It's not something that your microservice needs to do.

You can easily split an aggregate root into more entities. For example, the bank account aggregate root
could contain balance and transaction entities. The balance entity could be handled by a separate *account-balance-service*.
Still, all bank account operations must be made to the *bank-account-service*, which will orchestrate, e.g., *withdraw* and
*deposit* operations using the *account-balance-service* and *account-transaction-service*. We can even split the *account-service*
to two separate microservices: *account-service* for account CRUD operations (excluding updates related to balance) and
*account-money-transfer-service* that will handle *withdraw* and *deposit* operations using the two lower-level microservices:
*account-balance-service* and *account-transaction-service*. In the previous chapter, we had an example of the latter case when
we discussed distributed transactions.

#### Actors

Actors perform commands. End-users are actors, but also services can be actors.
For example, in a data exporter microservice, there can be an input message consumer service that has a command
to consume a message from a data source.

#### Factories

In domain-driven design, the creation of domain objects can be separated from the object classes to factories. Factories are
objects that are dedicated to creating objects of a particular type. More about the *factory pattern* in a later section of this chapter.

#### Repositories

A repository is an object with methods for persisting domain objects and retrieving them from a data store (e.g., a database).
Typically, there is one repository for each aggregate root, e.g., an *order repository* for order entities.

#### Services

Services can be divided into *domain* and *application* services. Application services are used to implement business use cases. External clients connect to the application services via input interface adapters.
Domain services contain functionality that is not directly part of any specific object. A domain service is a class that does not represent a
concept in the problem domain. It is also called *pure fabrication* according to [GRASP](https://en.wikipedia.org/wiki/GRASP_(object-oriented_design)) principles.
Services orchestrate operations on aggregate roots. For example, an `OrderService` orchestrates operations on order entities.
An application service typically uses a related repository to perform persistence-related operations. A service can also be seen as an actor with specific command(s).
For example, in a data exporter microservice, there can be an input message consumer service that has a command
to consume a message from a data source.

#### Events

Events are operations on entities and form the business use cases. Services usually handle events. For example, there could be the following events related to order entities: create, update, and cancel an order. These
events can be implemented by having an `OrderService` with the following methods: `create_order`, `update_order`, and
`cancel_order`.

#### Event Storming

*Eevent storming* is a lightweight method a team can use to discover DDD-related concepts in a software component.
The event storming process typically follows the below
steps:

1) Figure out *domain events* (events are usually written in past tense)
2) Figure out *commands* that caused the *domain events*
3) Add *actors*/*services* that execute the *commands*
4) Figure out related *entities*

In event storming, the different DDD concepts like events, commands, actors, and entities are represented with sticky notes in different colors. These sticky notes are put on a wall, and related sticky notes are grouped together, like actor(s), command(s), and entity/entities for a specific
domain event. If you are interested in details of the event storming process, there is a book named *Introducing EventStorming* by *Alberto Brandolini*.

### Tactical DDD Example 1: Data Exporter Microservice

Let's have a DDD example with a microservice for exporting data. Data exporting will be our top-level
domain. The development team should participate in the DDD and
object-oriented design (OOD) process. An expert-level software developer,
e.g., the team tech lead, could do the DDD and OOD alone, but it is not how it should be done. Other team members,
especially the junior ones, should be involved to learn and develop their skills further.

The DDD process is started by first defining the big picture (top-level domain) based on requirements from the product management and the architecture team:

> Data exporter handles data that consists of messages that contain multiple fields. Data exporting should happen from
> an input system to an output system. During the export, various transformations to the data can be made, and
> the data formats in the input and output systems can differ.

Let's start the event-storming process by figuring out the domain events:

1) A message is consumed from the input system
2) The input message is decoded into an internal representation (i.e., an internal message)
3) The internal message is transformed
4) The transformed message is encoded to the wanted output format
5) The transformed message is produced in the output system
6) Configuration is read and parsed

From the above events, we can figure out four subdomains:

- Input (Events 1, 2 and 6)
- Internal Message (Events 2 and 3)
- Transform (Events 3 and 6)
- Output (Events 4, 5 and 6)

![Data Exporter Subdomains](resources/chapter2/images/data_exporter_subdomains.png)

Let's take the first domain event, "Messages are consumed from the input system," and figure out what caused the
event and who was the actor. Because no end-user is involved, we can conclude that the event was caused
by an "input message consumer" *service* executing a "consume message" *command*. This operation creates an "input message" *entity*.
The picture below shows how this would look with sticky notes on the wall.

![Event Storming](resources/chapter2/images/event_storming.png)

When continuing the event storming process further for the *Input* domain, we can figure out that it consists of the following
additional DDD concepts:

- Commands
  - Read input configuration
  - Parse input configuration
  - Consume input message
  - Decode input message
- Actors/Services
  - Input configuration reader
  - Input configuration parser
  - Input message consumer
  - Input message decoder
- Entities
  - Input message
- Value Objects
  - Input configuration

The event-storming process that resulted in the above list of DDD concepts is actually [object-oriented analysis](https://en.wikipedia.org/wiki/Object-oriented_analysis_and_design#Object-oriented_analysis) (OOA).
We got an initial set of objects that our use case needs when implemented. We got all of them only by looking at the domain events that consist of a verb and an object. We just have to figure out the actor that causes the domain event to happen. Many times, it can also be directly inferred from the domain event.

The actors/services are often singleton objects.
Entities and value objects are objects. Commands are the main methods in the actor/service classes. The OOA phase should result
in an initial [class diagram](https://en.wikipedia.org/wiki/Class_diagram) showing the main classes and their relationships
with other classes.

Below is the list of sub-domains, interfaces, and classes in the *Input* domain:

- Input message
    - Contains the message consumed from the input data source
    - `InputMessage` is a protocol that can have several concrete implementations, like `KafkaInputMessage` representing
      an input message consumed from a Kafka data source
- Input message consumer
    - Consumes messages from the input data source and creates `InputMessage` instances
    - `InputMessageConsumer` is a protocol that can have several concrete implementations, like `KafkaInputMessageConsumer` for consuming messages from a Kafka data source
- Input Message decoder
    - Decodes input messages into internal messages
    - `InputMessageDecoder` is a protocol that can have several concrete implementations, like `AvroBinaryInputMessageDecoder`,
      which decodes input messages encoded in Avro binary format. (If you provide multiple implementations for the `InputMessageDecoder` protocol, you must also create a factory class that can create different kinds of input message decoders).
- Input configuration
    - Input configuration reader
        - Reads the domain's configuration
        - `InputConfigReader` is a protocol that can have several concrete implementations, like
          `LocalFileSystemInputConfigReader` or `HttpRemoteInputConfigReader`
    - Input configuration parser
        - Parses the read configuration to produce an `InputConfig` instance
        - `InputConfigParser` is a protocol that can have several concrete implementations, like `JsonInputConfigParser` or `YamlInputConfigParser`
    - `InputConfig` instance contains parsed configuration for the domain, like the input data source type, host, port, and input data format.

![Input Subdomain](resources/chapter2/images/04-02.png)

Next, we should perform [object-oriented design](https://en.wikipedia.org/wiki/Object-oriented_analysis_and_design#Object-oriented_design) (OOD) and design objects in a more detailed way, using various design principles and patterns.
As shown in the below class diagram, we have applied the *dependency inversion / program against interfaces principle* to the result of the earlier OOA phase:

![Input Subdomain Class Diagram](resources/chapter2/images/input_domain.svg)

When applying the event storming process to the *Internal Message* domain, we can figure out that it consists of the following DDD concepts:

- Entities
    - Internal message
    - Internal field
- Aggregate
    - Internal message (consists of fields)
- Aggregate root
    - Internal message

Below is the list of sub-domains, interfaces, and classes in the *Internal Message* domain:

- Internal Message
  - Internal message consists of one or more internal fields
  - `InternalMessage` is an interface for a class that provides an internal representation of an input message
- Internal Field
  - `InternalField` is an interface for classes representing a single field of an internal message

When applying the event storming process to the *Transform* domain, we can figure out that it consists of the following DDD concepts:

- Commands
    - Read transformer configuration
    - Parse transformer configuration
    - Transform message
    - Transform field
- Actors/Services
    - Transformer configuration reader
    - Transformer configuration parser
    - Message transformer
    - Field transformer
- Value objects
    - Transformer configuration

Below is the list of sub-domains, interfaces, and classes in the *Transform* domain:

- Field transformer
    - `FieldTransformers` is a collection of `FieldTransformer` objects
    -  A Field transformer transforms the value of an internal field into the value of an output message field
    - `FieldTransformer` is a protocol that can have several concrete implementations, like
      `FilterFieldTransformer`, `CopyFieldTransformer`, `TypeConversionFieldTransformer` and `ExpressionTransformer`
- Message Transformer
    - `MessageTransformer` takes an internal message and transforms it using field transformers
- Transformer configuration
    - Transformer configuration reader
        - Reads the domain's configuration
        - `TransformerConfigReader` is a protocol that can have several concrete implementations,
          like `LocalFileSystemTransformerConfigReader`
    - Transformer configuration parser
        - Parses read configuration to produce a `TransformerConfig` instance
        - `TransformerConfigParser` is a protocol that can have several concrete implementations,
          like `JsonTransformerConfigParser`
    - `TransformerConfig` instance contains parsed configuration for the *Transformer* domain

![Transform Subdomain](resources/chapter2/images/transformer_subdomain.png)

Below is the class diagram for the Transform subdomain. I have left the configuration part out of the diagram because
it is pretty much the same as the configuration part in the Input domain.

![Transform Subdomain Class Diagram](resources/chapter2/images/transform_domain.svg)

When applying the event storming process to the *Output* domain, we can figure out that it consists of the following DDD concepts:

- Commands
    - Read output configuration
    - Parse output configuration
    - Encode output message
    - Produce output message
- Actors/Services
    - Output configuration reader
    - Output configuration parser
    - Output message encoder
    - Output message producer
- Entities
    - Output message
- Value objects
    - Output configuration

Below is the list of sub-domains, interfaces, and classes in the *Output* domain:

- Output Message encoder
    - Encodes transformed message to an output message with a specific data format
    - `OutputMessageEncoder` is a protocol that can have several concrete implementations, like `CsvOutputMessageEncoder`,
      `JsonOutputMessageEncoder`, `AvroBinaryOutputMessageEncoder`
- Output message
    - `OutputMessage` is a container for an output byte sequence
- Output message producer
    - Produces output messages to the output destination
    - `OutputMessageProducer` is a protocol that can have several concrete implementations, like `KafkaOutputMessageProducer`
- Output configuration
    - Output configuration reader
        - Reads the domain's configuration
        - `OutputConfigReader` is a protocol that can have several concrete implementations,
          like `LocalFileSystemOutputConfigReader`
    - Output configuration parser
        - Parse the read configuration to an `OutputConfig` instance
        - `OutputConfigParser` is a protocol that can have several concrete implementations, like `JsonOutputConfigParser`
    - `OutputConfig` instance contains parsed configuration for the domain, like output destination type, host, port, and the output data format

![Output Subdomain](resources/chapter2/images/output_subdomain.png)

Below is the class diagram for the Output subdomain. I have left the configuration part out of the diagram because
it is pretty much the same as the configuration part in the Input domain.

![Output Subdomain Class Diagram](resources/chapter2/images/output_domain.svg)

The above design also follows the *clean microservice design* principle. Note that this principle applies to all kinds of microservices with input or output, not just APIs. From the above design, we can discover the following interface adapters that
are not part of the business logic of the microservice:

- `InputMessageConsumer` interface implementations
- `InputMessageDecoder` interface implementations
- `OutputMessageEncoder` interface implementations
- `OutputMessageProducer` interface implementations
- `InputConfigReader` interface implementations
- `InputConfigParser` interface implementations
- `TransformerConfigReader` interface implementations
- `TransformerConfigParser` interface implementations
- `OutputConfigReader` interface implementations
- `OutputConfigParser` interface implementations

We should be able to modify the implementations mentioned above or add a new implementation without modifying other parts of the code (the core or business logic). This means that we can easily adapt our microservice
to consume data from different data sources in different data formats and output the transformed data to different data sources
in various data formats. Additionally, the configuration of our microservice can be read from various sources in different formats. For example, if we now read some configuration from a local file in JSON format, in the future, we
could introduce new classes for reading the configuration from an API using some other data format.

After defining the interfaces between the above-defined subdomains, the four subdomains can be developed very much in parallel.
This can speed up the microservice development significantly. The code of each subdomain should be put into separate source
code folders. We will discuss source code organization more in the next chapter.

Based on the above design, the following  data processing pipeline can
be implemented:

```python
class DataExporterApp:
    def run(self) -> None:
        while self.__is_running:
            input_message = (
                self.__input_msg_consumer.consume_input_message()
            )

            internal_message = self.__input_msg_decoder.decode(
                input_message
            )

            transformed_message = self.__msg_transformer.transform(
                internal_message
            )

            output_message = self.__output_msg_encoder.encode(
                transformed_message
            )

            self.__output_msg_producer.produce(output_message)
```

The `transform` method of the `MessageTransformer` class can be implemented in the following way:

```python
class MessageTransformer:
    def transform(
        self, internal_message: InternalMessage
    ) -> InternalMessage:
        transformed_message = InternalMessage()

        for field_transformer in self.__field_transformers:
            field_transformer.transform_field(
                internal_message, transformed_message
            )

        return transformed_message
```

### Tactical DDD Example 2: Anomaly Detection Microservice

Let's have another DDD example with an anomaly detection microservice. The purpose of the microservice is to detect anomalies
in measurement data. This concise description of the microservice's purpose reveals the two subdomains of the microservice:

- Anomaly
- Measurement

Let's first analyze the *Measurement* subdomain in more detail and define domain events for it:

- Measurement data source definitions are loaded
- Measurement data source definitions are parsed
- Measurement definitions are loaded
- Measurement definitions are parsed
- Measurement data is fetched from data sources
- Measurement data is scaled (for further AI processing)

Let's continue using the event storming and define additional DDD concepts:

- Commands
    - Load measurement data source definitions
    - Parse measurement data source definitions
    - Load measurement definitions
    - Parse measurement definitions
    - Fetch measurement data from data sources
    - Scale measurement data
-Actors/Services
    - Measurement data source definitions loader
    - Measurement data source definitions parser
    - Measurement definitions loader
    - Measurement definitions parser
    - Measurement data fetcher
    - Measurement data scaler
- Entities
    - Measurement data source
    - Measurement data
    - Measurement
- Aggregates
    - Measurement
        - Measurement data source
        - Measurement query
        - Measurement data
- Aggregate root
    - Measurement
- Value Objects
    - Measurement query

Let's define domain events for the *Anomaly* subdomain:

- Anomaly detection configuration is parsed
- Anomaly detection configuration is created
- Anomaly detection rule is parsed
- Anomaly detection rule is created
- Anomalies are detected in a measurement according to the anomaly detection rule using a trained anomaly model
- Anomaly detection is triggered at regular intervals
- Anomaly model is trained for a measurement
- Anomaly model is created
- Anomaly model training is triggered at regular intervals
- A detected anomaly (i.e., an anomaly indicator) is created
- A detected anomaly (i.e., an anomaly indicator) is serialized to a wanted format, e.g., JSON
- The detected anomaly (i.e., an anomaly indicator) is published to a specific destination using a specific protocol

Let's continue with the event storming and define additional DDD concepts:

- Commands
    - Parse anomaly detection configuration
    - Create anomaly detection configuration
    - Parse anomaly detection rule definition
    - Create anomaly detection rule
    - Detect anomalies in a measurement according to the anomaly detection rule using a trained anomaly model
    - Trigger anomaly detection at regular intervals
    - Train anomaly model for a measurement using a specific AI technique, like self-organizing maps (SOM)
    - Create an anomaly model
    - Trigger anomaly model training at regular intervals
    - Create anomaly indicator
    - Serialize anomaly indicator
    - Publish anomaly indicator
- Actors/Services
    - Anomaly detection configuration parser
    - Anomaly detection rule parser
    - Anomaly detector
    - Anomaly detection engine
    - Anomaly model trainer (e.g. SOM)
    - Anomaly training engine
    - Anomaly indicator serializer (e.g. JSON)
    - Anomaly indicator publisher (e.g., REST or Kafka)
- Factories
    - Anomaly detection configuration factory
    - Anomaly detection rule factory
    - Anomaly model factory
    - Anomaly indicator factory
- Entities
    - Anomaly detection rule
    - Anomaly model
    - Anomaly indicator

The two domains, anomaly and measurement, can be developed in parallel. The anomaly domain interfaces with the measurement domain
to fetch data for a particular measurement from a particular data source. The development effort of both the
anomaly and measurement domains can be further split to achieve even more development parallelization. For example,
one developer could work with anomaly detection, another with anomaly model training, and the third with anomaly indicators.

If you want to know more about DDD, I suggest you to read *Implementing Domain-Driven Design* by *Vaughn Vernon*.

## Design Patterns

The following sections present 25 design patterns, most of which are made famous by the *Gang of Four* and their book [Design Patterns](https://en.wikipedia.org/wiki/Design_Patterns).
Design patterns are divided into creational, structural, and behavioral patterns.

### Design Patterns for Creating Objects

This section describes design patterns for creating objects. The following design patterns will be presented:

- Factory pattern
- Abstract factory pattern
- Static factory method pattern
- Builder pattern
- Singleton pattern
- Prototype pattern
- Object pool pattern

#### Factory Pattern

> ***Factory pattern allows deferring what kind of object will be created to the point of calling the *create*-method of the factory***.

A factory allows a dynamic way of creating objects instead of a static way by directly calling a concrete class constructor. A factory typically consists of precisely one or multiple methods for creating objects of a particular base type. This base type is usually an interface type. The factory decides what concrete type of object will be created. A factory
separates the logic of creating objects from the objects themselves, which is in accordance with the *single responsibility principle*.

Below is an example `ConfigParserFactory` that has a single `create` method for creating different kinds of
`ConfigParser` objects. The return type of the factory's create method is an interface. This allows different kinds of objects
in the class hierarchy to be created. In the case of a factory with a single create method, the method usually contains a match-case statement or
an if/elif structure. Factories are the only place where extensive match-case statements or if/elif structures
are allowed in object-oriented programming. If you have a lengthy match-case statement or long if/elif structure somewhere
else in the code, that is typically a sign of a non-object-oriented design.

```python
from enum import Enum
from typing import Protocol


class ConfigParser(Protocol):
    # ...


class JsonConfigParser(ConfigParser):
    # ...


class YamlConfigParser(ConfigParser):
    # ...


class ConfigFormat(Enum):
    JSON = 1
    YAML = 2


class ConfigParserFactory:
    @staticmethod
    def create_config_parser(config_format: ConfigFormat) -> ConfigParser:
        match config_format:
            case ConfigFormat.JSON:
                return JsonConfigParser()
            case ConfigFormat.YAML:
                return YamlConfigParser()
            case _:
                raise ValueError('Unsupported config format')
```

Below is an example of a factory with multiple *create* methods:

```python
class ShapeFactory:
    @staticmethod
    def create_circle_shape(radius: int) -> Shape:
        return CircleShape(radius)

    @staticmethod
    def create_rectangle_shape(width: int, height: int) -> Shape:
        return RectangleShape(width, height)

    @staticmethod
    def create_square_shape(side_length: int) -> Shape:
        return SquareShape(side_length)
```

#### Abstract Factory Pattern

> ***In the abstract factory pattern, there is an abstract factory (interface) and one or more concrete factories (classes that implement the factory interface).***

The abstract factory pattern extends the earlier described *factory pattern*. Usually, the abstract factory pattern should be
used instead of the plain factory pattern. Below is an example of an abstract `ConfigParserFactory` with one concrete implementation:

```python
class ConfigParserFactory(Protocol):
    def create_config_parser(self, config_format: ConfigFormat) -> ConfigParser:
        pass


class ConfigParserFactoryImpl(ConfigParserFactory):
    def create_config_parser(self, config_format: ConfigFormat) -> ConfigParser:
        match config_format:
            case ConfigFormat.JSON:
                return JsonConfigParser()
            case ConfigFormat.YAML:
                return YamlConfigParser()
            case _:
                raise ValueError('Unsupported config format')
```

You should follow the *program against interfaces principle* and use the abstract `ConfigParserFactory` in your code instead of
a concrete factory. Then, using the *dependency injection principle*, you can inject the wanted factory
implementation, like `ConfigParserFactoryImpl`.

When unit testing code, you should create mock objects instead of real ones with a
factory. The abstract factory pattern comes to your help because you can supply a mock instance of the `ConfigParserFactory` in the tested code. Then, you can expect the mocked `create_config_parser` method to be called and return a mock instance
conforming to the `ConfigParser` protocol. Then, you can expect the `parse` method to be called on the `ConfigParser` mock and return
a mocked configuration. Below is an example unit test. We test the `initialize` method in the `Application` class containing
a `ConfigParserFactory` type attribute. The `Application` class uses the `ConfigParserFactory` instance to create a `ConfigParser` object
to parse the application configuration.

```python
from typing import Protocol


class Config(Protocol):
    # ...


class ConfigParser(Protocol):
    def parse(self) -> Config:
        pass

    # ...


class ConfigParserFactory(Protocol):
    def create_config_parser(self) -> ConfigParser:
        pass

    # ...


class Application:
    def __init__(self, config_parser_factory: ConfigParserFactory):
        self.__config_parser_factory = config_parser_factory
        self.__config: Config | None = None

    def initialize(self) -> None:
        # ...
        config_parser = self.__config_parser_factory.create_config_parser(...)
        self.__config = config_parser.parse(...)
        # ...

    @property
    def config(self):
        return self.__config
```

Here is the `ApplicationTests` class:

```python
from unittest import main, TestCase
from unittest.mock import Mock

from Application import Application
from Config import Config
from ConfigParser import ConfigParser
from ConfigParserFactory import ConfigParserFactory


class ConfigParserFactoryMock(ConfigParserFactory):
     pass


class ConfigParserMock(ConfigParser):
    pass


class ConfigMock(Config):
    pass


class ApplicationTests(TestCase):
    def test_initialize(self):
        # GIVEN
        config_parser_factory_mock = ConfigParserFactoryMock()
        config_parser_mock = ConfigParserMock()

        config_parser_factory_mock.create_config_parser = Mock(
            return_value=config_parser_mock
        )

        config_mock = ConfigMock()
        config_parser_mock.parse = Mock(return_value=config_mock)
        application = Application(config_parser_factory_mock)

        # WHEN
        application.initialize()

        # THEN
        self.assertEqual(application.config, config_mock)


if __name__ == '__main__':
    main()
```

In the above example, we created mocks manually. The `Mock` constructor creates a mock object that is also callable (i.e., it is also a mock function). You can supply the return value the mock should return when it is called in the `Mock` constructor.
It is also possible to create mocks automatically using the `@patch` decorator, which can make code slightly less verbose, as shown
below:

```python
from unittest import TestCase, main
from unittest.mock import Mock, patch

from Application import Application
from Config import Config
from ConfigParser import ConfigParser
from ConfigParserFactory import ConfigParserFactory


class ApplicationTests(TestCase):
    @patch.object(ConfigParserFactory, '__new__')
    @patch.object(ConfigParser, '__new__')
    @patch.object(Config, '__new__')
    def test_initialize(
        self,
        config_mock: Mock,
        config_parser_mock: Mock,
        config_parser_factory_mock: Mock,
    ):
        # GIVEN
        config_parser_factory_mock.create_config_parser.return_value = (
            config_parser_mock
        )

        config_parser_mock.parse.return_value = config_mock
        application = Application(config_parser_factory_mock)

        # WHEN
        application.initialize()

        # THEN
        self.assertEqual(application.config, config_mock)


if __name__ == '__main__':
    main()
```

Unit testing and mocking are better described later in the *testing principles* chapter.

#### Static Factory Method Pattern

> ***In the static factory method pattern, objects are created using one or more factory methods in a class, and the class constructor is made private.***
> ***The factory methods are usually class methods.***

Static factory methods are useful if you need more than one constructor with different arguments because in Python a class can have only one constructor, the `__init__` methods.

If you want to validate the parameters supplied to a constructor,
the constructor may raise an error. You cannot return an error value from a constructor. Creating constructors that cannot
raise an error is recommended because it is relatively easy
to forget to catch errors raised in a constructor if nothing in the constructor signature tells it can raise an error.
See the next chapter for a discussion about the *error/exception handling principle*.

```python
class Url:
    def __init__(
        self,
        scheme: str,
        port: int,
        host: str,
        path: str,
        query: str
    ):
        # Validate the arguments and raise an error if invalid
```

You can use the static factory method pattern to overcome the problem of raising an error in a constructor.
You can make a factory method to return an optional value (if you don't need to return an error cause) or
make the factory method raise an error. You can add a *try* prefix to the factory method name
to signify that it can raise an error. Then, the function signature (function name) communicates to readers
that the function may raise an error.

Below is an example class with two factory methods. The constructor of the class is made private using a
`PrivateConstructor` metaclass. Users can only create instances of the class by using a factory method.

```python
from typing import Any, TypeVar

T = TypeVar("T")


class PrivateConstructor(type):
    def __call__(
        cls: type[T],
        *args: tuple[Any, ...],
        **kwargs: dict[str, Any]
     ):
        raise TypeError('Constructor is private')

    def _create(
        cls: type[T],
        *args: tuple[Any, ...],
        **kwargs: dict[str, Any]
    ) -> T:
        return super().__call__(*args, **kwargs)


class Url(metaclass=PrivateConstructor):
    def __init__(
        self,
        scheme: str,
        port: int,
        host: str,
        path: str,
        query: str
    ):
        # ...

    @classmethod
    def create_url(
        cls,
        scheme: str,
        port: int,
        host: str,
        path: str,
        query: str
    ) -> 'Url | None':
        # Validate the arguments and return 'None' if invalid
        # If valid return a 'Url' instance:
        # return cls._create(str, port, host, path, query)

    @classmethod
    def try_create_url(
        cls,
        scheme: str,
        port: int,
        host: str,
        path: str,
        query: str
    ) -> 'Url':
        # Validate the arguments and raise an error if invalid
        # If valid return a 'Url' instance:
        # return cls._create(str, port, host, path, query)
```

Returning an optional value from a factory method allows the utilization of functional programming techniques. Python does not
have an optional class, but let's first define an `Optional` class:

```python
from collections.abc import Callable
from typing import Generic, TypeVar

T = TypeVar('T')
U = TypeVar('U')


class Optional(Generic[T], metaclass=PrivateConstructor):
    def __init__(self, value: T | None):
        self.__value = value

    @classmethod
    def of(cls, value: T) -> 'Optional[T]':
        return cls._create(value)

    @classmethod
    def of_nullable(cls, value: T | None) -> 'Optional[T]':
        return cls._create(value)

    @classmethod
    def empty(cls) -> 'Optional[T]':
        return cls._create(None)

    def is_empty(self) -> bool:
        return True if self.__value is None else False

    def is_present(self) -> bool:
        return False if self.__value is None else True

    def try_get(self) -> T:
        if self.__value is None:
            raise ValueError('No value to get')
        return self.__value

    def try_get_or_else_raise(self, error: Exception):
        if self.__value is None:
            raise error
        return self.__value

    def if_present(self, consume: Callable[[T], None]) -> 'Optional[T]':
        if self.__value is not None:
            consume(self.__value)
        return self

    def or_else(self, other_value: T) -> T:
        return other_value if self.__value is None else self.__value

    def or_else_get(self, supply_value: Callable[[], T]) -> T:
        return supply_value() if self.__value is None else self.__value

    def map(self, map_: Callable[[T], U | None]) -> 'Optional[U]':
        return (
            self
            if self.__value is None
            else self.of_nullable(map_(self.__value))
        )

    def flat_map(self, map_: Callable[[T], 'Optional[U]']) -> 'Optional[U]':
        return self if self.__value is None else map_(self.__value)
```

{aside}
**NOTE!** When I use the `Optional` class in this book, it is always the above-defined class, not the `Optional` from
the Python's `typing` module.
{/aside}

Notice how the above `Optional` class code utilized the static factory method pattern. It has a private constructor and
three factory methods to create different kinds of `Optional` objects. The additional benefit of using the static factory method pattern is that you can name
the factory methods descriptively, which you can't do with a single constructor. The name of the factory method tells what
kind of object will be created.

```python
class Url(metaclass=PrivateConstructor):
     def __init__(
          self,
          scheme: str,
          port: int,
          host: str,
          path: str,
          query: str
      ):
          # ...

    @classmethod
    def create_url(
        cls,
        scheme: str,
        host: str,
        port: int,
        path: str,
        query: str
    ) -> Optional['Url']:
        # ...


maybe_url = Url.create_url(...)

# Do something with the URL in lambda
maybe_url.if_present(lambda url: print(url))

def print_url(url: Url):
    print(url)

# Do something with the URL using a function
maybe_url.if_present(print_url)
```

#### Builder Pattern

> ***Builder pattern allows you to construct objects piece by piece.***

In the builder pattern, you add properties to the built object with the *add_xxx* methods of the builder class. After adding all the needed properties, you can build the final object using
the *build* or *build_xxx* method of the builder class.

For example, you can construct a URL from parts of the URL. Below is an example of using a `UrlBuilder` class:

```python
url = UrlBuilder().add_scheme('https').add_host('www.google.com').build_url()
```

The builder pattern has the benefit that properties given for the builder can be validated in the build method. You can make the
builder's build method return an optional indicating whether the building was successful. Or, you can make the build method raise an error if you need to provide
an error. If the build method can raise an error, name it using a *try* prefix, for example, `try_build_url`.
The builder pattern also has the benefit of not needing to add default properties to the builder. For example, *https* could be
the default scheme, and if you are building an HTTPS URL, the `add_scheme` does not need to be called. The only problem is
that you must consult the builder documentation to determine the default values. They are not visible in the API.

One drawback with the builder pattern is that you can give the parameters logically in the wrong order like this:

```python
url = UrlBuilder().add_host('www.google.com').add_scheme('https').build_url()
```

It works, of course, but it does not look so nice. So, if you are using a builder, always try to give the parameters for the builder in a
logically correct order if such an order exists. The builder pattern works well when there isn't any inherent order among the parameters.
Below is an example of such a case: A house built with a `HouseBuilder` class.

```python
house = HouseBuilder()\
  .add_kitchen()\
  .add_living_room()\
  .add_bedrooms(3)\
  .add_bath_rooms(2)\
  .add_garage()\
  .build_house()
```

You can achieve functionality similar to a builder with a factory method with parameters that have default values:

```python
class Url(metaclass=PrivateConstructor):
     def __init__(
          self,
          host: str,
          path: str,
          query: str,
          scheme: str = 'https',
          port: int = 443,
      ):
          # ...

    @classmethod
    def create_url(
        cls,
         host: str,
         path: str,
         query: str,
         scheme: str = 'https',
         port: int = 443,
    ) -> 'Url | None':
        # ...
```

In the factory method above, there is clear visibility of what the default values are. But you cannot now
give the parameters in a logical order. There is also a greater possibility that you accidentally provide some parameters
in the wrong order because many of them are of the same type (string). This won't be a potential issue with
a builder where you use a method with a specific name to give a specific parameter. In modern development environments,
giving parameters in the wrong order is less probable because IDEs offer so-called [inlay hints](https://www.jetbrains.com/help/idea/inlay-hints.html) for parameters. With inlay hints enabled, it is easier to see
if you provide a particular parameter in the wrong position. As shown below, giving parameters in the wrong order can also be avoided using
semantically validated function parameter types. That topic will be discussed later in this chapter when we discuss primitive-type obsession.

```python
class Url(metaclass=PrivateConstructor):
    # ...

    @classmethod
    def create_url(
        cls,
        host: Host,
        path: Path,
        query: Query,
        scheme: Scheme = Scheme.create('https'),
        port: Port = Port.create(443),
    ) -> 'Url | None':
        # ...
```

You can always use a parameter object as an additional alternative to a builder. Below is an example:

```python
from dataclasses import dataclass

from Optional import Optional
from PrivateConstructor import PrivateConstructor


@dataclass
class UrlParams:
    host: str
    scheme: str = 'https'
    port: int = 443
    path: str = ""
    query: str = ""


class Url(metaclass=PrivateConstructor):
    def __init__(self, url_params: UrlParams):
        # ...

    @classmethod
    def create_url(cls, url_params: UrlParams) -> 'Optional[Url]':
        # ...


url_params = UrlParams('www.google.com', query='query=design+patterns')
maybe_url = Url.create_url(url_params)
```

The above solution is quite similar to using a builder.

#### Singleton Pattern

> ***Singleton pattern defines that a class can have only one instance.***

Singletons are very common in pure object-oriented languages like Java. In many cases, a singleton class can be identified as not having any state.
This is why only one instance of the class is needed. There is no point in creating
multiple instances that are the same. In some non-pure object-oriented languages, singletons are not necessarily as common as
in pure object-oriented languages and can often be replaced by just defining functions.

In Python, a singleton instance can be created in a module and exported from it. When you import the singleton instance from the module
in other modules, the other modules will always get the same exported instance, not a new instance every time.
Below is an example of such a singleton. First, we define a singleton instance of the `__MyClass` class in a module named *my_class_singleton.py*. The `__MyClass` class is prefixed with two underscores to denote it is private. The only public thing in the module is the singleton instance.

{title: "my_class_singleton.py"}
```python
class __MyClass:
    # ...

my_class_singleton = __MyClass()
```

And in the *other_module_1.py* module, we import the singleton instance and use it:

{title: "other_module_1.py"}
```python
from my_class_singleton import my_class_singleton

print(my_class_singleton)
```

We use the singleton instance also in *other_module_2.py* module:

{title: "other_module_2.py"}
```python
from my_class_singleton import my_class_singleton
import other_module_1

print(my_class_singleton)
```

When you run the *other_module_2.py*, you should have the following kind of output where the object addresses are
the same meaning that `my_class_singleton` is really a singleton.

```
<my_class_singleton.__MyClass object at 0x101042f90>
<my_class_singleton.__MyClass object at 0x101042f90>
```

The singleton pattern can also be implemented using a class with static methods only. The problem with such a class is that
the singleton class is then hardcoded, and static classes can be hard or impossible to mock in unit testing (at least in some languages). We should remember
to *program against interfaces*. Also, if you try to implement the singleton pattern yourself, you must remember to ensure thread safety in case
of a multi-threaded program when creating the singleton instance. You might be better off using a dependency injection library that can provide
thread-safe singletons out of the box.

The best way to implement the singleton pattern is by using the *dependency inversion principle* and the *dependency injection principle*. Below is
an example using the [dependency-injector](https://python-dependency-injector.ets-labs.org/) library for handling dependency injection. The constructor of the `FileConfigReader` class
expects a `ConfigParser` instance. We annotate the constructor with the `@inject` annotation and provide a `ConfigParser` instance
with the name `config_parser` from the DI container (defined later):

```python
from typing import Final, Protocol

from dependency_injector.wiring import Provide, inject
# Import ConfigParser protocol with try_parse method here...
# Import Config ...


class ConfigReader(Protocol):
    def try_read(self, config_location: str) -> Config:
        pass


class FileConfigReader(ConfigReader):
    @inject
    def __init__(
        self,
         config_parser: ConfigParser = Provide['config_parser']
     ):
        self.__config_parser: Final = config_parser


    def try_read(self, config_file_path_name: str) -> Config:
        config_file_contents = # Read configuration file
        config = self.__config_parser.try_parse(config_file_contents)
        return config
```

In the below `DiContainer` class, we first configure wiring, and then
the name `config_parser` is bound to a singleton instance of the `ConfigParserImpl` class. (The `ConfigParserImpl` class code is not shown here). The `wiring_config`
expects that the `FileConfigReader` class is defined in a module named `FileConfigReader.py`.

```python
from dependency_injector import containers, providers


class DiContainer(containers.DeclarativeContainer):
    wiring_config = containers.WiringConfiguration(
        modules=['FileConfigReader']
    )

    config_parser = providers.ThreadSafeSingleton(ConfigParserImpl)
```

The *dependency-injector* library will ensure that creating the `config_parser` singleton happens
without race conditions in a multiple threads context. You can use `providers.Singleton` if you only have a single thread in your software component.

Another way to ensure that only one singleton instance is created is to ensure the DI container is created at the beginning
of the application initialization (before starting threads) and singletons are created eagerly, not lazily. Eagerly means the singleton is created immediately, and lazily means it is created only when somebody needs it.
A lazy instantiation can bring problems in a multi-threaded environment if synchronization is not used when the singleton instance is actually created.

#### Prototype Pattern

> ***The prototype pattern lets you create a new object using an existing object as a prototype.***

Let's have an example with a `DrawnShape` class:

```python
from typing import Final, Protocol


class Shape(Protocol):
    # ...


# Implement concrete shapes...


class Position(Protocol):
    def get_x(self) -> int:
        pass

    def get_y(self) -> int:
        pass


class DrawnShape:
    def __init__ (self, position: Position, shape: Shape):
        self.__position: Final = position
        self.__shape: Final = shape

    def clone(self, position: Position) -> 'DrawnShape':
        return DrawnShape(position, self.__shape)
```

Applying the prototype pattern is to call the `clone` method on
a prototype object and give the `position` parameter to specify where the new shape should be positioned.

The prototype pattern is also used in JavaScript to implement prototypal inheritance. Since EcmaScript version 6,
class-based inheritance has been available, and prototypal inheritance does not need to be used.

The idea behind prototypal inheritance is that the common parts for the same class objects are stored in a
prototype instance. These common parts typically mean the shared methods. There is no sense in storing the methods multiple
times in each object. That would be a waste of resources because Javascript functions are objects themselves.

When you create a new object with the `Object.create` method, you give the prototype as a parameter. After that,
you can set properties for the newly created object. When you call a method on the created object, and if that method
is not found in the object's properties, the prototype object will be looked up for the method. Prototypes can be chained
so that a prototype object contains another prototype object. This chaining is used to implement an inheritance chain.
Below is a simple example of prototypal inheritance:

```js
const pet = {
  name: '',
  getName: function() { return this.name; }
};

// Creates a new object with 'pet' object as a prototype
const petNamedBella = Object.create(pet);

petNamedBella.name = 'Bella';
console.log(petNamedBella.getName()); // Prints 'Bella'

// Prototype of a dog which contains 'pet' as nested prototype
const dog = {
  bark: function() { console.log('bark'); },
  __proto__: pet
}

// Creates a new object with 'dog' object as prototype
const dogNamedLuna = Object.create(dog);

dogNamedLuna.name = 'Luna';
console.log(dogNamedLuna.getName()); // Prints 'Luna'
dogNamedLuna.bark(); // Prints 'bark'
```

#### Object Pool Pattern

> ***In the object pool pattern, created objects are stored in a pool where objects can be acquired from and returned for reuse. The object pool pattern is an optimization pattern because it allows the reuse of once-created objects.***

If you need to create many short-lived objects, you should utilize an object pool and reduce the need
for memory allocation and de-allocation, which takes time. Frequent object creation and
destruction in garbage-collected languages cause extra work for the garbage collector, which consumes CPU time.

Below is an example object pool protocol.

```python
from typing import Final, Protocol, TypeVar

T = TypeVar('T')


class ObjectPool(Protocol[T]):
    def acquire_object(self, cls: type[T]) -> T:
        pass

    def return_object(self, object_: T) -> None:
        pass
```

Below is an example object pool implementation.

```python
class LimitedSizeObjPool(ObjectPool[T]):
    def __init__(self, max_pool_size: int):
        self.__max_pool_size: Final = max_pool_size
        self.__pooled_objects: Final[list[T]] = []

    def acquire_object(self, cls: type[T]) -> T:
        if self.__pooled_objects:
            return self.__pooled_objects.pop(0)
        else:
            return cls()

    def return_object(self, object_: T) -> None:
        pool_is_not_full = len(self.__pooled_objects) < self.__max_pool_size

        if pool_is_not_full:
            self.__pooled_objects.append(object_)


class MyObject:
    # ...


my_object_pool: LimitedSizeObjPool[MyObject] = LimitedSizeObjPool(2)
my_object_1 = my_object_pool.acquire_object(MyObject)
my_object_2 = my_object_pool.acquire_object(MyObject)
my_object_3 = my_object_pool.acquire_object(MyObject)
my_object_pool.return_object(my_object_1)
my_object_pool.return_object(my_object_2)
my_object_pool.return_object(my_object_3) # Does not fit to pool
print(len(my_object_pool._LimitedSizeObjPool__pooled_objects))
my_object_4 = my_object_pool.acquire_object(MyObject)
print(my_object_1 == my_object_4) # Prints True
```

Below is a slightly different implementation of an object pool. The below implementation accepts clearable objects, meaning
objects returned to the pool are cleared before reusing. You can also supply parameters used when constructing an object.

```python
class Clearable(Protocol):
    def clear(self) -> None:
        pass


T = TypeVar('T', bound=Clearable)


class LimitedSizeObjPool(ObjectPool[T]):
    def __init__(self, max_pool_size: int, *args, **kwargs):
        self.__max_pool_size: Final = max_pool_size
        self.__args: Final = args
        self.__kwargs: Final = kwargs
        self.__pooled_objects: Final[list[T]] = []

    def acquire_object(self, cls: type[T]) -> T:
        if self.__pooled_objects:
            return self.__pooled_objects.pop(0)
        else:
            return cls(*self.__args, **self.__kwargs)

    def return_object(self, object_: T) -> None:
        pool_is_not_full = len(self.__pooled_objects) < self.__max_pool_size

        if pool_is_not_full:
            object_.clear()
            self.__pooled_objects.append(object_)


class MyObject(Clearable):
    def __init__(self, param1: int, param2: str, **kwargs):
        print(param1, param2, kwargs)

    def clear(self) -> None:
        print('Cleared')


my_object_pool: LimitedSizeObjPool[MyObject] = LimitedSizeObjPool(
    2, 1, 'test', name='John'
)

# Prints: 1 test {'name': 'John'}
my_object_1 = my_object_pool.acquire_object(MyObject)

# Prints: Cleared
my_object_pool.return_object(my_object_1)
```

### Structural Design Patterns

This section describes structural design patterns. Most patterns use object composition as the primary method
to achieve a particular design. The following design patterns are presented:

- Composite pattern
- Facade pattern
- Bridge pattern
- Strategy pattern
- Adapter pattern
- Proxy pattern
- Decorator pattern
- Flyweight pattern

#### Composite Pattern

> ***In the composite pattern, a class can be composed of itself, i.e., the composition is recursive.***

Recursive object composition can be depicted by how a user interface can be composed of different widgets.
The example below shows a `Pane` class that is a `Widget`. A `Pane` object can contain several other `Widget` objects,
meaning a `Pane` object can contain other `Pane` objects.

```python
from typing import Final, Protocol


class Widget(Protocol):
    def render(self) -> None:
        pass


class Pane(Widget):
    def __init__(self, widgets: list[Widget]):
        self.__widgets: Final = widgets.copy()

    def render(self) -> None:
        # Render each widget inside pane


class StaticText(Widget):
    def render(self) -> None:
        # Render static text widget


class TextInput(Widget):
    def render(self) -> None:
        # Render text input widget


class Button(Widget):
   def render(self) -> None:
        # Render button widget


class Window:
    def __init__(self, widgets: list[Widget]):
        self.__widgets: Final = widgets.copy()

    def render(self) -> None:
        for widget in self.__widgets:
            widget.render()
```

Objects that form a tree structure are composed of themselves recursively.
Below is an [Avro](https://avro.apache.org/) record field schema with a nested record field:

```json
{
  "type": "record",
  "name": "sampleMessage",
  "fields": [
    {
      "name": "field1",
      "type": "string"
    },
    {
      "name": "nestedRecordField",
      "namespace": "nestedRecordField",
      "type": "record",
      "fields": [
        {
          "name": "nestedField1",
          "type": "int",
          "signed": "false"
        }
      ]
    }
  ]
}
```

For parsing an Avro schema, we could define classes for different sub-schemas by the field type. When analyzing the below example, we can
notice that the `RecordAvroFieldSchema` class can contain any `AvroFieldSchema` object, also
other `RecordAvroFieldSchema` objects, making a `RecordAvroFieldSchema` object a composite object.

```
from typing import Protocol


class AvroFieldSchema(Protocol):
    # ...


class RecordAvroFieldSchema(AvroFieldSchema):
    def __init__(self, sub_field_schemas: list[AvroFieldSchema]):
        self.__sub_field_schemas: Final = sub_field_schemas.copy()


class StringAvroFieldSchema(AvroFieldSchema):
    # ...


class IntAvroFieldSchema(AvroFieldSchema):
    # ...


# Schema classes for rest of Avro data types ...
```

#### Facade Pattern

> ***In the facade pattern, an object on a higher level of abstraction is composed of objects on a lower level of abstraction.***
> ***The higher-level object acts as a facade in front of the lower-level objects. Lower-level objects behind the facade are***
> ***either only or mainly only accessible by the facade.***

Let's use the data exporter microservice as an example. For that microservice, we could create a `Config` interface
that can be used to obtain configuration for the different parts (input,
transform, and output) of the data exporter microservice. The `Config` interface acts as a facade. Users of
the facade need not see behind the facade. They don't know what happens behind the facade. And they shouldn't care
because they are just using the interface provided by the facade.

There can be various classes doing the actual work behind the facade. In the below example, there is a `ConfigReader` class
that reads configuration from possibly different sources (from a local file or a remote service, for example)
and there are configuration parsers that can parse a specific part of the configuration, possibly in different data formats like
JSON or YAML. None of these implementations and details are visible to the user of the facade. Any of these implementations
behind the facade can change at any time without affecting the users of the facade because facade users are not coupled to the lower-level implementations.

Below is the implementation of the `Config` facade:

```python
from typing import Protocol

from dependency_injector.wiring import Provide, inject
# Rest of imports ...


class Config(Protocol):
    def try_get_input_config(self) -> InputConfig:
        pass

    def try_get_transformer_config(self) -> TransformerConfig:
        pass

    def try_get_output_config(self) -> OutputConfig:
        pass


class ConfigImpl(Config):
    @inject
    def __init__(
        self,
        config_reader: ConfigReader = Provide['config_reader'],
        input_config_parser: InputConfigParser = Provide[
            'input_config_parser'
        ],
        transformer_config_parser: TransformerConfigParser = Provide[
            'transformer_config_parser'
        ],
        output_config_parser: OutputConfigParser = Provide[
            'output_config_parser'
        ]
    ):
        self.__config_reader: Final = config_reader
        self.__input_config_parser: Final = input_config_parser
        self.__transformer_config_parser: Final = transformer_config_parser
        self.__output_config_parser: Final = output_config_parser
        self.__config_string = ""
        self.__input_config = None
        self.__output_config = None
        self.__transformer_config = None

    def try_get_input_config(self) -> InputConfig:
        if self.__input_config is None:
            self.__try_read_config_if_needed()

            self.__input_config = self.__input_config_parser.try_parse(
                self.__config_string
            )

        return self.__input_config

    def try_get_transformer_config(self) -> TransformerConfig:
        # ...

    def try_get_output_config(self) -> OutputConfig:
        # ...

    def __try_read_config_if_needed(self) -> None:
        if not self.__config_string:
            self.__config_string = self.__config_reader.try_read(...)
```

#### Bridge Pattern

> ***In the bridge pattern, the implementation of a class is *delegated* to another class. The original class is "abstract"***
> ***in the sense that it does not have any behavior except the delegation to another class, or it can have some higher level***
> ***control logic on how it delegates to another class.***

Don't confuse the word "abstract" here with an abstract class. In an abstract class, some behavior is not implemented at all,
but the implementation is deferred to subclasses of the abstract class. Here, instead of "abstract class", we could use the term
*delegating class* instead.

![Bridge Pattern](resources/chapter2/images/bridge_pattern.png)

Let's have an example with shapes and drawings capable of drawing different shapes:

```python
from typing import Final, Protocol

from Point import Point
from ShapeRenderer import ShapeRenderer


class Shape(Protocol):
    def render(self, renderer: ShapeRenderer) -> None:
        pass


class RectangleShape(Shape):
    def __init__(
        self,
        upper_left_corner: Point,
        width: int,
        height: int
    ):
        self.__upper_left_corner: Final = upper_left_corner
        self.__width: Final = width
        self.__height: Final = height

    def render(self, renderer: ShapeRenderer) -> None:
        renderer.render_rectangle(
            self.__upper_left_corner,
            self.__width,
            self.__height
        )


class CircleShape(Shape):
    def __init__(self, center: Point, radius: int):
        self.__center: Final = center
        self.__radius: Final = radius

    def render(self, renderer: ShapeRenderer):
        renderer.render_circle(self.__center, self.__radius)
```

The above `RectangleShape` and `CircleShape` classes are abstractions (or delegating classes) because they delegate their functionality (rendering)
to an external class (implementation class) of the `ShapeRenderer` type. We can provide different rendering implementations for
the shape classes. Let's define two shape renderers, one for rendering raster shapes and another for rendering vector
shapes:

```python
from typing import Final, Protocol

from Canvas import Canvas
from Point import Point
from SvgElement import SvgElement


class ShapeRenderer(Protocol):
    def render_circle(self, center: Point, radius: int) -> None:
        pass

    def render_rectangle(
        self,
        upper_left_corner: Point,
        width: int,
        height: int
    ) -> None:
        pass

    # Methods for rendering other shapes ...


class RasterShapeRenderer(ShapeRenderer):
    def __init__(self, canvas: Canvas):
        self.__canvas: Final = canvas

    def render_circle(self, center: Point, radius: int):
        # Render circle to canvas

    def render_rectangle(
        self,
        upper_left_corner: Point,
        width: int,
        height: int
    ):
        # Render rectangle to canvas

    # Methods for rendering other shapes to canvas ...


class VectorShapeRenderer(ShapeRenderer):
    def __init__(self, svg_root: SvgElement):
        self.__svg_root: Final = svg_root

    def render_circle(self, center: Point, radius: int):
        # Render circle as SVG element and attach as child to SVG root

    def render_rectangle(
        self,
        upper_left_corner: Point,
        width: int,
        height: int
    ):
        # Render rectangle as SVG element
        # and attach as child to SVG root

    # Methods for rendering other shapes ...
```

Let's implement two different drawings, a raster and a vector drawing:

```python
from abc import abstractmethod
from typing import Final, Protocol

from Canvas import Canvas
from RasterShapeRenderer import RasterShapeRenderer
from Shape import Shape
from ShapeRenderer import ShapeRenderer
from SvgElement import SvgElement
from VectorShapeRenderer import VectorShapeRenderer


class Drawing(Protocol):
    def draw(self, shapes: list[Shape]) -> None:
        pass

    def save(self) -> None:
        pass


class AbstractDrawing(Drawing):
    def __init__(self, name: str):
        self.__name: Final = name

    @final
    def draw(self, shapes: list[Shape]) -> None:
        for shape in shapes:
            shape.render(self._get_shape_renderer())

    @final
    def save(self) -> None:
        file_name = self.__name + self._get_file_extension()
        data = self._get_data()

        # Save the 'data' to 'file_name' ...

    @abstractmethod
    def _get_shape_renderer(self) -> ShapeRenderer:
        pass

    @abstractmethod
    def _get_file_extension(self) -> str:
        pass

    @abstractmethod
    def _get_data(self) -> bytearray:
        pass


class RasterDrawing(AbstractDrawing):
    def __init__(self, name: str):
        super().__init__(name)
        self.__canvas: Final = Canvas()
        self.__shape_renderer = RasterShapeRenderer(self.__canvas)

    def _get_shape_renderer(self) -> ShapeRenderer:
        return self.__shape_renderer

    def _get_file_extension(self) -> str:
        return '.png'

    def _get_data(self) -> bytearray:
        # Get data from the 'canvas' object


class VectorDrawing(AbstractDrawing):
    def __init__(self, name: str):
        super().__init__(name)
        self.__svg_root: Final = SvgElement()
        self.__shape_renderer = VectorShapeRenderer(self.__svg_root)

    def _get_shape_renderer(self) -> ShapeRenderer:
        return self.__shape_renderer

    def _get_file_extension(self) -> str:
        return '.svg'

    def _get_data(self) -> bytearray:
        # Get data from the 'svg_root' object
```

In the above example, we have delegated the rendering behavior of the shape classes to concrete classes implementing the `ShapeRenderer` protocol.
The `Shape` classes only represent a shape but don't render the shape. They have a single responsibility of representing
a shape. Regarding rendering, the shape classes are "abstractions" because they delegate the rendering to another class responsible for rendering different shapes.

Now, we can have a list of shapes and render them differently. We can do this as shown below because we did not couple the shape classes with
any specific rendering behavior.

```python
shapes = [RectangleShape(Point(...), 2, 3), CircleShape(Point(...), 4)]

raster_drawing = RasterDrawing('raster-drawing')
raster_drawing.draw(shapes)
raster_drawing.save()

vector_drawing = VectorDrawing('vector-drawing')
vector_drawing.draw(shapes)
vector_drawing.save()
```

#### Strategy Pattern

> ***In the strategy pattern, the functionality of an object can be changed by changing an instance of a composed type to a different instance of that type.***

Below is an example where the behavior of a `ConfigReader` class can be changed by changing the value of the `config_parser` attribute
to an instance of a different class. The default behavior is to parse the configuration in JSON format,
which can be achieved by calling the constructor without a parameter.

```python
from typing import Final, Protocol


# Define Config class ...


class ConfigParser(Protocol):
    def try_parse(self, config_str: str) -> Config:
        pass


# Define JsonConfigParser class ...


class ConfigReader:
    def __init__(self, config_parser: ConfigParser = JsonConfigParser()):
        self.__config_parser: Final = config_parser

    def try_read(self, config_file_path_name: str) -> Config:
        # Try read 'config_file_path_name' to 'config_str'

        config = self.__config_parser.try_parse(config_str)
        return config
```

Using the strategy pattern, we can change the functionality of a `ConfigReader` instance by changing
the `config_parser` attribute value. For example, there could be the following classes available that
implement the `ConfigParser` protocol:

- `JsonConfigParser`
- `YamlConfigParser`
- `TomlConfigParser`
- `XmlConfigParser`

We can dynamically change the behavior of a `ConfigReader` instance to use the YAML parsing strategy
by giving an instance of the `YamlConfigParser` class as a parameter for the `ConfigReader` constructor.

#### Adapter Pattern

> ***The adapter pattern changes one interface to another interface. It allows you to adapt different interfaces to a single interface.***

In the below example, we have defined a `Message` protocol for messages that can be consumed from a data source
using a `MessageConsumer`.

{title: "Message.py"}
```python
from typing import Protocol


class Message(Protocol):
    def get_data(self) -> bytearray:
        pass

    def get_length_in_bytes(self) -> int:
        pass
```

{title: "MessageConsumer.py"}
```python
from typing import Protocol

import Message from Message


class MessageConsumer(Protocol):
    def consume_message(self) -> Message:
        pass
```

Next, we can define the message and message consumer adapter classes for [Apache Kafka](https://kafka.apache.org/) and [Apache Pulsar](https://pulsar.apache.org/):

{title: "KafkaMsgConsumer.py"}
```python
from MessageConsumer import MessageConsumer


class KafkaMsgConsumer(MessageConsumer):
    def consume_message(self) -> Message:
        # Consume a message from Kafka using a 3rd party
        # Kafka library
        # Wrap the consumed message inside an instance
        # of the below-defined KafkaMessage class
        # Return that KafkaMessage instance
```

{title: "KafkaMessage.py"}
```python
from typing import Final

from Message import Message


class KafkaMessage(Message):
    def __init__(self, kafka_lib_msg):
        # This is the "raw" message from the Kafka library
        self.__kafka_lib_msg: Final = kafka_lib_msg

    def get_data(self) -> bytearray:
        # Suppose there is 'value' property to get the
        # message data as byte array
        return self.__kafka_lib_msg.value

    def get_length_in_bytes(self) -> int:
        return len(self.__kafka_lib_msg.value)
```

{title: "PulsarMsgConsumer.py"}
```python
from MessageConsumer import MessageConsumer


class PulsarMsgConsumer(MessageConsumer):
    def consume_message(self) -> Message:
        # Consume a message from Pulsar using a Pulsar client
        # library.
        # Wrap the consumed Pulsar message inside an instance
        # of the below-defined PulsarMessage
        # Return that PulsarMessage instance
```

{title: "PulsarMessage.py"}
```python
from typing import Final

from Message import Message


class PulsarMessage(Message):
     def __init__(self, pulsar_lib_msg):
        # This is the "raw" message from the Pulsar library
        self.__pulsar_lib_msg: Final = pulsar_lib_msg

    def get_data(self) -> bytearray:
        # Suppose there is 'data' method to get the
        # message data as byte array
        return self.__pulsar_lib_msg.data()

    def get_length_in_bytes(self) -> int:
        # Suppose there is 'length' method
        # to get the message length
        return len(self.__pulsar_lib_msg.length())
```

Now, we can use Kafka or Pulsar data sources with identical consumer and message interfaces.
In the future, it will be easy to integrate a new data source into the system.
We only need to implement appropriate adapter classes (message and consumer classes) for the new data source. No other code changes are required.
Thus, we would be following the *open-closed principle* correctly.

Let's imagine that the API of the used Kafka library changed. We don't need to make changes in many
places in the code. We need to create new adapter classes (message and consumer classes) for the new API and use those new adapter classes
in place of the old adapter classes. All of this work is again following the *open-closed principle*.

Consider using the adapter pattern even if there is nothing to adapt to, especially when working with 3rd party libraries.
Because then you will be prepared for the future when changes can come. It might be possible that a 3rd party library interface
changes or there is a need to take a different library into use. If you have not used the adapter pattern, taking a new library
or library version into use could mean that you must make many small
changes in several places in the codebase, which is error-prone and against the *open-closed principle*.

Let's have an example of using a 3rd party logging library. Initially, our adapter `AbcLogger` for a fictive *abc-logging-library* is just a
wrapper around the `abc_logger` instance from the library. There is not any actual adapting done.

{title: "Logger.py"}
```python
from typing import Protocol

from LogLevel import LogLevel


class Logger(Protocol):
    def log(self, log_level: LogLevel, message: str) -> None:
        pass
```

{title: "AbcLogger.py"}
```python
from abc_logging_library import abc_logger
from Logger import Logger
from LogLevel import LogLevel

class AbcLogger(Logger):
    def log(self, log_level: LogLevel, message: str) -> None:
        abc_logger.log(log_level, message)
```

When you use the logger in your application, you can utilize the *singleton pattern* and create a singleton instance of
the `AbcLogger` in the DI container and let the DI framework inject the logger to all parts of the software component where
a logger is needed. Here is how the DI container could look:

{title: "DiContainer.py"}
```python
from AbcLogger import AbcLogger
from dependency_injector import containers, providers


class DiContainer(containers.DeclarativeContainer):
    wiring_config = containers.WiringConfiguration(
        # List all the modules here
        modules=[...]
    )

    logger = providers.ThreadSafeSingleton(AbcLogger)
    # Other bindings ...
```

Now, when you need the logger in any other class of the application, you can get it:

```python
from dependency_injector.wiring import Provide, inject
from Logger import Logger


class SomeClass:
    @inject
    def __init__(self, logger: Logger = Provide['logger']):
        # ...
```

or even easier:

```python
from dependency_injector.wiring import Provide
from Logger import Logger


class SomeClass:
    __logger: Logger = Provide['logger']

    def __init__(self):
        # ...
```

Suppose that in the future, a better logging library is available called *xyz-logging-library*, and we
would like to take that into use, but it has a slightly different interface. Its logging instance is called `xyz_log_writer`,
the logging method is named differently, and the parameters are given in different order compared to the *abc-logging-library*.
We can create a `XyzLogger` adapter class for the new logging library and update the `DiContainer`. No other code changes are required elsewhere in the codebase to take the new logging library into use.

{title: "XyzLogger.py"}
```python
from xyz_logging_library import xyz_log_writer
from Logger import Logger
from LogLevel import LogLevel

class XyzLogger(Logger):
    def log(self, log_level: LogLevel, message: str) -> None:
        xyz_log_writer.write_log_entry(message, log_level)
```

{title: "DiContainer.py"}
```python
# ...

class DiContainer(containers.DeclarativeContainer):
    # ...
    logger = providers.ThreadSafeSingleton(XyzLogger)
    # ...
```

We didn't have to modify all the places where logging is used in the codebase (and we can be sure that logging is used in many places!).
We have saved ourselves from a lot of error-prone and unnecessary work, and once again, we have followed the *open-closed principle* successfully.

In some languages (not Python) where mocking of, e.g., concrete classes is not possible, wrapping a 3rd party library in an adapter class enables you to unit test against the adapter class interface instead of the concrete classes of the 3rd party library.

#### Proxy Pattern

> ***The proxy pattern enables conditionally modifying or augmenting the behavior of an object.***

When using the proxy pattern, you define a proxy class that wraps another class (the proxied class). The proxy class conditionally delegates
to the wrapped class. The proxy class implements the interface of the wrapped class and is used in place of the wrapped
class in the code.

Below is an example of a proxy class, `CachingEntityStore`, that caches the results of entity store operations:

{title: "Cache.py"}
```python
from typing import Protocol, TypeVar

TKey = TypeVar('TKey')
TValue = TypeVar('TValue')


class Cache(Protocol[TKey, TValue]):
    def retrieve_value(self, key: TKey) -> TValue | None:
        pass

    def store(
        self,
        key: TKey,
        value: TValue,
        time_to_live_in_secs: int = 0
    ) -> None:
        pass
```

{title: "MemoryCache.py"}
```python
from typing import TypeVar

from Cache import Cache

TKey = TypeVar('TKey')
TValue = TypeVar('TValue')

class MemoryCache(Cache[TKey, TValue]):
    # ...

    def retrieve_value(self, key: TKey) -> TValue | None:
        # ...

    def store(
        self,
        key: TKey,
        value: TValue,
        time_to_live_in_secs: int = 0
    ) -> None:
        # ...
```

{title: "EntityStore.py"}
```python
from collections.abc import Awaitable
from typing import Protocol, TypeVar

T = TypeVar('T')


class EntityStore(Protocol[T]):
    async def try_get_entity(self, id_: int) -> Awaitable[T]:
        pass
```

{title: "DbEntityStore.py"}
```python
from collections.abc import Awaitable
from typing import TypeVar

from EntityStore import EntityStore

T = TypeVar('T')


class DbEntityStore(EntityStore[T]):
    async def try_get_entity(self, id_: int) -> Awaitable[T]:
        # Try get entity from database ...
```

{title: "CachingEntityStore.py"}
```python
from collections.abc import Awaitable
from typing import Final, TypeVar

from EntityStore import EntityStore
from MemoryCache import MemoryCache

T = TypeVar('T')


class CachingEntityStore(EntityStore[T]):
    def __init__(self, entity_store: EntityStore[T]):
        self.__entity_store: Final = entity_store
        self.__entity_cache: Final[MemoryCache[int, T]] = MemoryCache()

    async def try_get_entity(self, id_: int) -> Awaitable[T]:
        entity = self.__entity_cache.retrieve_value(id_)

        if entity is None:
            entity = await self.__entity_store.try_get_entity(id_)
            time_to_live_in_secs = 60
            self.__entity_cache.store(id_, entity, time_to_live_in_secs)

        return entity
```

In the above example, the `CachingEntityStore` class is the proxy class wrapping an instance implementing the `EntityStore` protocol, like an instance of the `DbEntityStore` class that stores entities in a database.
The proxy class is modifying the wrapped class behavior by conditionally delegating to the wrapped class. It
only delegates to the wrapped class if an entity is not found in the cache. Instead of a class, you can use the proxy pattern with a function by utilizing Python's `@cache` decorator.

Below is another example of a proxy class that authorizes a user before performing a service operation:

```python
from collections.abc import Awaitable
from typing import Final, Protocol

from User import User
from UserAuthorizer import UserAuthorizer


class UserService(Protocol):
    class Error(Exception):
        pass

    async def try_get_user(self, id_: int) -> Awaitable[User]:
        pass


class UserServiceImpl(UserService):
    async def try_get_user(self, id_: int) -> Awaitable[User]:
        # Try get user by id ...


class AuthorizingUserService(UserService):
    def __init__(
        self,
        user_service: UserService,
        user_authorizer: UserAuthorizer
    ):
        self.__user_service: Final = user_service
        self.__user_authorizer: Final = user_authorizer

    async def try_get_user(self, id_: int) -> Awaitable[User]:
        try:
            await self.__user_authorizer.try_authorize_user(id_)
        except UserAuthorizer.AuthorizeUserError as error:
            raise self.Error(error)

        return await self.__user_service.try_get_user(id_)
```

In the above example, the `AuthorizingUserService` class is a proxy class that wraps an instance implementing the
`UserService` protocol, like an instance of the `UserServiceImpl` class (the actual implementation of the user service). The proxy class is modifying the wrapped class behavior by conditionally delegating to the wrapped
class. It will delegate to the wrapped class only if authorization is successful.

As the last example, we could define a `RateLimitedXyzService` proxy class that wraps a `XyzService` class. The rate-limited service class delegates to the wrapped class only if the service calling rate limit is not exceeded. The rate-limited service class should raise an error if the rate is exceeded.

#### Decorator Pattern

> ***The decorator pattern enables augmenting the functionality of a class method(s) without the need to modify the class method(s).***

A decorator class wraps another class whose functionality will be augmented. The decorator class implements the interface
of the wrapped class and is used in place of the wrapped class in the code. The decorator pattern is useful when you
cannot modify an existing class, e.g., the existing class is in a 3rd party library. The decorator pattern also helps
to follow the *open-closed principle* because you don't have to modify an existing
method to augment its functionality. Instead, you can create a decorator class that contains the new functionality.

Below is an example of the decorator pattern. There is a standard SQL statement executor implementation and
two decorated SQL statement executor implementations: one that adds logging functionality and one that adds SQL statement
execution timing functionality. Finally, a double-decorated SQL statement executor is created that logs an SQL statement and times its execution.

```python
import time
from collections.abc import Awaitable
from typing import Any, Protocol

from logger import logger
from LogLevel import LogLevel


class SqlStatementExecutor(Protocol):
    async def try_execute(
        self, sql_statement: str, parameter_values: list[Any] | None = None
    ) -> Awaitable[Any]:
        pass


class SqlStatementExecutorImpl(SqlStatementExecutor):
    async def try_execute(
        self, sql_statement: str, parameter_values: list[Any] | None = None
    ) -> Awaitable[Any]:
        return await self.__get_connection().execute(
            sql_statement, parameter_values
        )

    # Implement __get_connection() ...


class LoggingSqlStatementExecutor(SqlStatementExecutor):
    def __init__(self, sql_statement_executor: SqlStatementExecutor):
        self.__sql_statement_executor = sql_statement_executor

    async def try_execute(
        self, sql_statement: str, parameter_values: list[Any] | None = None
    ) -> Awaitable[Any]:

        logger.log(
            LogLevel.DEBUG, f'Executing SQL statement: {sql_statement}'
        )

        return await self.__sql_statement_executor.try_execute(
            sql_statement, parameter_values
        )


class TimingSqlStatementExecutor(SqlStatementExecutor):
    def __init__(self, sql_statement_executor: SqlStatementExecutor):
        self.__sql_statement_executor = sql_statement_executor

    async def try_execute(
        self, sql_statement: str, parameter_values: list[Any] | None = None
    ) -> Awaitable[Any]:
        start_time_in_ns = time.time_ns()

        result = await self.__sql_statement_executor.try_execute(
            sql_statement, parameter_values
        )

        end_time_in_ns = time.time_ns()
        duration_in_ns = end_time_in_ns - start_time_in_ns
        duration_in_ms = duration_in_ns / 1_000_000

        logger.log(
            LogLevel.DEBUG,
            f'SQL statement execution duration: {duration_in_ms} ms',
        )

        return result


timing_and_logging_sql_statement_executor = LoggingSqlStatementExecutor(
    TimingSqlStatementExecutor(SqlStatementExecutorImpl())
)
```

You can also use the decorator pattern with functions and methods in Python. Python decorators allow us to wrap a function
to extend its behavior. Python decorators are functions that
take a function as a parameter and return another function that is used in place of the decorated function. Let's have an elementary
example of a function decorator in Python:

```python
# Decorator
def print_hello(func):
    def wrapped_func(*args, **kwargs):
        print('Hello')
        return func(*args, **kwargs)

    return wrapped_func


@print_hello
def add(a: int, b: int) -> int:
    return a + b


result = add(1, 2)
print(result) # Prints: Hello 3
```

Let's have another example with a decorator that times the execution of a function and prints it to the console:

```python
import time
from functools import wraps


# Decorator
def timed(func):
    @wraps(func)
    def wrapped_func(*args, **kwargs):
        start_time_in_ns = time.perf_counter_ns()
        result = func(*args, **kwargs)
        end_time_in_ns = time.perf_counter_ns()
        duration_in_ns = end_time_in_ns - start_time_in_ns

        print(
            f'Exec of func "{func.__name__}" took {duration_in_ns} ns'
        )

        return result

    return wrapped_func


@timed
def add(a: int, b: int) -> int:
    return a + b


result = add(1, 2)
print(result)
# Prints, for example:
# Exec of func "add" took 625 ns
# 3
```

You can combine multiple decorators, for example:

```python
# Decorator
def logged(func):
    @wraps(func)
    def wrapped_func(*args, **kwargs):
        result = func(*args, **kwargs)
        # In real-life, you use a logger here instead of print
        print(f'Func "{func.__name__}" executed')
        return result

    return wrapped_func

@logged
@timed
def add(a: int, b: int) -> int:
    return a + b

result = add(1, 2)
print(result)
# Prints, for example:
# Exec of func "add" took 583 ns
# Func "add" executed
# 3
```

If you change the order of decorators, you get the output in a different order, too. And the execution time of the function
is now longer because the time spent in logging is also added to the total execution time:

```python
@timed
@logged
def add(a: int, b: int) -> int:
    return a + b


result = add(1, 2)
print(result)
# Prints, for example:
# Func "add" executed
# Exec of func "add" took 9708 ns
# 3
```

You can also use decorator functions without the @-syntax to create new functions:

```python
def add(a: int, b: int) -> int:
    return a + b

logged_add = logged(add)
timed_add = timed(add)
logged_timed_add = logged(timed(add))
timed_logged_add = timed(logged(add))

print(logged_add(1,2))
# Prints
# Func "add" executed
# 3

print(timed_add(1,2))
# Prints, for example
# Exec of func "add" took 209 ns
# 3

print(logged_timed_add(1, 2))
# Prints, for example
# Exec of func "add" took 208 ns
# Func "add" executed
# 3

print(timed_logged_add(1, 2))
# Prints, for example
# Func "add" executed
# Exec of func "add" took 1250 ns
# 3
```

#### Flyweight Pattern

> ***The flyweight pattern is a memory-saving optimization pattern where flyweight objects reuse objects.***

Let's have a simple example with a game where different shapes are drawn at different positions. Let's assume
that the game draws a lot of similar shapes but in different positions so that we can notice the difference
in memory consumption after applying this pattern.

Shapes that the game draws have the following properties: size, form, fill color, stroke color, stroke width, and stroke style.

```python
from typing import Final, Protocol


class Shape(Protocol):
    # ...


# Define Color...
# Define StrokeStyle...


class AbstractShape(Shape):
    def __init__(
        self,
        fill_color: Color,
        stroke_color: Color,
        stroke_width: int,
        stroke_style: StrokeStyle
    ):
        self.__fill_color: Final = fill_color
        self.__stroke_color: Final = stroke_color
        self.__stroke_width: Final = stroke_width
        self.__stroke_style: Final = stroke_style


class CircleShape(AbstractShape):
    def __init__(
        self,
        fill_color: Color,
        stroke_color: Color,
        stroke_width: int,
        stroke_style: StrokeStyle,
        radius: int
    ):
        super().__init__(
            fill_color,
            stroke_color,
            stroke_width,
            stroke_style
        )

        self.__radius: Final = radius


# Define LineSegment ...


class PolygonShape(AbstractShape):
  def __init__(
      self,
      fill_color: Color,
      stroke_color: Color,
      stroke_width: int,
      stroke_style: StrokeStyle,
      line_segments: list[LineSegment]
  ):
      super().__init__(
          fill_color,
          stroke_color,
          stroke_width,
          stroke_style
      )

      self.__line_segments: Final = line_segments.copy()
```

When analyzing the `PolygonShape` class, we notice that it contains many properties
that consume memory. A polygon with many line segments can consume a noticeable amount of memory.
If the game draws many identical polygons in different screen positions and always creates a new `PolygonShape` object, there would be a lot of identical `PolygonShape` objects in the memory. To remediate this, we can introduce a flyweight class, `DrawnShapeImpl`,
which contains the position of a shape and a reference to the actual shape. In this way, we can draw a lot of
`DrawnShapeImpl` objects that all contain a reference to the same `PolygonShape` object:

```python
from typing import Final, Protocol


class DrawnShape(Protocol):
    # ...


class DrawnShapeImpl(DrawnShape):
    def __init__(self, shape: Shape, screen_position: Position):
        self.__shape: Final = shape
        self.__screen_position: Final = screen_position

    # ...


polygon = PolygonShape(...)
positions = generate_lots_of_positions()
drawn_polygons = [DrawnShapeImpl(polygon, position) for position in positions]
```

### Behavioral Design Patterns

Behavioral design patterns describe ways to implement new behavior using object-oriented design.
The following behavioral design patterns will be presented in the following sections:

- Chain of responsibility pattern
- Observer pattern
- Command/Action pattern
- Iterator pattern
- Interpreter pattern
- State pattern
- Mediator pattern
- Template method pattern
- Memento pattern
- Visitor pattern
- Null object pattern

#### Chain of Responsibility Pattern

> ***The chain of responsibility pattern lets you pass requests along a chain of handlers.***

The chain of responsibility pattern allows you to add pluggable behavior to handling requests. That pluggable behavior is something
that can be executed always or conditionally. This pattern allows you to follow the open-closed principle because you don't modify the
request-handling process directly but only extend it with plug-ins. This pattern allows you to follow the single responsibility principle by putting specific behavior into a plug-in.

When receiving a request, each handler can decide what to do:

- Process the request and then pass it to the next handler in the chain
- Process the request without passing it to the subsequent handlers (terminating the chain)
- Leave the request unprocessed and pass it to the next handler

The FastAPI web framework utilizes the chain of responsibility pattern for handling requests.
In the FastAPI framework, you can write pluggable behavior using [middlewares](https://fastapi.tiangolo.com/tutorial/middleware/), a concept similar to
[Servlet filters](https://www.oracle.com/java/technologies/filters.html) in Java. Below is an example of a middleware that adds HTTP request processing time to the response
in a custom HTTP header:

```python
import time

from fastapi import FastAPI, Request

app = FastAPI()


@app.middleware('http')
async def add_request_processing_time_header(request: Request, call_next):
    start_time_in_ns = time.time_ns()
    response = await call_next(request)
    end_time_in_ns = time.time_ns()
    processing_time_in_ns = end_time_in_ns - start_time_in_ns
    processing_time_in_ms = processing_time_in_ns / 1_000_000

    response.headers['X-Processing-Time-Millis'] = str(
        processing_time_in_ms
    )

    return response
```

Below is an authorization and logging middleware example:

```python
from fastapi import FastAPI, Response, Request

app = FastAPI()

# Authorization middleware
@app.middleware('http')
async def authorize(request: Request, call_next):
    # From request's 'Authorization' header,
    # extract the bearer JWT, if present
    # set 'token_is_present' variable value to True
    # Verify the validity of JWT and assign result
    # to 'token_is_valid' variable

    if token_is_valid:
        response = await call_next(request)
    elif token_is_present:
        # NOTE! call_next is not invoked,
        # this will terminate the request
        response = Response('Unauthorized', 403)
    else:
        # NOTE! call_next is not invoked,
        # this will terminate the request
        response = Response('Unauthenticated', 401)

    return response


# Logging middleware
@app.middleware('http')
async def log(request: Request, call_next):
    print(f'GET {str(request.url)}')
    return await call_next(request)


@app.get('/hello')
def hello():
    return 'Hello!'
```

#### Observer Pattern

> ***The observer pattern lets you define an observe-notify (or publish-subscribe) mechanism to notify one or more objects***
> ***about events that happen to the observed object.***

One typical example of using the observer pattern is a UI view observing a model.
The UI view will be notified whenever the model changes and can redraw itself. Let's have an example:

```python
from typing import Protocol, Final


class Observer(Protocol):
    def notify_about_change(self) -> None:
        pass


class Observable(Protocol):
    def observe_by(self, observer: Observer) -> None:
        pass


class ObservableImpl(Observable):
    def __init__(self):
        self.__observers: Final[list[Observer]] = []

    def observe_by(self, observer: Observer):
        self.__observers.append(observer)

    def _notify_observers(self) -> None:
        for observer in self.__observers:
            observer.notify_about_change()


# Define Todo ...


class TodosModel(ObservableImpl):
    __todos: list[Todo]

    def __init__(self):
        super().__init__()
        self.__todos = []

    def add_todo(self, todo: Todo) -> None:
        self.__todos.append(todo)
        self._notify_observers()

    def remove_todo(self, todo: Todo) -> None:
        self.__todos.remove(todo)
        self._notify_observers()


class TodosView(Observer):
    def __init__(self, todos_model: TodosModel):
        self.__todos_model = todos_model
        todos_model.observe_by(self)

    def notify_about_change(self) -> None:
        # Will be called when todos model change
        self.render()

    def render(self) -> None:
        # Renders todos ...
```

Let's have another example that utilizes the publish-subscribe pattern. Below, we define a `MessageBroker` class that
contains the following methods: `publish`, `subscribe`, and `unsubscribe`.

```python
from collections.abc import Callable
from typing import Final, Protocol, TypeVar

T = TypeVar('T')


class MessagePublisher(Protocol[T]):
    def publish(self, topic: str, message: T) -> None:
        pass


class MessageSubscriber(Protocol[T]):
    def subscribe(
        self, topic: str, handle_message: Callable[[T], None]
    ) -> None:
        pass


MessageHandlers = list[Callable[[T], None]]


class MessageBroker(MessagePublisher[T], MessageSubscriber[T]):
    def __init__(self):
        self.__topic_to_handle_msgs: Final[dict[str, MessageHandlers]] = {}

    def publish(self, topic: str, message: T) -> None:
        handle_messages = self.__topic_to_handle_msgs.get(topic)

        if handle_messages is not None:
            for handle_message in handle_messages:
                handle_message(message)

    def subscribe(
        self, topic: str, handle_message: Callable[[T], None]
    ) -> None:
        handle_messages = self.__topic_to_handle_msgs.get(topic)

        if handle_messages is None:
            self.__topic_to_handle_msgs[topic] = [handle_message]
        else:
            handle_messages.append(handle_message)

    def unsubscribe(
        self, topic: str, handle_message: Callable[[T], None]
    ) -> None:
        handle_messages = self.__topic_to_handle_msgs.get(topic)

        if handle_messages is not None:
            handle_messages.remove(handle_message)


message_broker: MessageBroker[str] = MessageBroker()


def print_message(message: str):
    print(message)


topic = 'test'
message_broker.subscribe(topic, print_message)
message_broker.publish(topic, 'Hi!') # Prints 'Hi!'
message_broker.unsubscribe(topic, print_message)

# Does not print, because unsubscribed
message_broker.publish('test', 'Hi!')
```

#### Command/Action Pattern

> ***Command or action pattern defines commands or actions as objects that can be given as parameters to other functions for later execution.***

The command/action pattern is one way to follow the open-closed principle, i.e., extending code by creating new command/action classes for additional functionality instead of modifying existing code.

{aside}
Python full-stack developers familiar with React and Redux know that Redux requires implementing reducers, which are typically switch-case
statements for executing a particular action. For improved object-oriented design, you could create a reducer that takes action objects as input
and performs the action when it receives an action object. This approach is discussed in more detail in my other book *Clean Code Principles And Patterns: A Software Practitioner's Handbook*.
{/aside}

Let's create a simple action and command protocol:

```python
from typing import Protocol


class Action(Protocol):
    def perform(self) -> None:
        pass


class Command(Protocol):
    def execute(self) -> None:
        pass
```

Let's create a simple concrete action/command that prints a message:

```python
from typing import Final


class PrintAction(Action):
    def __init__(self, message: str):
        self.__message: Final = message

    def perform(self) -> None:
        print(self.__message)


class PrintCommand(Command):
    def __init__(self, message: str):
        self.__message: Final = message

    def execute(self) -> None:
        print(self.__message)
```

As can be seen, the above `PrintAction` and `PrintCommand` instances encapsulate the state that is used when the action/command
is performed (usually at a later stage compared to action/command instance creation).

Now we can use our print action/command:

```python
actions = [PrintAction('Hello'), PrintAction('World')]
for action in actions:
    action.perform()

commands = [PrintCommand('Hello'), PrintCommand('World')]
for command in commands:
    command.execute()
```

Using actions or commands makes it possible to follow the open-closed principle because every new action or command
does not modify existing code but adds a new class.

Actions and commands can be made undoable, provided that the action/command is undoable. The above print action/command
is not undoable because you cannot undo print to the console. Let's introduce an undoable action: add an item to a list.
It is an action that can be undone by removing the item from the list.

```python
from typing import Final, Generic, TypeVar

T = TypeVar('T')


class AddToListAction(Action, Generic[T]):
    def __init__(self, item: T, items: list[T]):
        self.__item: Final = item
        self.__items: Final = items

    def perform(self) -> None:
        self.__items.append(self.__item)

    def undo(self) -> None:
        try:
            self.__items.remove(self.__item)
        except ValueError:
            pass


values = [1, 2]
add3ToValuesAction = AddToListAction(3, values)
add3ToValuesAction.perform()
print(values)   # Prints [1, 2, 3]
add3ToValuesAction.undo()
print(values)   # Prints [1, 2]
```

#### Iterator Pattern

> ***The iterator pattern can be used to add iteration capabilities to a class.***

Let's create a reverse iterator for the Python's `list` class. We implement the `Iterator` abstract base class by supplying an implementation
for the `__next__` method:

```python
from collections.abc import Iterator
from typing import Final, TypeVar

T = TypeVar('T')


class ReverseListIterator(Iterator[T]):
    def __init__(self, values: list[T]):
        self.__values: Final = values.copy()
        self.__position = len(values) - 1

    def __next__(self) -> T:
        if self.__position < 0:
            raise StopIteration()
        next_value = self.__values[self.__position]
        self.__position -= 1
        return next_value

    def __iter__(self) -> Iterator[T]:
        return self
```

We can put the `ReverseListIterator` class into use in a `ReverseList` class defined below:

```python
class ReverseList(list[T]):
    def  __iter__(self) -> Iterator[T]:
      return ReverseListIterator(self)
```

Now, we can use the new iterator to iterate over a list in reverse order:

```python
reversed_numbers = ReverseList([1, 2, 3, 4, 5])

for number in reversed_numbers:
    print(number)

// Prints:
// 5
// 4
// 3
// 2
// 1
```

#### Interpreter Pattern

> ***The interpreter pattern evaluates an expression in a specialized computer language using an abstract syntax tree (AST).***

The [abstract syntax tree](https://en.wikipedia.org/wiki/Abstract_syntax_tree) (AST) is represented by expression objects using the *composite pattern* to create a tree
structure of the expressions. The expressions are divided into two main types: a leaf and a non-leaf expression:

```python
class Expression:
    # ...
    def evaluate(self):
        # ...


class LeafExpression(Expression):
    def evaluate(self):
        # ...


class NonLeafExpression(Expression):
    def evaluate(self):
        # ...
```

Let's have an example with a simple specialized language where we can write addition operations, like `1 + 2 + 3`.
We need to define the expression classes. Our implementation will have one non-leaf type expression class called `AddExpression` that represents an addition operation and another leaf type expression class named `LiteralExpression` that represents a literal
(integer) value.

```python
from typing import Protocol


class Expression(Protocol):
    def evaluate(self) -> int:
        pass


class AddExpression(Expression):
    def __init__(self, left: Expression, right: Expression):
         self.__left = left
         self.__right = right

    def evaluate(self) -> int:
        return self.__left.evaluate() + self.__right.evaluate()


class LiteralExpression(Expression):
    def __init__(self, value: int):
        self.__value = value

    def evaluate(self) -> int:
        return self.__value
```

What we need is a parser for the AST. A parser goes through a "sentence" in the specialized language and produces
an AST ready for evaluation. The parser implementation is not part of this design pattern, but I will present the parser implementation below using the *test-driven development* (TDD) process. The TDD process is better described in the coming *testing principles* chapter.

First, we will list the things we need to test.

- Parse a literal, e.g., 5
- Parse another literal, e.g., 7
- Parse an invalid literal, e.g., XX
- Parse empty sentence
- Parse single addition, e.g,. 2+5
- Parse single addition with white space, e.g., ' 2  +   5 '
- Parse invalid operator, e.g., 2 * 5
- Parse addition with invalid left literal, e.g., XX + 5
- Parse addition with invalid right literal, e.g., 2 + YY
- Parse two additions, e.g., 1 + 2 + 3
- Parse two additions with invalid operator, e.g., 1 + 2 ++ 3
- Parse two additions with invalid literal, e.g., 1 + 2 + XX
- Parse three additions, e.g., 1 + 2 + 3 + 4
- Parse nine additions, e.g., '1+  2 +3 +  4 +5+ 6 +7 +8 +9+10 '
- We can stop adding tests because we have generalized the implementation enough to support any number of additions.

Let's start with the first test:

```python
class ParserTests(unittest.TestCase):
    __parser = Parser()

    def test_parse__with_literal(self):
        # WHEN
        ast = self.__parser.parse('5')

        # THEN
        self.assertEqual(ast.evaluate(), 5)
```

Let's create an implementation that makes the above test pass. We should write the simplest possible code and only enough code to make
the test pass.

```python
class Parser:
    def parse(self, sentence: str) -> Expression:
        return LiteralExpression(5)
```

Let's add a test:

```python
class ParserTests(unittest.TestCase):
    def test_parse__with_literal(self):
        # WHEN
        ast = self.__parser.parse('5')
        ast2 = self.__parser.parse('7')

        # THEN
        self.assertEqual(ast.evaluate(), 5)
        self.assertEqual(ast2.evaluate(), 7)
```

Let's generalize the implementation:

```python
class Parser:
    def parse(self, sentence: str) -> Expression:
        literal = int(sentence)
        return LiteralExpression(literal)
```

Let's add a test:

```python
class ParserTests(unittest.TestCase):
    # ...

    def test_try_parse__with_invalid_literal(self):
        # WHEN + THEN
        self.assertRaises(Parser.ParseError, self.__parser.try_parse, 'XX')
```

Let's modify the implementation to make the above test pass:

```python
class Parser:
    class ParseError(Exception):
        pass

    def try_parse(self, sentence: str) -> Expression:
        try:
            literal = int(sentence)
        except ValueError:
            raise self.ParseError()

        return LiteralExpression(literal)
```

Let's add a test:

```python
class ParserTests(unittest.TestCase):
    # ...

    def test_try_parse__with_empty_sentence(self):
        # WHEN + THEN
        self.assertRaises(Parser.ParseError, self.__parser.try_parse, 'XX')
        self.assertRaises(Parser.ParseError, self.__parser.try_parse, '')
```

The above test passes without code modification. Let's add a test:

```python
class ParserTests(unittest.TestCase):
    # ...

    def test_try_parse__with_addition(self):
        # WHEN
        ast = self.__parser.try_parse('2+5')

        # THEN
        self.assertEqual(ast.evaluate(), 7)
```

Let's modify the implementation to make the above test pass. We will split the input sentence and also extract the
literal parsing into a separate private method:

```python
class Parser:
    class ParseError(Exception):
        pass

    def try_parse(self, sentence: str) -> Expression:
        [left, *right] = sentence.split('+')
        left_literal = self.__parse_literal(left.strip())

        if not right:
            return left_literal
        else:
            right_literal = self.__parse_literal(right[0].strip())
            return AddExpression(left_literal, right_literal)

    def __parse_literal(self, literal_str: str):
        try:
            literal = int(literal_str)
        except ValueError:
            raise self.ParseError()

        return LiteralExpression(literal)
```

Let's add a test:

```python
class ParserTests(unittest.TestCase):
    # ...

    def test_try_parse__with_addition(self):
        # WHEN
        ast = self.__parser.try_parse('2+5')
        ast2 = self.__parser.try_parse(' 2  +   5 ')

        # THEN
        self.assertEqual(ast.evaluate(), 7)
        self.assertEqual(ast2.evaluate(), 7)
```

We notice that the above test passes. Let's add another test, then:

```python
class ParserTests(unittest.TestCase):
    # ...

    def test_try_parse__with_invalid_addition(self):
        # WHEN + THEN
        self.assertRaises(Parser.ParseError, self.__parser.try_parse, '2 * 5')
```

Let's modify the implementation:

```python
class Parser:
    # ...

    def try_parse(self, sentence: str) -> Expression:
        [first_token, *rest_of_tokens] = sentence.split('+')
        left_literal = self.__parse_literal(first_token.strip())

        if not rest_of_tokens:
            return left_literal
        else:
            right_literal = self.__parse_literal(rest_of_tokens[0].strip())
            return AddExpression(left_literal, right_literal)

    # ...
```

Let's add a test:

```python
class ParserTests(unittest.TestCase):
    # ...

    def test_try_parse__with_invalid_addition(self):
        # WHEN + THEN
        self.assertRaises(Parser.ParseError, self.__parser.try_parse, '2 * 5')
        self.assertRaises(Parser.ParseError, self.__parser.try_parse, 'XX + 5')
```

This test passes. Let's add another test that also passes:

```python
class ParserTests(unittest.TestCase):
    # ...

    def test_try_parse__with_invalid_addition(self):
        # WHEN + THEN
        self.assertRaises(Parser.ParseError, self.__parser.try_parse, '2 * 5')
        self.assertRaises(Parser.ParseError, self.__parser.try_parse, 'XX + 5')
        self.assertRaises(Parser.ParseError, self.__parser.try_parse, '2 + YY')
```

Let's add a test:

```python
class ParserTests(unittest.TestCase):
    # ...

    def test_try_parse__with_addition(self):
        # WHEN
        ast = self.__parser.try_parse('2+5')
        ast2 = self.__parser.try_parse(' 2  +   5 ')
        ast3 = self.__parser.try_parse('1 + 2 + 3')

        # THEN
        self.assertEqual(ast.evaluate(), 7)
        self.assertEqual(ast2.evaluate(), 7)
        self.assertEqual(ast3.evaluate(), 6)
```

Let's make the test pass:

```python
class Parser:
    # ...

    def try_parse(self, sentence: str) -> Expression:
        [first_token, *rest_of_tokens] = sentence.split('+')
        left_literal = self.__parse_literal(first_token.strip())

        if not rest_of_tokens:
            return left_literal
        elif len(rest_of_tokens) == 1:
            right_literal = self.__parse_literal(rest_of_tokens[0].strip())
            return AddExpression(left_literal, right_literal)
        else:
            return AddExpression(
                left_literal, self.try_parse('+'.join(rest_of_tokens))
            )

    # ...
```

The final four tests also pass:

```python
class ParserTests(unittest.TestCase):
    # ...

    def test_try_parse__with_invalid_addition(self):
        # WHEN + THEN
        self.assertRaises(Parser.ParseError, self.__parser.try_parse, '2 * 5')
        self.assertRaises(Parser.ParseError, self.__parser.try_parse, 'XX + 5')
        self.assertRaises(Parser.ParseError, self.__parser.try_parse, '2 + YY')

        # Added tests
        self.assertRaises(
            Parser.ParseError, self.__parser.try_parse, '1 + 2 ++ 3'
        )

        self.assertRaises(
            Parser.ParseError, self.__parser.try_parse, '1 + 2 + XX'
        )
```

```python
class ParserTests(unittest.TestCase):
    # ...

    def test_try_parse__with_addition(self):
        # WHEN
        ast = self.__parser.try_parse('2+5')
        ast2 = self.__parser.try_parse(' 2  +   5 ')
        ast3 = self.__parser.try_parse('1 + 2 + 3')
        ast4 = self.__parser.try_parse('1 + 2 + 3 + 4')
        ast5 = self.__parser.try_parse('1+  2 +3 +  4 +5+ 6 +7 +8 +9+10 ')

        # THEN
        self.assertEqual(ast.evaluate(), 7)
        self.assertEqual(ast2.evaluate(), 7)
        self.assertEqual(ast3.evaluate(), 6)

        # Added tests
        self.assertEqual(ast4.evaluate(), 10)
        self.assertEqual(ast5.evaluate(), 55)
```

In the above example, I added new tests to existing methods. You can also put each test into a separate test method.
I didn't do that because most test methods would have been only 1-2 statements long.

{aside}
If you are interested in creating a parser, read about [Recursive descent parser](https://en.wikipedia.org/wiki/Recursive_descent_parser).
{/aside}

For the expression `1 + 2 + 3`, the parser should produce the following kind of AST:

```python
# Parser should return 'ast' of below kind
ast = AddExpression(
    LiteralExpression(1),
    AddExpression(LiteralExpression(2), LiteralExpression(3))
)

# Prints 6
print(ast.evaluate())
```

#### State Pattern

> ***The state pattern lets an object change its behavior depending on its current state.***

Developers don't often treat an object's state as an object but as an enumerated value (enum), for example.
Below is an example where we have defined a `UserStory` class representing a user story that can be
rendered on screen. An enum value represents the state of a `UserStory` object.

```python
from enum import Enum
from typing import Protocol


class UserStoryState(Enum):
    TODO = 1
    IN_DEVELOPMENT = 2
    IN_VERIFICATION = 3
    READY_FOR_REVIEW = 4
    DONE = 5


class Icon(Protocol):
    # ...


class TodoIcon(Icon):
    # ...


# Define rest of icons ...


class UserStory:
    def __init__(self, name: str):
        self.__name = name
        self.__state = UserStoryState.TODO

    def set_state(self, state: UserStoryState) -> None:
        self.__state = state

    def render(self) -> None:
        match self.__state:
            case UserStoryState.TODO:
                icon = TodoIcon()
            case UserStoryState.IN_DEVELOPMENT:
                icon = InDevelopmentIcon()
            case UserStoryState.IN_VERIFICATION:
                icon = InVerificationIcon()
            case UserStoryState.READY_FOR_REVIEW:
                icon = ReadyForReviewIcon()
            case UserStoryState.DONE:
                icon = DoneIcon()
            case _:
                raise ValueError('Invalid user story state')

        # Draw a UI element on screen representing the user story
        # using the above assigned 'icon'
```

The above solution is not an object-oriented one. We should replace the conditionals (match-case statement) with a polymorphic design.
This can be done by introducing state objects. In the state pattern, the state of an object is represented with an object instead of an enum value.
Below is the above code modified to use the state pattern:

```
from typing import Protocol

# Import Icon classes ...


class UserStoryState(Protocol):
    @property
    def icon(self) -> Icon:
        pass


class TodoUserStoryState(UserStoryState):
    @property
    def icon(self) -> Icon:
        return TodoIcon()


class InDevelopmentUserStoryState(UserStoryState):
    @property
    def icon(self) -> Icon:
        return InDevelopmentIcon()


class InVerificationUserStoryState(UserStoryState):
    @property
    def icon(self) -> Icon:
        return InVerificationIcon()


class ReadyForReviewUserStoryState(UserStoryState):
    @property
    def icon(self) -> Icon:
        return ReadyForReviewIcon()


class DoneUserStoryState(UserStoryState):
    @property
    def icon(self) -> Icon:
        return DoneIcon()


class UserStory:
    def __init__(self, name: str):
        self.__name = name
        self.__state = TodoUserStoryState()

    def set_state(self, state: UserStoryState) -> None:
        self.__state = state

    def render(self) -> None:
        icon = self.__state.icon
        # Draw a UI element on screen representing
        # the user story using the given 'icon'
```

Let's have another example with an `Order` class. An order can have a state, like paid, packaged, delivered, etc.
Below, we implement the order states as classes:

```python
from typing import Protocol

from Customer import Customer
from EmailService import EmailService


class OrderState(Protocol):
    def create_message(self, order_id: str) -> str:
        pass


class PaidOrderState(OrderState):
    def create_message(self, order_id: str) -> str:
        return f'Order {order_id} is successfully paid'


class DeliveredOrderState(OrderState):
    def create_message(self, order_id: str) -> str:
        return f'Order {order_id} is delivered'


# Implement the rest of possible order states ...


class Order:
    def __init__(self, id_: str, state: OrderState, customer: Customer):
        self.__id = id_
        self.__state = state
        self.__customer = customer

    @property
    def customer_email_address(self) -> str:
        return self.__customer.email_address

    @property
    def state_message(self) -> str:
        return self.__state.create_message(self.__id)


email_service = EmailService(...)
order = Order(...)

email_service.send_email(
    order.customer_email_address,
    order.state_message
)
```

#### Mediator Pattern

> ***The mediator pattern lets you reduce dependencies between objects. It restricts direct communication between***
> ***two different layers of objects and forces them to collaborate only via a mediator object or objects.***

The mediator pattern eliminates the coupling of two different layers of objects. So, changes to one layer of objects can be
made without the need to change the objects in the other layer. This pattern is called *indirection* in GRASP principles.

A typical example of the mediator pattern is the Model-View-Controller (MVC) pattern. In the MVC pattern, model and view
objects do not communicate directly but only via mediator objects (controllers). Next, several ways to use the MVC
pattern in frontend clients are presented. Traditionally, the MVC pattern was used in the backend when the backend also generated
the view to be shown in the client device (web browser). With the advent of [single-page web clients](https://en.wikipedia.org/wiki/Single-page_application), a modern backend is a simple API containing only a model and controller (MC).

![Model-View-Controller](resources/chapter2/images/03-06.png)

In the below picture, you can see how dependency inversion is used, and none of the implementation classes depend on concrete implementations.
You can easily change any implementation class to a different one without the need to modify any other implementation class.
Notice how the `ControllerImpl` class uses the *bridge pattern* and implements two bridges, one towards the model and the other towards the view.

We should be able to replace a view implementation or create a new one without changes to other layers (model and controller). For example, we could have a view implementation that is a GUI, and we could have a "view" implementation where input and output are voice. This is called *orthogonality*, a concept from the *Pragmatic Programmer* book. Orthogonality means that a change in one place should not require a change in another place. The orthogonality principle is related to the *single responsibility principle* and the *separation of concerns principle*. When you implement software using the two
latter principles, the software becomes orthogonal.

![Dependencies in MVC pattern](resources/chapter2/images/04-06.png)

The picture below shows that the controller can also be used as a bridge adapter. The controller can be modified to
adapt to changes in the view layer (`View2` instead of `View`) without changing the model layer.
The modified modules are shown with a gray background in the picture. Similarly, the controller can be modified to adapt to
changes in the model layer without changing the view layer (not shown in the picture).

![Adapting to Changes in MVC pattern](resources/chapter2/images/04-07.png)

The following examples use a specialization of the MVC pattern called Model-View-Presenter (MVP). In the MVP pattern, the controller is
called the presenter. I use the more generic term *controller* in all examples, though. A presenter acts as a middle-man
between a view and a model. A presenter-type controller object has a reference to a view object and a model object. A view object
commands the presenter to perform actions on the model. The model object asks the presenter to update the view object.

Let's have a simple todo application as an example. First, we implement the `Todo` class, which is part of the model.

{title: "Todo.py"}
```python
class Todo:
    def __init__(self, id_: int, name: str, is_done: bool):
        self.__id = id_
        self.__name = name
        self.__is_done = is_done

    @property
    def id(self) -> int:
        return self.__id

    @id.setter
    def id(self, id_: int) -> None:
        self.__id = id

    @property
    def name(self) -> str:
        return self.__name

    @name.setter
    def name(self, name: str) -> None:
        self.__name = name

    @property
    def is_done(self) -> bool:
        return self.__is_done

    @is_done.setter
    def is_done(self, is_done: bool) -> None:
        self.__is_done = is_done
```

Next, we implement the protocol for the view layer:

{title: "TodoView.py"}
```python
from typing import Protocol

from Todo import Todo


class TodoView(Protocol):
    def show_todos(self, todos: list[Todo]) -> None:
        pass

    def show_error_message(self, error_message: str) -> None:
        pass
```

Below is the implementation of the view layer:

{title: "TodoViewImpl.py"}
```python
from typing import Final

from Todo import Todo
from TodoController import TodoController
from TodoView import TodoView


class TodoViewImpl(TodoView):
    def __init__(self, controller: TodoController):
        self.__controller: Final = controller
        controller.view = self
        controller.start_fetch_todos()

    def show_todos(self, todos: list[Todo]) -> None:
        # Update the view to show the given todos
        # Add listener for each todo checkbox
        # Listener calls: self.__controller.toggle_todo_done(todo.id)

    def show_error_message(self, error_message: str) -> None:
        # Update the view to show error message
```

Then, we implement a generic `Controller` class that acts as a base class for concrete controllers:

{title: "Controller.py"}
```python
from typing import Protocol, TypeVar

TModel = TypeVar('TModel')
TView = TypeVar('TView')


class Controller(Protocol[TModel, TView]):
    def __init__(self):
        self.__model: TModel | None = None
        self.__view: TView | None = None

    @property
    def model(self) -> TModel | None:
        return self.__model

    @model.setter
    def model(self, model: TModel) -> None:
        self.__model = model

    @property
    def view(self) -> TView | None:
        return self.__view

    @view.setter
    def view(self, view: TView) -> None:
        self.__view = view
```

The below `TodoControllerImpl` class implements two model-related actions, `start_fetch_todos` and `toggle_todo_done`, which
delegate to the model layer. It also implements two view-related actions, `update_view_with_todos` and `update_view_with_error_message`,
that delegate to the view layer.

{title: "TodoController.py"}
```python
from typing import Protocol

from Todo import Todo


class TodoController(Protocol):
    async def start_fetch_todos(self) -> None:
        pass

    async def toggle_todo_done(self, id_: int) -> None:
        pass

    def update_view_with_todos(self, todos: list[Todo]) -> None:
        pass

    def update_view_with_error_message(self, error_message: str) -> None:
        pass
```

{title: "TodoControllerImpl.py"}
```python
from Controller import Controller
from Todo import Todo
from TodoController import TodoController
from TodoModel import TodoModel
from TodoView import TodoView


class TodoControllerImpl(Controller[TodoModel, TodoView], TodoController):
    async def start_fetch_todos(self) -> None:
        if self.model is not None:
            await self.model.fetch_todos()

    async def toggle_todo_done(self, id_: int) -> None:
        if self.model is not None:
            await self.model.toggle_todo_done(id_)

    def update_view_with_todos(self, todos: list[Todo]) -> None:
        if self.view is not None:
            self.view.show_todos(todos)

    def update_view_with_error_message(self, error_message: str) -> None:
        if self.view is not None:
            self.view.show_error_message(error_message)
```

The below `TodoModelImpl` class implements the fetching of todos (`fetch_todos`) using the supplied `todo_service`.
The `todo_service` accesses the backend to read todos from a database, for example. The `TodoService` protocol is a counterpart for the same protocol in the backend API software component.
When todos are successfully fetched, the controller is told to update the view. If fetching
of the todos fails, the view is updated to show an error. Toggling a todo done is implemented
using the `todo_service` and its `try_update_todo` method.

{title: "TodoService.py"}
```python
from collections.abc import Awaitable
from typing import Protocol

from Todo import Todo


class TodoService(Protocol):
    class Error(Exception):
        # ...

    async def try_get_todos(self) -> Awaitable[list[Todo]]:
        pass

    async def try_update_todo(self, todo: Todo) -> None:
        pass
```

{title: "TodoModel.py"}
```python
from typing import Protocol

class TodoModel(Protocol):
    async def fetch_todos(self) -> None:
        pass

    async def toggle_todo_done(self, id_: int) -> None:
        pass
```

{title: "TodoModelImpl.py"}
```python
from typing import Final

from Todo import Todo
from TodoController import TodoController
from TodoModel import TodoModel
from TodoService import TodoService


class TodoModelImpl(TodoModel):
    __todos: list[Todo]

    def __init__(
        self, controller: TodoController, todo_service: TodoService
    ):
        self.__controller: Final = controller
        controller.model = self
        self.__todo_service: Final = todo_service
        self.__todos = []

    async def fetch_todos(self) -> None:
        try:
            self.__todos = await self.__todo_service.try_get_todos()
        except TodoService.Error as error:
            self.__controller.update_view_with_error_message(error.message)
        else:
            self.__controller.update_view_with_todos(self.__todos)

    async def toggle_todo_done(self, id_: int) -> None:
        found_todos = [todo for todo in self.__todos if todo.id == id_]
        todo = found_todos[0] if found_todos else None

        if todo:
            todo.is_done = not todo.is_done

            try:
                await self.__todo_service.try_update_todo(todo)
            except TodoService.Error as error:
                self.__controller.update_view_with_error_message(
                    error.message
                )
```

Let's make an exception to having all examples in Python and implement the above example using [Web Components](https://developer.mozilla.org/en-US/docs/Web/API/Web_components).
If you are not a full-stack Python developer, you can skip the rest of this section because it will be frontend-related
TypeScript code. The web component view should extend the `HTMLElement` class.
The `connectedCallback` method of the view will be called on the component mount. It starts fetching todos.
The `showTodos` method renders the given todos as HTML elements. It also adds event listeners for
the *Mark done* buttons. The `showError` method updates the inner HTML of the view to show an error
message.

{format: ts}
![Todo.ts](resources/chapter2/code/mediator_web_component/Todo.ts)

{format: ts}
![TodoView.ts](resources/chapter2/code/mediator_web_component/TodoView.ts)

{format: ts}
![TodoViewImpl.ts](resources/chapter2/code/mediator_web_component/TodoViewImpl.ts)

We can use the same controller and model APIs for this Web Component example as in the Python example.
We need to convert the Python code to the respective TypeScript code:

{format: ts}
![Controller.ts](resources/chapter2/code/mediator_web_component/Controller.ts)

{format: ts}
![TodoController.ts](resources/chapter2/code/mediator_web_component/TodoController.ts)

{format: ts}
![todoController.ts](resources/chapter2/code/mediator_web_component/_todoController.ts)

{format: ts}
![TodoService.ts](resources/chapter2/code/mediator_web_component/TodoService.ts)

{format: ts}
![TodoModel.ts](resources/chapter2/code/mediator_web_component/TodoModel.ts)

{format: ts}
![TodoModelImpl.ts](resources/chapter2/code/mediator_web_component/TodoModelImpl.ts)

We could use the above-defined controller and model as such with a React view component:

{format: ts, type: code}
![ReactTodoView.tsx](resources/chapter2/code/mediator_web_component/ReactTodoView.tsx)

If you have multiple views using the same controller, you can derive your controller from the below-defined `MultiViewController` class:

{format: ts}
![MultiViewController.ts](resources/chapter2/code/mediator_web_component/MultiViewController.ts)

Let's say we want to have two views for todos, one for the actual todos and one viewing the todo count.
We need to modify the controller slightly to support multiple views:

{format: ts}
![TodoControllerImpl.ts](resources/chapter2/code/mediator_multi_view/TodoControllerImpl.ts)

Many modern UI frameworks and state management libraries implement a specialization of the MVC pattern called,
Model-View-ViewModel (MVVM). In the MVVM pattern, the controller is called the view model. I use the more
generic term *controller* in the below example, though. The main difference between the view model and the presenter
in the MVP pattern is that in the MVP pattern, the presenter has a reference to the view, but the view model does not.
The view model provides bindings between the view's events and actions in the model. This can happen so that the view model
adds action dispatcher functions as properties of the view. In the other direction, the view model
maps the model's state to the properties of the view. For example, when using React and Redux, you can connect the view to the model
using the `mapDispatchToProps` function and connect the model to the view using the `mapStateToProps` function.
These two mapping functions form the view model (or the controller) that binds the view and model together.

Let's first implement the todo example with React and Redux and later show how the React view can be replaced
with an Angular view without modifying the controller or the model layer.

Let's implement a list view for todos:

{format: ts, type: code}
![TodosListView.tsx](resources/chapter2/code/mediator_react_redux/TodosListView.tsx)

Below is the base class `AbstractAction` for Redux actions and the base class `Controller` for controllers:

{format: ts}
![AbstractAction.ts](resources/chapter2/code/mediator_react_redux/AbstractAction.ts)

{format: ts}
![Controller.ts](resources/chapter2/code/mediator_react_redux/Controller.ts)

Below is the controller for todos:

{format: ts}
![todosController.ts](resources/chapter2/code/mediator_react_redux/todosController.ts)

In the development phase, we can use the following temporary
implementation of the `StartFetchTodosAction` class:

{format: ts}
![initialTodosState.ts](resources/chapter2/code/mediator_react_redux/initialTodosState.ts)

{format: ts}
![AbstractTodoAction.ts](resources/chapter2/code/mediator_react_redux/AbstractTodoAction.ts)

{format: ts}
![StartFetchTodosAction.ts](resources/chapter2/code/mediator_react_redux/StartFetchTodosAction.ts)

{format: ts}
![MarkDoneTodoAction.ts](resources/chapter2/code/mediator_react_redux/MarkDoneTodoAction.ts)

Now we can introduce a new view for todos, a `TodosTableView` which can utilize the same controller as the
`TodosListView`.

{format: ts, type: code}
![TodosTableView.tsx](resources/chapter2/code/mediator_react_redux/TodosTableView.tsx)

We can notice some duplication in the `TodosListView` and `TodosTableView` components.
For example, both are using the same effect. We can create a `TodosView` for which we can give
as parameter the type of a single todo view, either a list item or a table row view:

{format: ts, type: code}
![TodosView.tsx](resources/chapter2/code/mediator_react_redux/TodosView.tsx)

Below is the view for showing a single todo as a list item:

{format: ts}
![TodoViewProps.ts](resources/chapter2/code/mediator_react_redux/TodoViewProps.ts)

{format: ts, type: code}
![ListItemTodoView.tsx](resources/chapter2/code/mediator_react_redux/ListItemTodoView.tsx)

Below is the view for showing a single todo as a table row:

{format: ts, type: code}
![TableRowTodoView.tsx](resources/chapter2/code/mediator_react_redux/TableRowTodoView.tsx)

![Frontend MVC Architecture with Redux](resources/chapter2/images/03-07.png)

![Frontend MVC Architecture with Redux + Backend](resources/chapter2/images/03-08.png)

In many cases, you should not store the state in a view, even if the state is for that particular view only. Instead, when you store it in the model, it brings the following benefits:

- Possibility to easily persist state either in the browser or in the backend
- Possibility to easily implement undo-actions
- State can be easily shared with another view(s) later if needed
- Migrating views to use a different view technology is more straightforward
- Easier debugging of state-related problems, e.g., using the [Redux DevTools](https://github.com/reduxjs/redux-devtools) browser extension

We can also change the view implementation from React to Angular without modifying the controller
or model layer. This can be done, for example, using the [@angular-redux2/store](https://www.npmjs.com/package/@angular-redux2/store) library.
Below is a todos table view implemented as an Angular component:

{format: ts}
![todos-table-view.component.ts](resources/chapter2/code/mediator_react_redux/todos-table-view.component.ts)

{format: ts}
![app.component.ts](resources/chapter2/code/mediator_react_redux/app.component.ts)

{format: ts}
![app.module.ts](resources/chapter2/code/mediator_react_redux/app.module.ts)

#### Template Method Pattern

> ***Template method pattern allows you to define a *template method* in a base class, and subclasses define the final***
> ***implementation of that method. The template method contains one or more calls to abstract methods implemented in the subclasses.***

In the below example, the `AbstractDrawing` class contains a template method, `draw`. This method includes a call to
the `_get_shape_renderer` method,
an abstract method implemented in the subclasses of the `AbstractDrawing` class. The `draw` method is a template method,
and a subclass defines how to draw a single shape. The template method should always be marked as final using the `@final` decorator.
The subclasses should never override it. They should implement the abstract methods.

```python
from abc import abstractmethod
from typing import Final, Protocol

from Shape import Shape
from ShapeRenderer import ShapeRenderer


class Drawing(Protocol):
    def draw(self) -> None:
        pass


class AbstractDrawing(Drawing):
    def __init__(self, shapes: list[Shape]):
        self.__shapes: Final = shapes

    @final
    def draw(self) -> None:
        for shape in self.__shapes:
            shape.render(self._get_shape_renderer())

    @abstractmethod
        def _get_shape_renderer(self) -> ShapeRenderer:
            pass
```

We can now implement two subclasses of the `AbstractDrawing` class, which define the final behavior of
the templated `draw` method.

```python
from typing import Final

from AbstractDrawing import AbstractDrawing
from Canvas import Canvas
from RasterShapeRenderer import RasterShapeRenderer
from Shape import Shape
from ShapeRenderer import ShapeRenderer
from SvgElement import SvgElement
from VectorShapeRenderer import VectorShapeRenderer


class RasterDrawing(AbstractDrawing):
    def __init__(self, shapes: list[Shape]):
        super().__init__(shapes)
        canvas = Canvas()
        self.__shape_renderer: Final = RasterShapeRenderer(canvas)

    def _get_shape_renderer(self) -> ShapeRenderer:
        return self.__shape_renderer


class VectorDrawing(AbstractDrawing):
    def __init__(self, shapes: list[Shape]):
        super().__init__(shapes)
        svg_root = SvgElement()
        self.__shape_renderer: Final = VectorShapeRenderer(svg_root)

    def _get_shape_renderer(self) -> ShapeRenderer:
        return self.__shape_renderer
```

Template method pattern is useful to avoid code duplication in case two subclasses have methods with almost identical behavior. In that case, make that common functionality a template method in a common superclass and refine that template
method behavior in the two subclasses.

#### Memento Pattern

> ***The memento pattern can be used to save the internal state of an object to another object called the *memento* object.***

Let's have an example with a `TextEditor` class. First, we define a `TextEditorState` protocol and its implementation.
Then, we define a `TextEditorStateMemento` class for storing a memento of the text editor's state.

```python
from typing import Protocol


class TextEditorState(Protocol):
    def clone(self) -> 'TextEditorState':
        pass


class TextEditorStateImpl(TextEditorState):
    # Implement text editor state here ...


class TextEditorStateMemento:
    def __init__(self, state: TextEditorState):
        self.__state = state.clone()

    @property
    def state(self) -> TextEditorState:
        return self.__state
```

The `TextEditor` class stores mementos of the text editor's state. It provides methods
to save a state, restore a state, or restore the previous state:

```python
from typing import Final

from TextEditorStateImpl import TextEditorStateImpl
from TextEditorStateMemento import TextEditorStateMemento


class TextEditor:
    def __init__(self):
        self.__state_mementos: Final[list[TextEditorStateMemento]] = []
        self.__current_state = TextEditorStateImpl(...)
        self.__current_version = 1

    def save_state(self) -> None:
        self.__state_mementos.append(
            TextEditorStateMemento(self.__current_state)
        )

        self.__current_version += 1

    def restore_state(self, version: int) -> None:
        if 1 <= version <= len(self.__state_mementos):
            self.__current_state = self.__state_mementos[version - 1].state
            self.__current_version += 1

    def restore_previous_state(self) -> None:
        if self.__current_version > 1:
            self.restore_state(self.__current_version - 1)
```

In the above example, we can add a memento
for the text editor's state by calling the `save_state` method. We can recall the previous version
of the text editor's state using the `restore_previous_state` method and any version of the text editor's state
using the `restore_state` method.

#### Visitor Pattern

> ***Visitor pattern allows adding functionality to a class (like adding new methods) without modifying the class.***
> ***This is useful, for example, with library classes that you cannot modify.***

First, let's have an example with classes that we can modify:

```python
from typing import Final, Protocol


class Shape(Protocol):
    def draw(self) -> None:
        pass


class CircleShape(Shape):
    def __init__(self, radius: int):
        self.__radius: Final = radius

    def draw(self) -> None:
        # Draw circle ...

    @property
    def radius(self) -> int:
        return self.__radius


class RectangleShape(Shape):
    def __init__(self, width: int, height: int):
        self.__width: Final = width
        self.__height: Final = height

    def draw(self) -> None:
        # Draw rectangle ...

    @property
    def width(self) -> int:
        return self.__width

    @property
    def height(self) -> int:
        return self.__height
```

Let's assume we need to calculate the total area of shapes in a drawing. Currently, we are in a situation where
we can modify the shape classes, so let's add `calculate_area` methods to the classes:

```python
import math
from typing import Protocol


class Shape(Protocol):
    # ...

    def calculate_area(self) -> float:
        pass


class CircleShape(Shape):
    # ...

    def calculate_area(self) -> float:
        return math.pi * self.__radius**2


class RectangleShape(Shape):
    # ...

    def calculate_area(self) -> float:
        return self.__width * self.__height
```

Adding a new method to an existing class may be against the *open-closed principle*.
In the above case, adding the `calculate_area` methods
is safe because the shape classes are immutable. And even if they were not, adding the `calculate_area` methods would be safe because
they are read-only, i.e., they don't modify the object's state. Also, we don't have to worry about thread safety if we agree that our example application is not multithreaded.

Now that we have the area calculation methods added, we can use a common algorithm to calculate the total area of shapes
in a drawing:

```python
from functools import reduce

shapes = [CircleShape(1), RectangleShape(2, 2)]

total_shapes_area = reduce(
    lambda accum_shapes_area, shape: accum_shapes_area + shape.calculate_area(),
    shapes,
    0.0
)

print(total_shapes_area) # Prints 7.141592653589793
```

But what if the shape classes, without the area calculation capability, were in a 3rd party library that we cannot modify?
We would have to do something like this:

```python
import math
from functools import reduce

def shapes_area(accum_shapes_area: float, shape: Shape) -> float:
    if isinstance(shape, CircleShape):
        shape_area = math.pi * shape.radius**2
    elif isinstance (shape, RectangleShape):
        shape_area = shape.width * shape.height
    else:
        raise ValueError('Invalid shape')

    return accum_shapes_area + shape_area


total_shapes_area = reduce(
    shapes_area,
    shapes,
    0.0
)
```

The above solution is complicated and needs updating every time a new type of shape is introduced.
The above example does not follow object-oriented design principles: it contains an if/elif structure with `isinstance` checks, called the *chain of instanceof checks* code smell.

We can use the visitor pattern to replace the above conditionals with polymorphism. First, we introduce a visitor protocol that can be used to provide
additional behavior to the shape classes. Then, we introduce an `execute` method in the `Shape` protocol. In the shape classes,
we implement the `execute` methods so that additional behavior provided by a concrete visitor can be executed:

```python
from typing import Any, Final, Protocol


# This is our visitor protocol that
# provides additional behaviour to the shape classes
class ShapeBehavior(Protocol):
    def execute_for_circle(self, circle: 'CircleShape') -> Any:
        pass

    def execute_for_rectangle(self, rectangle: 'RectangleShape') -> Any:
        pass

    # Add methods for possible other shape classes here ...


class Shape(Protocol):
    # ...

    def execute(self, behavior: ShapeBehavior) -> Any:
        pass


class CircleShape(Shape):
    def __init__(self, radius: int):
        self.__radius: Final = radius

    @property
    def radius(self) -> int:
        return self.__radius

    def execute(self, behavior: ShapeBehavior) -> Any:
        return behavior.execute_for_circle(self)


class RectangleShape(Shape):
    def __init__(self, width: int, height: int):
        self.__width: Final = width
        self.__height: Final = height

    @property
    def width(self) -> int:
        return self.__width

    @property
    def height(self) -> int:
        return self.__height

    def execute(self, behavior: ShapeBehavior) -> Any:
        return behavior.execute_for_rectangle(self)
```

Suppose our application is multithreaded and the shape classes were mutable and made thread-safe. We would have to define the `execute` methods with appropriate
synchronization to make them also thread-safe:

```python
from threading import Lock


class CircleShape(Shape):
    # Constructor that initializes
    # self.__lock = Lock()

    def execute(self, behavior: ShapeBehaviour) -> Any:
        with self.__lock:
            return behavior.execute_for_circle(self)

    # Rest of methods ...


class RectangleShape(Shape):
    # Constructor that initializes
    # self.__lock = Lock()

    def execute(self, behavior: ShapeBehaviour) -> Any:
        with self.__lock:
            return behavior.execute_for_rectangle(self)

    # Rest of methods ...
```

Let's implement a concrete visitor for calculating areas of different shapes:

```python
class AreaCalculationShapeBehavior(ShapeBehavior):
    def execute_for_circle(self, circle: CircleShape) -> Any:
        return math.pi * circle.radius**2

    def execute_for_rectangle(self, rectangle: RectangleShape) -> Any:
        return rectangle.width * rectangle.height
```

Now, we can implement the calculation of shapes' total area by using a common algorithm, and we get rid of the
conditionals. Below, we execute the `area_calculation` behavior for each shape.

```python
shapes = [CircleShape(1), RectangleShape(2, 2)]
area_calculation = AreaCalculationShapeBehavior()

total_shapes_area = reduce(
    lambda accum_shapes_area, shape:
        accum_shapes_area + shape.execute(area_calculation),
    shapes,
    0.0
)
```

You can add more behavior to the shape classes by defining a new visitor. Let's define a
`PerimeterCalculationShapeBehaviour` class:

```python
class PerimeterCalculationShapeBehavior(ShapeBehavior):
    def execute_for_circle(self, circle: CircleShape) -> Any:
        return 2 * math.pi * circle.radius

    def execute_for_rectangle(self, rectangle: RectangleShape) -> Any:
        return 2 * rectangle.width + 2 * rectangle.height
```

Notice that we did not need to use the *visitor* term in our code examples. Adding the design pattern name to the names of
software entities (class/function names, etc.) often does not bring any real benefit but makes the names longer. However, there are some design patterns, like the *factory pattern* and *builder pattern* where you always use the design pattern name in a class name.

If you develop a third-party library and want the behavior of its classes to be extended by its users, you should make your library classes accept visitors who can perform additional behavior. Using the visitor pattern allows to add behavior to existing classes without modifying them, i.e., in accordance with the open-closed principle. However, there is one drawback to using the visitor pattern. You must create getters and setters for class attributes to allow visitors to add behavior. Adding getters and setters breaks the class encapsulation, as was discussed earlier in this chapter.

#### Null Object Pattern

> ***A null object is an object that does nothing.***

Use the null object pattern to implement a class for null objects that don't do anything. A null object can be used in place of a real object that
does something.

Let's have an example with a `Shape` protocol:

```python
from typing import Protocol


class Shape(Protocol):
    def draw(self) -> None:
        pass
```

We can easily define a class for *null* shape objects:

```python
from Shape import Shape


class NullShape(Shape):
    def draw(self) -> None:
        # Intentionally no operation
```

A *null* shape does not draw anything. We can use an instance of the `NullShape` class everywhere where a concrete implementation of the `Shape` protocol is wanted.

## Tell, Don't Ask Principle

> ***Tell, don't ask principle states that you should *tell* another object what to do, and not ask about the other object's state and then do the work by yourself in your object.***

If your object asks many things from another object using, e.g., multiple getters, you might be guilty of the *feature envy* design smell.
Your object is envious of a feature that the other object should have.

Let's have an example and define a cube shape class:

```python
from typing import Final, Protocol


class ThreeDShape(Protocol):
    # ...


class Cube3DShape(ThreeDShape):
    def __init__(self, width: int, height: int, depth: int):
        self.__width: Final = width
        self.__height: Final = height
        self.__depth: Final = depth

    @property
    def width(self, ) -> int:
        return self.__width

    @property
    def height(self) -> int:
        return self.__height

    @property
    def depth(self) -> int:
        return self.__depth
```

Next, we define another class, `CubeUtils,` that contains a method for calculating the total volume of cubes:

```python
from typing import final

from Cube3DShape import Cube3DShape


@final
class CubeUtils:
    @staticmethod
    def calculate_total_volume(cubes: list[Cube3DShape]) -> int:
        total_volume = 0
        for cube in cubes:
            total_volume += cube.width * cube.height * cube.depth
        return total_volume
```

In the `calculate_total_volume` method, we ask about a cube object's state three times. This is
against the *tell, don't ask principle*. Our method is envious of the volume calculation feature
and wants to do it by itself rather than telling a `Cube3DShape` object to calculate its volume.

Let's correct the above code so that it follows the *tell, don't ask principle*:

```python
from typing import Final, Protocol, final


class ThreeDShape(Protocol):
    def calculate_volume(self) -> int:
        pass


class Cube3DShape(ThreeDShape):
    def __init__(self, width: int, height: int, depth: int):
        self.__width: Final = width
        self.__height: Final = height
        self.__depth: Final = depth

    def calculate_volume(self) -> int:
        return self.__height * self.__width * self.__depth


@final
class ThreeDShapeUtils:
    @staticmethod
    def calculate_total_volume(three_d_shapes: list[ThreeDShape]) -> int:
        total_volume = 0
        for three_d_shape in three_d_shapes:
            total_volume += three_d_shape.calculate_volume()
        return total_volume
```

Our `calculate_total_volume` method does not ask anything about a cube object. It just tells a cube object
to calculate its volume. We also removed the *asking* methods (getters/properties) from the `Cube3DShape` class because
they are no longer needed.

Below is another example of asking instead of telling:

```python
import time


class AnomalyDetectionEngine:
    def run(self) -> None:
        while self.__is_running:
            now = time.time()

            if self.__anomaly_detector.anomalies_should_be_detected(now):
                anomalies = self.__anomaly_detector.detect_anomalies()
                # Do something with the detected anomalies ...

            time.sleep(1)
```

In the above example, we ask the anomaly detector if we should detect anomalies now. Then, depending
on the result, we call another method on the anomaly detector to detect anomalies. This could be simplified by making
the `detect_anomalies` method to check if anomalies should be detected using the `anomalies_should_be_detected` method.
Then, the `anomalies_should_be_detected` method can be made private, and we can simplify the above code as follows:

```python
class AnomalyDetectionEngine:
    def run(self) -> None:
        while self.__is_running:
            anomalies = self.__anomaly_detector.detect_anomalies()
            # Do something with the detected anomalies ...
            time.sleep(1)
```

Following the *tell, don't ask principle* is a great way to reduce coupling in your software component. In the above example,
we reduced the number of methods the `calculate_total_volume` method is dependent on from three to one. Following the principle
also contributed to higher cohesion in the software component because operations related to a cube are now inside the
`Cube` class and not scattered around in the code base. The *tell, don't ask principle* is the same as the *information
expert* from the GRASP principles. The information expert principle says to put behavior in a class with the most information required
to implement the behavior. In the above example, the `Cube` class clearly has the most information needed (width, height, and depth) to calculate a cube's area.

## Law of Demeter

> ***A method on an object received from another object's method call should not be called.***

The below statements are considered to break the law because of the chained method calls:

```python
user.get_account().get_balance()
user.get_account().withdraw(...)
```

The above statements can be corrected by moving functionality to a different class or by making
the second object to act as a facade (remember the *facade pattern*) between the first and the third object.

Below is an example of the latter solution, where we introduce two new methods in the `User` class and remove the
`get_account` method:

```python
user.get_account_balance()
user.withdraw_from_account(...)
```

In the above example, the `User` class is a facade in front of the `Account` class that we should not
access directly from our object.

However, you should always check if the first solution alternative could be used instead. It makes the code more object-oriented and does not require creating additional methods.

Below is an example that uses `User` and `SalesItem` entities and is not obeying the law of Demeter:

```python
from SalesItem import SalesItem
from User import User


def purchase(user: User, sales_item: SalesItem) -> None:
    account = user.get_account()

    # Breaks the law
    account_balance = account.get_balance()

    sales_item_price = sales_item.get_price()

    if account_balance >= sales_item_price:
        # Breaks the law
        account.withdraw(sales_item_price);

    # ...
```

We can resolve the problem in the above example by moving the `purchase` method
to the correct class, in this case, the `User` class:

```
from Account import Account
from SalesItem import SalesItem


class User:
    def __init__(self, account: Account):
        self.__account = account

    def purchase(self, sales_item: SalesItem) -> None:
        account_balance = self.__account.get_balance()
        sales_item_price = sales_item.get_price()

        if account_balance >= sales_item_price:
            self.__account.withdraw(sales_item_price)

        # ...
```

Following the *law of Demeter* is a great way to reduce coupling in your software component. When you follow the
*law of Demeter* you are not depending on the objects behind another object, but that other object provides a facade
to the objects behind it.

## Avoid Primitive Obsession Principle

> ***Avoid primitive obsession by defining semantic types for function parameters and function return value.***

Some of us have experienced a situation where we have supplied arguments to a function in the wrong order.
This can be easy if the function, for example, takes two integer parameters, but you accidentally give those two integer parameters
in the wrong order. You don't get a compilation error.

Another problem with primitive types as function arguments is that the argument values are not necessarily validated.
You may have to implement the validation logic in your function. However, the validation should typically happen when reading untrusted input and only already validated values are supplied to other functions in the software component.

Suppose you accept an integer parameter for a port number in a function.
In that case, you might get any integer value as the parameter value,
even though the valid port numbers are from 1 to 65535. Suppose you also had other functions in the same codebase
accepting a port number as a parameter. In that case, you could do the same validation logic in multiple places and have
duplicate code in your codebase.

Let's have a simple example of using this principle:

```python
from Shape import Shape


class RectangleShape(Shape):
    def __init__(self, width: int, height: int):
        self.__width = width
        self.__height = height
```

In the above example, the constructor has two parameters with the same primitive type (int). It is possible
to give `width` and `height` in the wrong order. But if we refactor the code to use objects instead of primitive
values, we can make the likelihood of giving the arguments in the wrong order much smaller:

```python
from typing import TypeVar, Generic, Final

from Shape import Shape

T = TypeVar('T')


class Value(Generic[T]):
    def __init__(self, value: T):
        self.__value: Final = value

    @property
    def value(self) -> T:
        return self.__value


class Width(Value[int]):
    pass


class Height(Value[int]):
    pass


class RectangleShape(Shape):
  def __init__(self, width: Width, height: Height):
          self.__width = width.value
          self.__height = height.value


width = Width(20)
height = Height(50)

# OK
rectangle = RectangleShape(width, height)

# Does not pass type check, parameters are in wrong order
rectangle2 = RectangleShape(height, width)

# Does not pass type check, first parameter is not a width
rectangle3 = RectangleShape(height, height)

# Does not pass type check, second parameter is not a height
rectangle4 = RectangleShape(width, width)

# Does not pass type check,
# Width and Height objects must be used
# instead of primitive types
rectangle5 = RectangleShape(20, 50)
```

In the above example, `Width` and `Height` are simple data classes. They don't contain any behavior. You
can use concrete data classes as function parameter types. There is no need to create an interface for a data class.
So, the *program against interfaces* principle does not apply here.

In Python, we have another way to safeguard against giving the same type of parameters in the wrong order: named
parameters. Without the named parameters, the creation of a new rectangle would look like the following:

```python
rectangle = RectangleShape(20, 50)
```

In the above example, we must be sure that the first parameter is width and the second is height.
When using named parameters, we don't have to remember the correct order of the parameters:

```python
rectangle = RectangleShape(width=20, height=50)
rectangle2 = RectangleShape(height=50, width=20)
```

Let's have another simple example where we have the following function signature:

```python
def do_something(namespaced_name: str, ...):
    # ...
```

The above function signature allows function callers to supply a non-namespaced name accidentally.
By using a custom type for the namespaced name, we can formulate the above function signature to the following:

```python
from typing import Final


class NamespacedName:
    def __init__(self, namespace: str, name: str):
        self.__namespaced_name: Final = (
            (namespace + '.' + name) if namespace else name
        )

    def get(self) -> str:
        return self.__namespaced_name


def do_something(namespaced_name: NamespacedName, ...):
    # ...
```

Let's have a more comprehensive example with an `HttpUrl` class. The class constructor has several parameters
that should be validated upon creating an HTTP URL:

```python
from typing import Final


class HttpUrl:
    def __init__(
        self,
        scheme: str,
        host: str,
        port: int,
        path: str,
        query: str
    ):
        self.__url_string: Final = (
              scheme
              + "://"
              + host
              + ":"
              + str(port)
              + path
              + "?"
              + query
        )
```

Let's introduce an abstract class for validated values:

```
from abc import abstractmethod
from typing import Final, Generic, TypeVar

from Optional import Optional

T = TypeVar('T')


class AbstractValidatedValue(Generic[T]):
    def __init__(self, value: T):
        self._value: Final = value

    def get(self) -> Optional[T]:
        return (
            Optional.of(self._value)
            if self._is_valid()
            else Optional.empty()
        )

    class GetError(Exception):
        pass

    def try_get(self) -> T:
        if self._is_valid():
            return self._value
        else:
            raise self.GetError('Invalid ' + self.__class__.__name__)

    @abstractmethod
        def _is_valid(self) -> bool:
            pass
```

Let's create a class for validated HTTP scheme objects:

```python
from functools import cache

from AbstractValidatedValue import AbstractValidatedValue


class HttpScheme(AbstractValidatedValue[str]):
    # Because instances are immutable, we can cache the validation result
    # This would be especially beneficial in cases where
    # the validation logic is complex and this method is
    # called many times
    @cache
    def _is_valid(self) -> bool:
        lowercase_value = self._value.lower()
        return lowercase_value == 'https' or lowercase_value == 'http'
```

Let's create a `Port` class (and similar classes for the host, path, and query should be created):

```python
from functools import cache

from AbstractValidatedValue import AbstractValidatedValue


class Port(AbstractValidatedValue[int]):
    # Because instances are immutable, we can cache the validation result
    @cache
    def _is_valid(self) -> bool:
        return 1 <= self._value <= 65535


# Implement Host class ...
# Implement Path class ...
# Implement Query class ...
```

Let's create a utility class, `OptionalUtils`, with a method for mapping a result for five optional values:

```python
from collections.abc import Callable
from typing import TypeVar, final

from Optional import Optional

T = TypeVar('T')
U = TypeVar('U')
V = TypeVar('V')
X = TypeVar('X')
Y = TypeVar('Y')
R = TypeVar('R')


@final
class OptionalUtils:
    @staticmethod
    def map_all(
      opt1: Optional[T],
      opt2: Optional[U],
      opt3: Optional[V],
      opt4: Optional[X],
      opt5: Optional[Y],
      mapper: Callable[[T, U, V, X, Y], R]
  ) -> Optional[R]:
        if (
            opt1.is_present()
            and opt2.is_present()
            and opt3.is_present()
            and opt4.is_present()
            and opt5.is_present()
         ):
            return Optional.of(
                map(
                    opt1.try_get(),
                    opt2.try_get(),
                    opt3.try_get(),
                    opt4.try_get(),
                    opt5.try_get(),
                )
            )
        else:
            return Optional.empty()
```

Next, we can reimplement the `HttpUrl` class to contain two alternative factory methods for creating an HTTP URL:

```python
# Imports ...

T = TypeVar("T")

class PrivateConstructor(type):
    def __call__(
        cls: type[T], *args: tuple[Any, ...], **kwargs: dict[str, Any]
    ):
        raise TypeError('Constructor is private')

    def _create(
        cls: type[T], *args: tuple[Any, ...], **kwargs: dict[str, Any]
    ) -> T:
        return super().__call__(*args, **kwargs)


class HttpUrl(metaclass=PrivateConstructor):
    def __init__(self, url_string: str):
        self.__url_string: Final = url_string

    @property
    def url_string(self) -> str:
        return self.__url_string

    # Factory method that returns an optional HttpUrl
    @classmethod
    def create(
        cls,
        scheme: HttpScheme,
        host: Host,
        port: Port,
        path: Path,
        query: Query
    ) -> Optional['HttpUrl']:
        return OptionalUtils.map_all(
            scheme.get(),
            host.get(),
            port.get(),
            path.get(),
            query.get(),
            lambda scheme_val, host_val, port_val, path_val, query_val: cls._create(
                scheme_val
                + '://'
                + host_val
                + ':'
                + port_val
                + path_val
                + '?'
                + query_val
            )
        )

    class CreateError(Exception):
        pass

    # Factory method that returns a valid HttpUrl or
    # raises an error
    @classmethod
    def try_create(
        cls,
        scheme: HttpScheme,
        host: Host,
        port: Port,
        path: Path,
        query: Query
    ) -> 'HttpUrl':
        try:
            return cls._create(
                scheme.try_get()
                + '://'
                + host.try_get()
                + ':'
                + str(port.try_get())
                + path.try_get()
                + '?'
                + query.try_get()
            )
        except AbstractValidatedValue.GetError as error:
            raise cls.CreateError(error)


maybe_http_url = HttpUrl.create(
    HttpScheme('https'),
    Host('www.google.com'),
    Port(443),
    Path('/query'),
    Query('search=jee')
)

# Prints https://www.google.com:443/query?search=jee
print(maybe_http_url.try_get().url_string)

# Raises an error: Invalid Port
http_url2 = HttpUrl.try_create(
    HttpScheme('https'),
    Host('www.google.com'),
    Port(443222),
    Path('/query'),
    Query('search=jee')
)
```

Notice how we did not hardcode the URL validation inside the `HttpUrl` class, but we created
small validated value classes: `HttpScheme`, `Host`, `Port`, `Path`, and `Query`. These classes can be further utilized
in other parts of the codebase if needed and can even be put into a shared validation library for broader usage.

An application typically receives unvalidated input data from the following external sources:

- Reading command line arguments
- Reading environment variables
- Reading standard input
- Reading files from the file system
- Reading data from a socket (network input)
- Receiving input from a user interface (UI)

Make sure that you validate any data received from the sources mentioned above. Use a ready-made validation library or
create your own validation logic if needed. Validate the input immediately after receiving it from an untrustworthy source and only pass valid values to other functions in the codebase. In this way, other functions in the codebase can trust the input they receive, and they don't have to validate it again. If you pass unvalidated data freely around in your application, you may need to implement validation logic in every function, which is unreasonable.

## Dependency Injection (DI) Principle

> ***Dependency injection (DI) allows changing the behavior of an application based on static or dynamic configuration.***

When using dependency injection, the dependencies are injected only after the application startup. The application can
first read its configuration and then decide what objects are created for the application. In many languages,
dependency injection is crucial for unit tests, too. When executing a unit test using DI, you can inject mock
dependencies into the tested code instead of using the standard dependencies of the application.

Below is an example of using the *singleton pattern* without dependency injection:

```python
from enum import Enum

from Logger import Logger


class LogLevel(Enum):
    ERROR = 1
    WARN = 2
    INFO = 3
    DEBUG = 4
    TRACE = 5


class StdOutLogger(Logger):
    @staticmethod
    def log(log_level: LogLevel, message: str) -> None:
        # Log to standard output


class Application:
    def run(self):
        StdOutLogger.log(LogLevel.INFO, 'Starting application')
        # ...
```

In the above example, we use a static method of the hard-coded `StdOutLogger` class. It is difficult to change the logger later (assuming the static log method is used in many places in the codebase).

We should refactor the above code not to use a static method but use dependency injection instead:

{title: "Logger.py"}
```python
from enum import Enum
from typing import Protocol


class LogLevel(Enum):
    ERROR = 1
    WARN = 2
    INFO = 3
    DEBUG = 4
    TRACE = 5


class Logger(Protocol):
    def log(self, log_level: LogLevel, message: str) -> None:
        pass
```

{title: "StdOutLogger.py"}
```
from Logger import Logger, LogLevel


class StdOutLogger(Logger):
    def log(self, log_level: LogLevel, message: str) -> None:
        # Log to standard output
```

{title: "Application.py"}
```python
from dependency_injector.wiring import Provide, inject
from Logger import Logger
from LogLevel import LogLevel


class Application:
    @inject
    def __init__(self, logger: Logger = Provide['logger']):
        self.__logger = logger

    def run(self):
        self.__logger.log(LogLevel.INFO, 'Starting application')
        # ...
```

Then we need to define the DI container:

{title: "DiContainer.py"}
```python
from dependency_injector import containers, providers
from StdOutLogger import StdOutLogger


class DiContainer(containers.DeclarativeContainer):
    wiring_config = containers.WiringConfiguration(
        modules=['Application']
    )

    logger = providers.Singleton(StdOutLogger)
```

Now, changing the code to use a different logger is easy. Let's say we want to log into a file instead of the standard output.
We can introduce a new class for the file-based logger (following the *open-closed principle*)

{title: "FileLogger.py"}
```python
from typing import Final

from Logger import Logger, LogLevel


class FileLogger(Logger):
    def __init__(self, log_file_directory: str):
        self.__log_file_directory: Final = log_file_directory

    def log(self, log_level: LogLevel, message: str):
        # Log to a file in self.__log_file_directory
```

Then, we can change the DI container to use the file-based logger instead of the `StdOutLogger`:

{title: "DiContainer.py"}
```python
import os

from dependency_injector import containers, providers
from FileLogger import FileLogger


class DiContainer(containers.DeclarativeContainer):
    wiring_config = containers.WiringConfiguration(modules=['Application'])

    logger = providers.Singleton(
        FileLogger, os.environ.get('LOG_FILE_DIRECTORY', '/var/log')
    )
```

We can also change the logging behavior dynamically based on the environment where the application is running:

{title: "DiContainer.py"}
```python
import os

from dependency_injector import containers, providers
from FileLogger import FileLogger
from StdOutLogger import StdOutLogger


class DiContainer(containers.DeclarativeContainer):
    wiring_config = containers.WiringConfiguration(modules=['Application'])

    if os.environ.get('LOG_DESTINATION') == 'file':
        logger = providers.Singleton(
            FileLogger, os.environ.get('LOG_FILE_DIRECTORY', '/var/log')
        )
    else:
        logger = providers.Singleton(StdOutLogger)
```

If you have a very simple microservice with few dependencies, you might think dependency injection is an overkill.
One thing that is sure in software development is change. Change is inevitable, and you cannot predict the future. For example, your microservice may start growing larger. Introducing DI in a late phase of a project might require
substantial refactoring. Therefore, consider using DI in all non-trivial applications from
the beginning.

For all full-stack Python developers, below is a TypeScript example of a *data-visualization-web-client* where the [noicejs](https://github.com/ssube/noicejs) NPM library is used for
dependency injection. This library resembles the famous [Google Guice](https://github.com/google/guice) library. Below is a `FakeServicesModule` class
that configures dependencies for different backend services that the web client uses.
As you can notice, all the services are configured to use fake implementations because this DI module is used when
the backend services are not yet available. A `RealServicesModule` class
can be implemented and used when the backend services become available. In the `RealServicesModule` class, the
services are bound to their actual implementation classes instead of fake implementations.

```ts
import { Module } from 'noicejs';
import FakeDataSourceService from ...;
import FakeMeasureService from ...;
import FakeDimensionService from ...;
import FakeChartDataService from ...;

export default class FakeServicesModule extends Module {
  override async configure(): Promise<void> {
    this.bind('dataSourceService')
      .toInstance(new FakeDataSourceService());

    this.bind('measureService')
      .toInstance(new FakeMeasureService());

    this.bind('dimensionService')
      .toInstance(new FakeDimensionService());

    this.bind('chartDataService')
      .toInstance(new FakeChartDataService());
    );
  }
}
```

With the *noicejs* library, you can configure several DI modules and create a DI container from the wanted modules.
The module approach lets you divide dependencies into multiple modules, so you don't have a single big
module and lets you instantiate a different module or modules based on the application configuration.

In the below example, the DI container is created from a single module, an instance of the `FakeServicesModule` class:

{title: "diContainer.ts"}
```ts
import { Container } from 'noicejs';
import FakeServicesModule from './FakeServicesModule';

const diContainer = Container.from(new FakeServicesModule());

export default diContainer;
```

In the development phase, we could create two separate modules, one for fake services and another one for real services,
and control the application behavior based on the web page's URL query parameter:

{title: "diContainer.ts"}
```ts
import { Container } from 'noicejs';
import FakeServicesModule from './FakeServicesModule';
import RealServicesModule from './RealServicesModule';

const diContainer = (() => {
  if (location.href.includes('useFakeServices=true')) {
    // Use fake services if web page URL
    // contains 'useFakeServices=true'
    return Container.from(new FakeServiceModule());
  } else {
    // Otherwise use real services
    return Container.from(new RealServicesModule());
  }
})();

export default diContainer;
```

Then, you must configure the `diContainer` before the dependency injection can be used.
In the below example, the `diContainer` is configured before a React application is rendered:

{title: "app.ts"}
```ts
import React from 'react';
import ReactDOM from 'react-dom';
import diContainer from './diContainer';
import AppView from './app/view/AppView';

diContainer.configure().then(() => {
  ReactDOM.render(<AppView />, document.getElementById('root'));
});
```

Then, in Redux actions, where you need a service, you can inject the required service with
the `@Inject` decorator. You specify the name of the service you want to inject. The service
will be injected as the class constructor argument's property (with the same name).

{title: "StartFetchChartDataAction.ts"}
```ts
// Imports ...

type ConstructorArgs = {
  chartDataService: ChartDataService,
  chart: Chart,
  dispatch: Dispatch;
};

export default
@Inject('chartDataService')
class StartFetchChartDataAction extends AbstractChartAreaAction {
  private readonly chartDataService: ChartDataService;
  private readonly chart: Chart;

  constructor({ chart,
                chartDataService,
                dispatch }: ConstructorArgs) {
    super(dispatch);
    this.chartDataService = chartDataService;
    this.chart = chart;
  }

  perform(currentState: ChartAreaState): ChartAreaState {
    this.chartDataService
      .fetchChartData(
        this.chart.dataSource,
        this.chart.getColumns(),
        this.chart.getFilters(),
        this.chart.getSortBys()
      )
      .then((columnNameToValuesMap: ColumnNameToValuesMap) => {
        this.dispatch(
          new FinishFetchChartDataAction(columnNameToValuesMap,
                                         this.chart.id)
        );
      })
      .catch((error) => {
        // Handle error
      });

    this.chart.isFetchingChartData = true;
    return ChartAreaStateUpdater
            .getNewStateForChangedChart(currentState, this.chart);
  }
}
```

To be able to dispatch the above action, a controller should be implemented:

```ts
import diContainer from './diContainer';
import StartFetchChartDataAction from './StartFetchChartDataAction';
import Controller from './Controller';
import store from './store';

class ChartAreaController extends Controller {
  readonly actionDispatchers = {
    startFetchChartData: (chart: Chart) =>
      // the 'chart' is given as a property to
      // StartFetchChartDataAction class constructor
      this.dispatchWithDi(diContainer,
                         StartFetchChartDataAction,
                         { chart });
  }
}

export const controller = new ChartAreaController(store.dispatch);
export type ActionDispatchers = typeof controller.actionDispatchers;
```

The following base classes are also defined:

{title: "AbstractAction.ts"}
```ts
export default abstract class AbstractAction<S> {
  abstract perform(state: S): S;
}
```

{title: "AbstractDispatchingAction.ts"}
```ts
// Imports...

export default abstract class AbstractDispatchingAction<S>
    extends AbstractAction<S> {
  constructor(protected readonly dispatch: Dispatch) {}
}
```

{title: "AbstractChartAreaAction.ts"}
```ts
// Imports...

export default abstract class AbstractChartAreaAction
         extends AbstractDispatchingAction<ChartAreaState> {
}
```

{title: "Controller.ts"}
```ts
export type ReduxDispatch =
  (reduxActionObject: { type: AbstractAction<any> }) => void;

export default class Controller {
  protected readonly dispatch:
    (action: AbstractAction<any>) => void;

  constructor(reduxDispatch: ReduxDispatch) {
    this.dispatch = (action: AbstractAction<any>) =>
      reduxDispatch({ type: action });
  }

  dispatchWithDi(
    diContainer: { create: (...args: any[]) => Promise<any> },
    ActionClass:
      abstract new (...args: any[]) => AbstractAction<any>,
    otherArgs: {}
  ) {
    // diContainer.create will create a new object of
    // class ActionClass.
    // The second parameter of the create function defines
    // additional properties supplied to ActionClass constructor.
    // The create method is asynchronous. When it succeeds,
    // the created action object is available in the 'then'
    // function and it can be now dispatched

    diContainer
      .create(ActionClass, {
        dispatch: this.dispatch,
        ...otherArgs
      })
      .then((action: any) => this.dispatch(action));
  }
}
```

## Avoid Code Duplication Principle

> ***At the class level, when you spot duplicated code in two different classes implementing the same interface, you should create a new base class to accommodate the common functionality and let the classes extend the new base class.***

Below is an `AvroBinaryKafkaInputMessage` class that implements the `InputMessage` protocol:

{title: "InputMessage.py"}
```python
from typing import Protocol

from DecodedMessage import DecodedMessage
from Schema import Schema


class InputMessage(Protocol):
    def try_decode_schema_id(self) -> int:
        pass

    def try_decode(self, schema: Schema) -> DecodedMessage:
        pass
```

{title: "AvroBinaryKafkaInputMessage.py"}
```python
from typing import Final

from DecodedMessage import DecodedMessage
from InputMessage import InputMessage
from KafkaMessage import KafkaMessage
from Schema import Schema


class AvroBinaryKafkaInputMessage(InputMessage):
    def __init__(self, kafka_message: KafkaMessage):
        self.__kafka_message: Final = kafka_message

    def try_decode_schema_id(self) -> int:
        # Try decode schema id from the beginning of
        # the Avro binary Kafka message

    def try_decode(self, schema: Schema) -> DecodedMessage:
        return schema.try_decode_message(
            self.__kafka_message.payload,
            self.__kafka_message.length
        )
```

If we wanted to introduce a new Kafka input message class for JSON, CSV, or XML format, we could create a class like the `AvroBinaryKafkaInputMessage` class.
But then we could notice the code duplication in the
`try_decode` method. We can notice that the `try_decode` method is the same regardless of the input message source and format.
According to this principle, we should move the duplicate code to a common base class, `AbstractInputMessage`. We could make
the `try_decode` method a template method according to the *template method pattern* and create abstract methods for getting the message data and
its length:

{title: "AbstractInputMessage.py"}
```python
from abc import abstractmethod
from typing import final

from DecodedMessage import DecodedMessage
from InputMessage import InputMessage
from Schema import Schema


class AbstractInputMessage(InputMessage):
    @abstractmethod
    def try_decode_schema_id(self) -> int:
        pass

    # Template method
    @final
    def try_decode(self, schema: Schema) -> DecodedMessage:
        return schema.try_decode_message(
            self._get_data(),
            self._get_length()
        )

    @abstractmethod
        def _get_data(self) -> bytearray:
            pass

    @abstractmethod
        def _get_length(self) -> int:
            pass
```

Next, we should refactor the `AvroBinaryKafkaInputMessage` class to extend the new `AbstractInputMessage` class and implement
the `_get_data` and `_get_length` methods.
But we can realize these two methods are the same for all Kafka input message data formats.
We should not implement those two methods in the `AvroBinaryKafkaInputMessage` class because
we would need to implement them as duplicates if we needed to add a Kafka input message class for another data format.
Once again, we can utilize this principle and create a new base class for Kafka input messages:

{title: "AbstractKafkaInputMessage.py"}
```python
from abc import abstractmethod
from typing import Final

from AbstractInputMessage import AbstractInputMessage
from KafkaMessage import KafkaMessage


class AbstractKafkaInputMessage(AbstractInputMessage):
    def __init__(self, kafka_message: KafkaMessage):
        self.__kafka_message: Final = kafka_message

    @abstractmethod
    def try_decode_schema_id(self) -> int:
        pass

    def _get_data(self) -> bytearray:
        return self.__kafka_message.payload

    def _get_length(self) -> int:
        return self.__kafka_message.length
```

Finally, we can refactor the `AvroBinaryKafkaInputMessage` class to contain no duplicated code:

{title: "AvroBinaryKafkaInputMessage.py"}
```python
from AbstractKafkaInputMessage import AbstractKafkaInputMessage


class AvroBinaryKafkaInputMessage(AbstractKafkaInputMessage):
    def try_decode_schema_id(self) -> int:
        # Try decode the schema id from the beginning of
        # the Avro binary Kafka message
        # Use base class _get_data() and _get_length()
        # methods to achieve that
```

## Inheritance in Cascading Style Sheets (CSS)

This last section of this chapter is for full-stack Python developers interested in how inheritance works in CSS.
In HTML, you can define classes (class names) for HTML elements in the following way:

```html
<span class="icon pie-chart-icon">...</span>
```

In a CSS file, you define CSS properties for CSS classes, for example:

```css
.icon {
  background-repeat: no-repeat;
  background-size: 1.9rem 1.9rem;
  display: inline-block;
  height: 2rem;
  margin-bottom: 0.2rem;
  margin-right: 0.2rem;
  width: 2rem;
}

.pie-chart-icon {
  background-image: url('pie_chart_icon.svg');
}
```

The problem with the above approach is that it is not correctly object-oriented. In the HTML code, you
must list all the class names to combine all the needed CSS properties. It is easy to forget
to add a class name. For example, you could specify `pie-chart-icon` only and forget to specify the `icon`.

It is also difficult to change the inheritance hierarchy afterward. Suppose you wanted to add a new
class `chart-icon` for all the chart icons:

```css
.chart-icon {
  // Define properties here...
}
```

You would have to remember to add the `chart-icon` class name to all places in the HTML code where you are rendering chart icons:

```html
<span class="icon chart-icon pie-chart-icon">...</span>
```

The above-described approach is very error-prone. What you should do is introduce proper object-oriented design.
You need a CSS preprocessor that makes extending CSS classes possible. In the below example, I am using [SCSS](https://sass-lang.com/guide/):

```html
<span class="pieChartIcon">...</span>
```

```scss
.icon {
  background-repeat: no-repeat;
  background-size: 1.9rem 1.9rem;
  display: inline-block;
  height: 2rem;
  margin-bottom: 0.2rem;
  margin-right: 0.2rem;
  width: 2rem;
}

.chartIcon {
  @extend .icon;

  // Other chart icon related properties...
}

.pieChartIcon {
  @extend .chartIcon;

  background-image: url('../../../../../assets/images/icons/chart/pie_chart_icon.svg');
}
```

In the above example, we define only one class for the HTML element. The inheritance hierarchy is defined in the SCSS file using the
`@extend` directive. We are now free to change the inheritance hierarchy in the future without
any modification needed in the HTML code.
