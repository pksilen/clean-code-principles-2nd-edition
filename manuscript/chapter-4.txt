# Testing Principles

Testing is traditionally divided into two categories: functional and non-functional testing.
This chapter will first describe the functional testing principles and then the non-functional testing
principles.

## Functional Testing Principles

Functional testing is divided into three phases:

- Unit testing
- Integration testing
- End-to-end (E2E) testing

Functional test phases can be described with the [testing pyramid](https://martinfowler.com/articles/practical-test-pyramid.html):

![Testing Pyramid](resources/chapter4/images/testing-pyramid.png)

The testing pyramid depicts the relative number of tests in each phase. Most tests are unit tests. The second most tests are
integration tests; the fewest are E2E tests. Unit tests should cover the whole codebase of a software component.
Unit testing focuses on testing individual public functions as units (of code). Software component integration tests cover
the integration of the unit-tested functions to a complete working software component, including testing the interfaces
to external services. External services include a database, a message broker, and other microservices. E2E testing
focuses on testing the end-to-end functionality of a complete software system.

There are various other terms used to describe different testing phases:

- Module testing (an older term for unit testing)
- (Software) Component testing (same as integration testing)
- System (Integration) testing (same as E2E testing)

The term *component testing* is also used to denote only the integration of the unit-tested modules in a software component without
testing the external interfaces. In connection with the *component testing* term, there is the term *integration testing*
used to denote the testing of external interfaces of a software component. Here, I use the term *integration testing* to
denote both the integration of unit-tested modules and external interfaces.

### Unit Testing Principle

> ***Unit tests should test the functionality of public functions as isolated units with as high coverage as possible. The isolation means that dependencies (other classes/modules/services) are mocked.***

The isolation is not a mandatory requirement, but you should mock at least all external dependencies like external services and databases.
There exists two schools of thought regarding unit testing and test-driven development (TDD): London and Detroit/Chicago. In the London school
of TDD, unit tests are created from top to bottom (from higher-level classes to lower-level classes) or from outside to inside (when considering
clean architecture). What this means in practice is that you must to create mocks for lower-level classes when you implement unit tests for
the high-level classes, because the low-level classes do not exists yet. London school of TDD is also called *Mockist*. It focuses on interactions between objects and ensuring correct messages are sent
between them. On the other hand, in the Detroit or Chicago school of TDD, unit tests are created from bottom to top (from lower-level classes to
higher level classes) or from inside to outside when considering clean architecture. This TDD style is also called *Classic*. What this means in practice is that you don't necessarily need
to create so many mocks. When you test a higher-level class, you have already implemented the needed lower-level classes and you can use their
implementation in your higher-level class tests. This would make the higher-level class tests as partial integration tests. There are benefits
and drawbacks in both schools of TDD. These styles are not mutually exclusive. The drawback of London school of TDD is that mocks create tight
coupling to lower-level interfaces and changing them requires changes to tests. The benefit is that you can follow the natural path from
creating higher-level classes first and lower-level classes later as needed. The drawback of Detroit/Chicago school of TDD is that you need
figure out the lower level classes first and implement them first which might be slow and feel unnatural. In theory, the Detroit/Chicago school
tests can be slower if they test dependencies, also a bug in a single unit can cause many tests to fail. The benefit of Detroit/Chicago school
of TDD is that code refactoring is easier and tests don't break easily if they are not relying on mocks. Which of the schools of TDD is better?
It is hard to say. London school of TDD (or unit testing in general) is probably more popular in general but it does not make it necessarily better.
Many TDD veterans favor the classic approach, i.e. Detroit/Chicago school.
Hybrid approaches, where elements from both schools are taken, are also popular. Many developers don't even know these two schools exists.
Later in the chapter I will give you an example of using both of these schools using the same example.

Unit tests should be written for public functions only. Do not try to test private functions separately. They
should be tested indirectly when testing public functions. Unit tests should test the function specification, i.e., what
the function is expected to do in various scenarios, not how the function is implemented. When you unit test only public functions,
you can easily refactor the function implementation, e.g., rewrite the private functions that the public function uses without
modifying the related unit tests (or with minimal changes).

Below is a JavaScript example:

{title: "parseConfig.js"}
```js
import { doSomething } from 'other-module';

function readFile(...) {
  // ...
}

export default function parseConfig(...) {
 // ...
 // readFile(...)
 // doSomething(...)
 // ...
}

In the above module, there is one public function, `parseConfig`, and one private function, `readFile`. In unit testing,
you should test the public `parseConfig` function in isolation and mock the `doSomething` function, which is imported from another module.
And you indirectly test the private `readFile` function when testing the public `parseConfig` function.

Below is the above example written in Java. You test the Java version in a similar way as the JavaScript version. You
write unit tests for the public `parseConfig` method only. Those tests will test the private `readFile` function indirectly.
You must supply a mock instance of the `OtherInterface` interface for the `ConfigParser` constructor. This is because you
should not be testing dependencies of the `ConfigParser` class in the unit tests of the `ConfigParser` class.

```java
public interface OtherInterface {
  void doSomething(...);
}


public class OtherClass implements OtherInterface {
 // ...

 public void doSomething(...) {
    // ...
 }
}


public class ConfigParser {
   private OtherInterface otherObject;

   public ConfigParser(final OtherInterface otherObject) {
      this.otherObject = otherObject;
   }

   // ...

   public Configuration parseConfig(...) {
      // ...
      // readFile(...)
      // otherClass.doSomething(...)
      // ...
   }

   private String readFile(...) {
      // ...
   }
}
```

If you have a public function using many private methods, testing the public method can become complicated, and the test method becomes long,
possibly with many expectations on mocks. It can be challenging to remember to test every scenario that exists in the public and related private methods.
What you should do is refactor all or some private methods into public methods of one or more new classes (This is the *extract class* refactoring technique explained in the previous chapter).
Then, the test method in the original class becomes shorter and more straightforward, with less expectation of mocks. This is an essential
refactoring step that should not be forgotten. It helps keep both the source code and unit tests readable and well-organized.
The unit test code must be the same high quality as the source. Unit test code should use type annotations and the same linter rules as the source code itself.
The unit test code should not contain duplicate code. A unit test method should be a maximum of 5-9 statements long.
Aim for a single assertion or put assertions in one well-named private method.
If you have more than 5-6 expectations on mocks, you need to refactor the source code as described above to reduce the number of expectations and to make the test method shorter.

Unit tests should test all the functionality of a public function: happy path(s),
possible failure situations, security issues, and edge/corner cases so that each code line of the function is covered by at least one unit test.
If a single line of code contains, e.g., a complex boolean statement, you might need several unit tests to cover all the sub-conditions in the boolean statement.
For example:

```java
if (isEmpty() || !isInitialized()) {
  // ...
}
```

To fully cover the functionality of the above unit, you need to write three tests: one for the `isEmpty()` condition to be true,
one for the `!isInitialized()` to be true and a test where the if-statement's condition is evaluated to false. If you use a test coverage tool, it will report about
untested sub-conditions of boolean expressions. You should always use a test coverage tool.

Security issues in functions are mostly related to the input the function gets. Is that input secure? If your function receives unvalidated
input data from an end-user, that data must be validated against a possible attack by a malicious end-user.

Below are some examples of edge/corner test cases listed:

- Is the last loop counter value correct? This test should detect possible off-by-one errors
- Test with an empty array
- Test with the smallest allowed value
- Test with the biggest allowed value
- Test with a negative value
- Test with a zero value
- Test with a very long string
- Test with an empty string
- Test with floating-point values having different precisions
- Test with floating-point values that are rounded differently
- Test with an extremely small floating-point value
- Test with an extremely large floating-point value

Unit tests should not test the functionality of dependencies. That is something to be tested
with integration tests. A unit test should test a function in isolation. If a function has one
or more dependencies on other functions defined in different classes (or modules), those
dependencies should be mocked. A *mock* is something that mimics the behavior of a real object or function. Mocking
will be described in more detail later in this section.

Testing functions in isolation has two benefits. It makes tests faster. This is a real benefit because
you can have a lot of unit tests, and you run them often, so the execution time of the unit tests must be as short as possible.
Another benefit is that you don't need to set up external dependencies, like a database, a message broker, and other microservices, because you are mocking the functionality
of the dependencies.

Unit tests give you protection against introducing accidental bugs when refactoring code. Unit tests ensure that the implementation code meets the
function specification. It should be remembered that it is hard to write the perfect code
on the first try. You are bound to practice refactoring to keep your code base clean and free of technical debt.
And when you refactor, the unit tests are on your side to prevent accidentally introducing bugs.

#### Test-Driven Development (TDD)

Test-driven development (TDD) is a software development process in which software requirements are formulated as
(unit and integration) test cases before the software is implemented. This is as opposed to the practice where software is
implemented first, and test cases are written only after that.

The benefits of TDD are many:

- Lesser likelihood of forgetting to implement some failure scenarios or edge cases
- No big upfront design, but emergent (and usually better) design because of constant refactoring
- Existing tests make the developer feel confident when refactoring
- Less cognitive load for the developer because only one scenario is implemented at a time
- You automatically write testable code
- You don't write unnecessary code because you only write code that makes the tests pass (you can follow the [YAGNI principle](https://en.wikipedia.org/wiki/You_aren%27t_gonna_need_it))
- Less need for debugging

TDD aims to maximize the external and internal quality of the software. External quality is the quality the users see, e.g., a minimal number of bugs and correct features.
Internal quality is what the developers see. If internal quality is low, it is more likely that a developer can introduce bugs (i.e., cause low external quality) to
the software. High internal quality can be achieved with TDD. You have a comprehensive set of tests and a good design with low technical debt.
It will be easy for a developer to maintain and develop such software further.

I have been in the industry for almost 30 years, and when I began coding, there were no automated tests or
test-driven development. Only since around 2010 have I been writing automated unit tests. Due to this
background, TDD has been quite difficult for me because there is something I have grown accustomed to: Implement
the software first and then do the testing. I assume many of you have also learned it like that, which can make switching to TDD
rather difficult. Little material exists that teaches topics using TDD. The internet is full of books, courses, videos, blogs, and
other posts that don't teach you the proper way of development: TDD. The same applies to this book, also. I present code samples in the book but don't always use
TDD because it would make everything complicated and more verbose.
We are constantly taught to program test-last!
When asking people who have conducted TDD, they say the main benefit is that it reduces stress because you don't have to achieve multiple goals simultaneously,
like designing the function and thinking of and implementing (and remembering to implement) all possible execution paths.

TDD can feel unnatural at the beginning. It can slow you down at first. You just have to keep practicing it systematically. In that way, you can start gradually seeing the benefits and
build a habit of always using TDD. Later, you will notice that it begins to feel natural, and it does not slow you down anymore but brings you the many benefits depicted above.
It can actually speed you up because less complex thinking (no big upfront design, safe refactoring) and debugging are needed.

The complete TDD cycle, as instructed by *Kent Beck* in his book *Test-Driven Development by Example* consists of the following steps:

1) Add a test for a specified functionality
2) Run all the tests (The just added test should fail if the functionality it is testing is not implemented yet)
3) Write the simplest possible code that makes the tests pass
4) Run all the tests. (They should pass now)
5) Refactor as needed (Existing tests should ensure that anything won't break)
6) Start again from the first step until all functionality is implemented, refactored, and tested

Instead of TDD, you should actually use low-level behavior-driven development (BDD). We discuss BDD later in this chapter
when addressing integration testing. Low-level BDD extends TDD by more formally defining the low-level (function/unit-level) behavior. To use BDD,
ensure the following in your unit tests:

- Test method name tells the tested public function name (feature name) and the scenario that is tested in that particular test method
- The test method body is organized into three sections: Given, When, and Then steps. This is basically the same thing as Arrange-Act-Assert,
  which is the suggested way to structure unit tests. In the Given/Arrange phase, you prepare everything for the test.
  In the Act/When phase, you perform an operation; in the Assert/Then phase, you make assertions about the operation result.
  Try to keep the assertion phase as simple as possible. The best is to have only a single assertion, but that is not always possible.
  You can also extract multiple assertions into a well-named private method and call that method from the test method in the assertion phase.

Some TDD practitioners suggest you name a test method after the feature it is testing, including a description of the scenario and expected outcome.
That approach can easily make the test method names too long and hard to read. You can always see the expected result by looking at the end of the test method,
which should preferably contain only a single assertion. I also like to put test methods in a test class in the same order I have in the tested class.
Also, the scenarios can be ordered from a more specialized scenario to a more generalized scenario (or vice versa).
All this makes navigating between test methods a breeze. Also, there is not much difference between a class's features and its public methods.
Each public method should implement a single feature only (remember the *single responsibility principle*).
A single feature (method) consists of one or more scenarios. For example, a `Stack` class has four features:
You can `push` an item to the stack and `pop` an item from the stack. You can also ask if the stack `isEmpty` or ask its `size`.
These features have multiple scenarios: for example, how they behave if the stack is empty versus the stack with items in it.
When you add features to a class, you should put them into new methods or, preferably, use the *open-closed principle* and put them in a totally new class.

Below are two Java examples of test method names in a `StackTests` class. The first one contains the method name that is tested, and the second one is named after the feature:

```java
class StackTests {
  @Test
  void testPop_whenStackIsEmpty() {
    // WHEN + THEN
    assertThrows(...);
  }


  // You can also use method name prefixed with 'it':
  // itShouldRaiseErrorWhenPoppingItemFromEmptyStack
  // The 'it' means what is tested: a stack
  @Test
  void shouldRaiseErrorWhenPoppingItemFromEmptyStack() {
    // WHEN + THEN
    assertThrows(...);
  }
}
```

Here is another Java example:

```java
class CarTests {
  @Test
  void testAccelerate() {
    // GIVEN
    final var initialSpeed = ...
    final var car = new Car(initialSpeed);

    // WHEN
    car.accelerate();

    // THEN
    assertTrue(car.getSpeed() > initialSpeed);
  }

  @Test
  void testDecelerate() {
    // GIVEN
    final var initialSpeed = ...
    final var car = new Car(initialSpeed);

    // WHEN
    car.decelerate();

    // THEN
    assertTrue(car.getSpeed() < initialSpeed);
  }

  @Test
  void shouldIncreaseSpeedWhenAccelerates() {
    // GIVEN
    final var initialSpeed = ...
    final var car = new Car(initialSpeed);

    // WHEN
    car.accelerate();

    // THEN
    assertTrue(car.getSpeed() > initialSpeed);
  }

  @Test
  void shouldDecreaseSpeedWhenDecelerates() {
    // GIVEN
    final var initialSpeed = ...
    final var car = new Car(initialSpeed);

    // WHEN
    car.decelerate();

    // THEN
    assertTrue(car.getSpeed() < initialSpeed);
  }
}
```

You could make the assertion even more readable by extracting a well-named method, for example:

```java
class CarTests {
  private final int INITIAL_SPEED = ...
  private Car car;

  @BeforeEach
  void setUp() {
    car = new Car(INITIAL_SPEED);
  }

  @Test
  void testAccelerate() {
    // WHEN
    car.accelerate();

    // THEN
    assertSpeedIsIncreased();
  }

  private void assertSpeedIsIncreased() {
    assertTrue(car.getSpeed() > INITIAL_SPEED);
  }
}
```

Notice how well the `testAccelerate` test method now reads:

> When car accelerate[s], then assert speed is increased

You can use the feature-based naming instead of the method-based naming convention if you want. I use the method-based naming convention in all unit tests in this book.

Let's continue with an example. Suppose there is the following user story in the backlog waiting to be implemented:

> *Parse configuration properties from a configuration string to a configuration object. Configuration properties can be accessed from the configuration object. If parsing the configuration fails, an error should be produced.*

Let's first write a test for the simplest 'happy path' scenario of the specified functionality: parsing a single property only.

```java
class ConfigParserTests {
  private final ConfigParser configParser = new ConfigParserImpl();

  @Test
  void testParse_whenSuccessful() {
    // GIVEN
    final var configString = "propName1=value1";

    // WHEN
    final var config = configParser.parse(configString);

    // THEN
    assertEquals('value1', config.getPropertyValue('propName1'));
  }
}
```

If we run the test, we get a compilation error, meaning the test case we wrote won't pass. Next,
we shall write the simplest possible code to make the test case both compile and pass. We can make shortcuts like using a fixed value (constant) instead of a more generalized solution. That is called
*faking it*. We can fake it until we make it, which happens when we add a new test that forces us to eliminate the constant value and write more generalized code. The part of TDD where you add more
tests to drive for a more generalized solution is called *triangulation*.

```java
public interface Config {
  String getPropertyValue(final String propertyName);
}


public class ConfigImpl implements Config {
  public String getPropertyValue(final String propertyName) {
    return "value1";
  }
}


public interface ConfigParser {
  Config parse(final String configString);
}


public class ConfigParserImpl implements ConfigParser {
  Config parse(final String configString) {
    return new ConfigImpl();
  }
}
```

Let's write a test for a 'happy path' scenario where we have two properties. This forces us to make the
implementation more generalized. We cannot use a constant anymore, and we should not use two constants with
an if/else-statement because if we want to parse more than two properties, the approach using constants does not scale.

```java
class ConfigParserTests {
  private final ConfigParser configParser = new ConfigParserImpl();

 @Test
  def testParse_whenSuccessful() {
    // GIVEN
    final var configString = "propName1=value1\npropName2=value2";

    // WHEN
    final var config = configParser.parse(configString);

    // THEN
    assertEquals('value1', config.getPropertyValue('propName1'));
    assertEquals('value2', config.getPropertyValue('propName2'));
  }
}
```

If we run all the tests, the new test will fail in the second assertion. Next,
we shall write code to make the test cases pass:

```java
public class ConfigImpl implements Config {
  private final Map<String, String> propNameToValue;

  public ConfigImpl(final Map<String, String> propNameToValue) {
    this.propNameToValue = propNameToValue;
  }

  public String getPropertyValue(final String propertyName) {
    return propNameToValue.get(propertyName);
  }
}

public class ConfigParserImpl implements ConfigParser {
  Config parse(final String configString) {
    // Parse configString and assign properties to
    // 'propNameToValue' variable

    return ConfigImpl(propNameToValue);
  }
}
```

Now, the tests pass, and we can add new functionality. Let's add a test for the case when parsing fails.
We can now repeat the TDD cycle from the beginning by creating a failing test first:

```java
class ConfigParserTests {
  // ...

  @Test
  void testParse_whenParsingFails() {
    // GIVEN
    final var configString = 'invalid';

    try {
      // WHEN
      configParser.parse(configString);

      // THEN
      fail("ConfigParser.ParseError should have been raised");
    except ConfigParser.ParseError:
      // THEN error was successfully raised
  }
}
```

Next, we should refactor the implementation to make the second test pass:

```java
public interface ConfigParser {
  // We assume here that this code is part of Data exporter app
  public static class ParseError extends DataExporterError {
    // ...
  }

  Config parse(final String configString) throws ParseError;
}

public class ConfigParserImpl extends ConfigParser {
  Config parse(final String configString) throws ParseError {
    // Try parse configString and if successful
    // assign config properties to 'maybePropNameToValue'
    // variable

    return maybePropNameToValue
      .map(propNameToValue -> ConfigImpl::new)
      .orElseThrow(ParseError::new);
  }
}
```

We could continue the TDD cycle by adding new test cases for additional functionality if such existed.

Before starting the TDD process, list all the requirements (scenarios) with bullet points so you don't forget any. The scenarios should cover all
happy paths, edge cases, and failure/security scenarios. For example, for a single method, you might identify the following six scenarios:

- Edge case scenarios
  - Scenario A
  - Scenario B
- Failure scenarios
  - Scenario C
  - Scenario D
- Security scenarios
  - Scenario E
- Success scenarios
  - Scenario F

Listing all scenarios is an important step in order not to forget to test something because if you don't write a test for something,
it's highly likely you won't implement it either. During the TDD process, you should
add any missing scenarios to that list. Always add it to the list immediately so that you don't forget it. Order the list so that the most specialized scenarios are listed first.
Then, start the TDD process by following the ordered list. The most specialized scenarios are typically the easiest to implement,
and this is why you should start with them. Specialized scenarios typically include edge cases and failure scenarios.

In the simplest case, a specialized scenario can be implemented, for example, by returning a constant from a function.
An example of a specialized scenario with a `List` class's `isEmpty` method is when the list is empty after creating a new `List` object.
Test that first, and only after that test a scenario where something is added to the list, and it is no longer empty.

At the end of the scenario list are the generalized scenarios that should make the function work with any input.
To summarize, during the TDD process, you work from a specialized implementation towards a more generalized implementation.

The drawback of TDD is that you cannot always be 100% sure if you have a generalized enough implementation.
You can only be 100% sure that your implementation works with the input used in the tests, but you cannot be sure if any input works correctly.
To ensure that, you would have to test all possible input values (like all integer values), which is typically unreasonable.

Let's have another TDD example with a function that has edge cases. We should implement a `contains` method for a Java string class
(we assume here that the Java String class's `contains` method does not exists). The method should do the following:

> *The method takes a string argument, and if that string is found in the string the string object represents, then `true` is returned. Otherwise, `false` is returned.*

There are several scenarios we might want to test to make sure that the function works correctly in every
case:

* Edge cases
    1) Strings are equal
    2) Both strings are empty
    3) Argument string is empty
    4) The string under test is empty
    5) Argument string is found at the beginning of the other string
    6) Argument string is found at the end of the other string
    7) Argument string is longer than the other string

* Happy paths
    8) Argument string is found in the middle of the other string
    9) Argument string is not found in the other string

Let's start with the first scenario:

```java
class MyStringTests {
  @Test
  void testContains_whenStringsAreEqual() {
    // GIVEN
    final var string = new MyString("String");
    final var another_string = "String";

    // WHEN
    final var strContainsAnotherStr =
      string.contains(anotherString);

    // THEN
    assertTrue(strContainsAnotherStr);
}
```

Next, we do as much implementation as is needed to make the above test pass. The simplest thing to do is to return
a constant:

```java
public class MyString {
  private final String value;

  public MyString(final String value) {
    this.value = value;
  }

  boolean contains(final String anotherString) {
    return true;
  }
}
```

Let's add a failing test for the second scenario:

```java
class MyStringTests {
  // ...

  @Test
  void testContains_whenBothStringsAreEmpty() {
    // GIVEN
    final var string = new MyString("");
    final var anotherString = "";

    // WHEN
    final var strContainsAnotherStr =
      string.contains(anotherString);

    // THEN
    assertTrue(strContainsAnotherStr);
}
```

We don't have to modify the implementation to make the above test pass.
Let's add a test for the third scenario:

```java
class MyStringTests {
  // ...

  @Test
  void testContains_whenArgumentStringsIsEmpty() {
    // GIVEN
    final var string = new MyString("String");
    final var anotherString = "";

    // WHEN
    final var strContainsAnotherStr =
      string.contains(anotherString);

    // THEN
    assertTrue(strContainsAnotherStr);
}
```

We don't have to modify the implementation to make the above test pass.
Let's add a test for the fourth scenario:

```java
class MyStringTests {
  // ...

  @Test
  void testContains_whenStringsIsEmpty() {
    // GIVEN
    final var string = new MyString("");
    final var anotherString = "String";

    // WHEN
    final var strContainsAnotherStr =
      string.contains(anotherString);

    // THEN
    assertFalse(strContainsAnotherStr);
}
```

Let's modify the implementation to make the above (and earlier tests) to pass:

```java
public class MyString {
  private final String value;

  public MyString(final String value) {
    this.value = value;
  }

  boolean contains(final String anotherString) {
    if (value.isEmpty() && !anotherString.isEmpty()) {
      return false;
    }

    return true;
  }
}
```

Let's add a test for the fifth scenario:

```java
class MyStringTests {
  // ...

  @Test
  void testContains_whenArgStringIsFoundAtBegin() {
    // GIVEN
    final var string = new MyString("String");
    final var anotherString = "Str";

    // WHEN
    final var strContainsAnotherStr =
      string.contains(anotherString);

    // THEN
    assertTrue(strContainsAnotherStr);
}
```

We don't have to modify the implementation to make the above test pass.
Let's add a test for the sixth scenario:

```java
class MyStringTests {
  // ...

  @Test
  void testContains_whenArgStringIsFoundAtEnd() {
    // GIVEN
    final var string = new MyString("String");
    final var anotherString = "ng";

    // WHEN
    final var strContainsAnotherStr =
      string.contains(anotherString);

    // THEN
    assertTrue(strContainsAnotherStr);
}
```

Let's add a test for the seventh scenario:

```java
class MyStringTests {
  // ...

  @Test
  void testContains_whenArgStringIsLongerThanOtherString() {
    // GIVEN
    final var string = new MyString("String");
    final var anotherString = "String111";

    // WHEN
    final var strContainsAnotherStr =
      string.contains(anotherString);

    // THEN
    assertFalse(strContainsAnotherStr);
}
```

Let's modify the implementation:

```java
public class MyString {
  private final String value;

  public MyString(final String value) {
    this.value = value;
  }

  boolean contains(final String anotherString) {
    if (value.isEmpty() && !anotherString.isEmpty() ||
        anotherString.length() > value.length()
    ) {
      return false;
    }

    return true;
  }
}
```

Let's add a test for the eigth scenario:

```java
class MyStringTests {
  // ...

  @Test
  void testContains_whenArgStringIsFoundInMiddle() {
    // GIVEN
    final var string = new MyString("String");
    final var anotherString = "ri";

    // WHEN
    final var strContainsAnotherStr =
      string.contains(anotherString);

    // THEN
    assertTrue(strContainsAnotherStr);
}
```

We don't have to modify the implementation to make the above test pass.
Let's add the final test:

```java
class MyStringTests {
  // ...

  @Test
  void testContains_whenArgStringIsNotFound() {
    // GIVEN
    final var string = new MyString("String");
    final var anotherString = "aa";

    // WHEN
    final var strContainsAnotherStr =
      string.contains(anotherString);

    // THEN
    assertFalse(strContainsAnotherStr);
}
```

Let's modify the implementation to make the above (and, of course, earlier tests) to pass:

```java
public class MyString {
  private final String value;

  public MyString(final String value) {
    this.value = value;
  }

  boolean contains(final String anotherString) {
    if (value.isEmpty() && !anotherString.isEmpty() ||
        anotherString.length() > value.length() ||
        value.indexOf(anotherString) == -1
    ) {
      return false;
    }

    return true;
  }
}
```

Next, we should refactor. We can notice that the if-statement condition can be simplified to the following:

```java
public class MyString {
  private final String value;

  public MyString(final String value) {
    this.value = value;
  }

  boolean contains(final String anotherString) {
    if (value.indexOf(anotherString) == -1) {
      return false;
    }

    return true;
  }
}
```

You may have noticed that some refactoring was needed until we came up with the final solution. This is what
happens with TDD. You only consider implementation one scenario at a time, which can result in writing code that
will be removed/replaced when making a test for the next scenario pass. This is called *emergent design*.

The above example is
a bit contrived because we finally used the `indexOf` method, which we could have done already in the first test case,
but we didn't because we were supposed to write the simplest implementation to make the test pass.
Consider the same example when no `indexOf` method is available, and we must implement the `indexOf` method functionality
(looping through the characters, etc.) in the `contains` method ourselves. Then, all the tests start to make sense.
Many of them are testing edge/corner cases that are important to test. If you are still in doubt, think about implementing
the `contains` method in a language that does not have a range-based for-loop, but you must use character indexes.
Then, you would finally understand the importance of testing the edge cases: reveal possible off-by-one errors, for example.

When you encounter a bug, it is usually due to a missing scenario: An edge case is not considered, or implementation for a failure scenario or happy path is missing.
To remedy the bug, you should practice TDD by adding a failing test for the missing scenario and then make the bug correction in the code to make the added test (and other tests) pass.

There will be one more TDD example later in this chapter when we have an example using BDD, DDD, OOD, and TDD. If you are not yet fully convinced about
TDD, the following section presents an alternative to TDD that is still better than doing tests last.

#### Unit Specification-Driven Development (USDD)

For some of you, the above-described TDD cycle may sound cumbersome, or you may not be fully convinced about the benefits it brings.
For this reason, I am presenting an alternative to TDD that you can use until you feel ready to try TDD out.
The approach I am presenting is inferior to TDD but superior to the traditional "test-last" approach because it reduces
the number of bugs by concentrating on the unit (function) specification before the implementation.
There are clear benefits in specifying the function behavior beforehand. I call this approach *unit specification-driven development* (USDD).
When function behavior is defined first, one is usually less likely to forget to test or implement something.
The USDD approach forces you to consider the function specification: happy path(s), possible security issues, edge, and failure cases.

If you don't practice USDD and always do the implementation first, it is more likely you will forget an edge case or a particular
failure/security scenario. When you don't practice USDD, you go straight to the implementation, and you tend to think about
the happy path(s) only and strive to get them working. When focusing 100% on getting the happy path(s) working,
you don't consider the edge cases and failure/security scenarios. You might forget to implement them or at least some of them.
If you forget to implement an edge case or failure scenario, you don't also test it. You
can have 100% unit test coverage for a function, but a particular edge case or failure/security scenario is left
unimplemented and untested. This is what has happened to me, also. And it has happened more than once. After realizing that the USDD approach
could save me from those bugs, I started to take it seriously.

You can conduct USDD as an alternative to TDD/BDD. In USDD,
you first specify the unit (i.e., function). You extract all the needed tests from the function specification, including
the "happy path" or "happy paths", edge cases, and failure/security scenarios. Then, you put a `fail` call in all the tests so as not to forget to implement them later.
Additionally, you can add a comment on the expected result of a test. For example, in failure scenarios, you can write a comment
that tells what kind of error is expected to be raised, and in an edge case, you can put a comment that tells with the input of *x*, the output of *y* is
expected. (Later, when the tests are implemented, the comments can be removed.)

Let's say that we have the following function specification:

> *Configuration parser's `parse` method parses configuration in JSON format into a configuration object. The method should produce an error if the configuration JSON cannot be parsed. Configuration JSON consists of optional and mandatory properties (name and value of specific type). A missing mandatory property should produce an error, and a missing optional property should use a default value. Extra properties should be discarded. A property with an invalid type of value should produce an error. Two property types are supported: integer and string. Integers must have value in a specified range, and strings have a maximum length. The mandatory configuration properties are the following: `name` (`type`) ... The optional configuration properties are the following: `name` (`type`) ...*

Let's first write a failing test case for the "happy path" scenario:

```java
class ConfigParserTests {
  @Test
  void testParse_whenSuccessful() {
    // Happy path scenario, returns a 'Config' object
    fail();
}
```

Next, let's write a failing test case for the other scenarios extracted from the above function specification:

```java
class ConfigParserTests {
  // ...

  @Test
  void testParse_whenParsingFails() {
    // Failure scenario, should produce an error
    fail();
  }

  @Test
  void testParse_whenMandatoryPropIsMissing() {
    // Failure scenario, should produce an error
    fail();
  }

  @Test
  void testParse_whenOptionalPropIsMissing() {
    // Should use default value
    fail();
  }

  @Test
  void testParse_withExtraProps() {
    // Extra props should be discarded
    fail();
  }

  @Test
  void testParse_whenPropHasInvalidType() {
    // Failure scenario, should produce an error
    fail();
  }

  @Test
  void testParse_whenIntegerPropOutOfRange() {
    // Input validation security scenario, should produce an error
    fail();
  }

  @Test
  void testParse_whenStringPropTooLong() {
    // Input validation security scenario, should produce an error
    fail();
  }
}
```

Now, you have a high-level specification of the function in the form of scenarios. Next, you can continue with
the function implementation. After you have completed the function implementation, implement the tests
one by one and remove the `fail` calls from them.

Compared to TDD, the benefit of this approach is that you don't have to switch continuously between the implementation source
code file and the test source code file. In each phase, you can focus on one thing:

1) Function specification
   - What does the function do? (The happy path scenario(s))
   - What failures are possible? (The failure scenario(s))
     - For example, if a function makes a REST API call, all scenarios related to the failure of the call should
       be considered: connection failure, timeout, response status code not being 2xx, any response data parsing failures
   - Are there security issues? (The security scenarios)
     - For example, if the function gets input from the user, it must be validated, and in case of invalid input, a proper action is taken
       , like raising an error. Input from the user can be obtained via environment variables, reading files, reading a network socket
       , and reading standard input.
   - Are there edge cases? (The edge case scenario(s))
   - When you specify the function, it is not mandatory to write the specification down. You can do it in your head if the function is simple. With a more complex function, you might benefit from writing the specification down to fully understand what the function should do
2) Define different scenarios as failing unit tests
3) Function implementation
4) Implementation of unit tests

In real life, the initial function specification is not always 100% correct or complete. During the function implementation,
you might discover, e.g., a new failure scenario that was not in the initial function specification. You should immediately
add a new failing unit test for that new scenario so you don't forget to implement it later. Once you think your function implementation
is complete, go through the function code line-by-line and check if any line can produce an error that has not yet been considered.
Having this habit will reduce the possibility of accidentally leaving some error unhandled in the function code.

Sometimes, you need to modify an existing function because you are not always able to follow the *open-closed principle*
for various reasons, such as not possible or feasible. When you need to modify an existing function, follow the below steps:

1) Specification of changes to the function
   - What changes in function happy path scenarios?
   - What changes in failure scenarios?
   - What changes in security scenarios?
   - What changes in edge cases?
2) Add/Remove/Modify tests
    - Add new scenarios as failing tests
    - Remove tests for removed scenarios
    - Modify existing tests
3) Implementation changes to the function
4) Implement added unit tests

Let's have an example where we change the configuration parser so that it should produce an error if the configuration
contains extra properties. Now we have the specification of the change defined. Next, we need to modify the tests.
We need to modify the `testParseWithExtraProps` method as follows:

```java
class ConfigParserTests {
  // ...

  @Test
  void testParse_withExtraProps() {
    // Change this scenario so that an error
    // is expected
  }
}
```

Next, we implement the wanted change and check that all tests pass.

Let's have another example where we change the configuration parser so that the configuration can be given
in YAML in addition to JSON. We need to add the following failing unit tests:

```java
class ConfigParserTests {
  // ...

  @Test
  void testParse_whenYamlParsingSucceeds() {
    fail();
  }

  @Test
  void testParse_whenYamlParsingFails(self):
    // Should produce an error
    fail();
}
```

We should also rename the following test methods: `testParse_whenSuccessful` and `testParse_whenParsingFails` to
`testParse_whenJsonParsingSucceeds` and `testParse_whenJsonParsingFails`. Next, we
implement the changes to the function, and lastly, we implement the two new tests. (Depending on the actual test implementation, you may
or may not need to make small changes to JSON parsing-related tests to make them pass.)

As the final example, let's make the following change: Configuration has no optional properties, but all properties are
mandatory. This means that we can remove the following test: `testParse_whenOptionalPropIsMissing`. We also need
to change the `testParse_whenMandatoryPropIsMissing` test:

```java
class ConfigParserTests {
  // ...

  @Test
  void testParse_whenMandatoryPropIsMissing() {
    // Change this scenario so that an error
    // is expected
  }
}
```

Once we have implemented the change, we can run all the tests and ensure they pass.

I have presented two alternative methods for writing unit tests: TDD and USDD.
As a professional developer, you should use either of them. TDD brings more benefits, but USDD is much better than *test-last*,
i.e., writing unit tests only after implementation is ready. If you think you are not ready for TDD yet, try USDD first and reconsider TDD at some point in the future.

#### Naming Conventions

When functions to be tested are in a class, a respective class for unit tests should be created.
For example, if there is a `ConfigParser` class, the respective class for unit tests should be
`ConfigParserTests`. This makes locating the file containing unit tests for a particular
implementation class easy.

If you are naming your test methods according to the method they test, use the following naming convention.
For example, if the tested method is `tryParse`, the test method name should be `testParse`.
There are usually several tests for a single function. All test method
names should begin with *test&lt;function-name&gt;*, but the test method name should also contain a description
of the specific scenario the test method tests, for example: `testParse_whenParsingFails`. The method name and
the scenario should be separated by a single underscore to enhance readability.

When using the BDD-style Jest testing library with JavaScript or TypeScript, unit tests are organized and named in the following
manner:

```js
describe('<class-name>', () => {
  describe('<public-method-name>', () => {
    it('should do this...', () => {
      // ...
    });

    it('should do other thing when...', () => {
      // ...
    });

    // Other scenarios...
  });
});

// Example:

describe('ConfigParser', () => {
  describe('parse', () => {
    it('should parse config string successfully', () => {
      // ...
    });

    it('should throw an error if parsing fails', () => {
      // ...
    });

    // Other scenarios...
  });
});
```

#### Mocking
Mocks are one form of *test doubles*. Test doubles are any kind of pretend objects used in place of real objects for testing purposes.
The following kinds of test doubles can be identified:

- *Fakes* are objects that have working implementations but are usually simplified versions (suitable for testing) of the real implementations.
- *Stubs* are objects providing fixed responses to calls made to them.
- *Spies* are stubs that record information based on how they were called.
- *Mocks* are the most versatile test doubles. They are objects pre-programmed with expectations (e.g., what a method should return when it is called), and like spies, they record information based on how they were called, and those calls can be verified in the test. Mocks are probably the ones you use on a daily basis.

Let's have a small Spring Boot example of mocking dependencies in unit tests. We have a service class that
contains public functions for which we want to write unit tests:

{title: "SalesItemServiceImpl.java"}
```java
@Service
public class SalesItemServiceImpl implements SalesItemService {
  @Autowired
  private SalesItemRepository salesItemRepository;

  @AutoWired
  private SalesItemFactory salesItemFactory;

  @Override
  public final SalesItem createSalesItem(
    final InputSalesItem inputSalesItem
  ) {
    return salesItemRepository.save(
      salesItemFactory.createFrom(inputSalesItem));
  }

  @Override
  public final Iterable<SalesItem> getSalesItems() {
    return salesItemRepository.findAll();
  }
}
```

In the Spring Boot project, we need to define the following dependency in the `build.gradle` file:

```groovy
dependencies {
  // Other dependencies ...
  testImplementation 'org.springframework.boot:spring-boot-starter-test'
}
```

Now, we can create unit tests using JUnit, and we can use the [Mockito](https://site.mockito.org/) library for mocking. Looking at the above code, we can notice
that the `SalesItemServiceImpl` service depends on a `SalesItemRepository`. According to the unit testing principle, we should mock
that dependency. Similarly, we should also mock the `SalesItemFactory` dependency:

{title: "SalesItemServiceTests.java"}
```java
import org.junit.jupiter.api.Test;
import org.mockito.InjectMocks;
import org.mockito.Mock;
import org.mockito.Mockito;
import org.springframework.boot.test.context.SpringBootTest;

import java.util.List;

import static org.junit.jupiter.api.Assertions.assertEquals;
import static org.mockito.ArgumentMatchers.any;
import static org.mockito.ArgumentMatchers.refEq;


@SpringBootTest
class SalesItemServiceTests {
  private static final String SALES_ITEMS_NOT_EQUAL = "Sales items not equal";

  // Create sales item mock
  @Mock
  private SalesItem mockSalesItem;

  // Create input sales item mock
  @Mock
  private InputSalesItem mockInputSalesItem;

  // Create mock implementation of
  // SalesItemRepository interface
  @Mock
  private SalesItemRepository mockSalesItemRepository;

  // Create mock implementation of
  // SalesItemFactory interface
  @Mock
  private SalesItemFactory mockSalesItemFactory;

  // Injects 'mockSalesItemRepository' and 'mockSalesItemFactory'
  // mocks to 'salesItemService'
  @InjectMocks
  private SalesItemService salesItemService = new SalesItemServiceImpl();

  @Test
  final void testCreateSalesItem() {
    // GIVEN
    // Instructs to return 'mockSalesItem' when
    // salesItemFactoryMock's createFrom
    // method is called with an argument that reference
    // equals 'mockInputSalesItem'
    Mockito
      .when(salesItemFactoryMock.createFrom(refEq(mockInputSalesItem)))
      .thenReturn(mockSalesItem);

    // Instructs to return 'mockSalesItem' when
    // salesItemRepositoryMock's 'save' method is called
    // with an argument that reference equals 'mockSalesItem'
    Mockito
      .when(salesItemRepositoryMock.save(refEq(mockSalesItem)))
      .thenReturn(mockSalesItem);

    // WHEN
    final var salesItem = salesItemService.createSalesItem(mockInputSalesItem);

    // THEN
    assertEquals(mockSalesItem, salesItem, SALES_ITEMS_NOT_EQUAL);
  }

  @Test
  final void testGetSalesItems() {
    // GIVEN
    // Instructs to return a list of containing one sales item
    // 'mockSalesItem' when salesItemRepository's 'findAll'
    // method is called
    Mockito
      .when(salesItemRepositoryMock.findAll())
      .thenReturn(List.of(mockSalesItem));

    // WHEN
    final var salesItems = salesItemService.getSalesItems();

    // THEN
    final var iterator = salesItems.iterator();

    assertEquals(iterator.next(), mockSalesItem, SALES_ITEMS_NOT_EQUAL);
    assertFalse(iterator.hasNext());
  }
}
```

Java has many testing frameworks and mocking libraries. Below is a small example from a JakartaEE
microservice that uses [TestNG](https://testng.org/) and [JMockit](https://jmockit.github.io/) libraries for unit testing and mocking, respectively. In the below example, we are testing
a couple of methods from a `ChartStore` class, which is responsible for handling the persistence of chart entities
using Java Persistence API (JPA).

{title: "ChartStoreTests.java"}
```java
import com.silensoft.conflated...DuplicateEntityError;
import mockit.Expectations;
import mockit.Injectable;
import mockit.Mocked;
import mockit.Tested;
import mockit.Verifications;
import org.testng.annotations.Test;

import javax.persistence.EntityExistsException;
import javax.persistence.EntityManager;

import java.util.Collections;
import java.util.List;

import static org.testng.Assert.assertEquals;
import static org.testng.Assert.fail;


public class ChartRepositoryTests {
  // chartRepository will contain an instance
  // of ChartRepositoryImpl after @Tested
  // annotation is processed
  @Tested
  private ChartRepositoryImpl chartRepository;

  // @Injectable annotation creates a mock instance
  // of EntityManager interface and then injects
  // it to tested object, in this case 'chartRepository'
  // ChartRepositoryImpl class has an attribute of
  // type EntityManager which is annotated with an
  // @Inject annotation from JakartaEE
  @Injectable
  private EntityManager mockEntityManager;

  // Create a mock instance of Chart
  // (does not inject anywhere)
  @Mocked
  private Chart mockChart;

  @Test
  void testCreate() {
    // WHEN
    chartRepository.create(mockChart);

    // THEN
    // JMockit's verification block checks
    // that below mock functions are called
    new Verifications() {{
      mockEntityManager.persist(mockChart);
      mockEntityManager.flush();
    }};
  }

  @Test
  void testCreate_whenChartAlreadyExists() {
    // GIVEN
    // JMockit's expectations block will define what mock methods
    // calls are expected and also can specify
    // the return value or result of the mock method call.
    // Below the 'persist' mock method call will throw
    // EntityExistsException
    new Expectations() {{
      mockEntityManager.persist(mockChart);
      result = new EntityExistsException();
    }};

    // WHEN + THEN
    assertThrows(
      DuplicateEntityError.class,
      () -> chartRepository.create(mockChart)
    );
  }

  @Test
  void testGetById() {
    // GIVEN
    new Expectations() {{
      mockEntityManager.find(Chart.class, 1L);
      result = mockChart;
    }};

    // WHEN
    final var chart = chartRepository.getById(1L);

    // THEN
    assertEquals(mockChart, chart);
  }
}
```

Let's have a unit testing example with JavaScript/TypeScript. We will write a unit test for the following
function using the Jest library:

{title: "fetchTodos.ts"}

```ts
import store from '../../store/store';
import todoService from '../services/todoService';

export default async function fetchTodos(): Promise<void> {
  const { todoState } = store.getState();
  todoState.isFetching = true;

  try {
    todoState.todos = await todoService.tryFetchTodos();
    todoState.fetchingHasFailed = false;
  } catch(error) {
    todoState.fetchingHasFailed = true;
  }

  todoState.isFetching = false;
}
```

Below is the unit test case for the happy path scenario:

{title: "fetchTodos.test.ts"}
```ts
import store from '../../store/store';
import todoService from '../services/todoService';
import fetchTodos from 'fetchTodos';
// ...

// Mock both 'store' and 'todoService' objects
jest.mock('../../store/store');
jest.mock('../services/todoService');

describe('fetchTodos', async () => {
  it('should fetch todos from todo service', async () => {
    // GIVEN
    const todoState = { todos: [] } as TodoState;
    store.getState.mockReturnValue({ todoState });

    const todos = [{
      id: 1,
      name: 'todo',
      isDone: false
    }];

    todoService.tryFetchTodos.mockResolvedValue(todos);

    // WHEN
    await fetchTodos();

    // THEN
    expect(todoState.isFetching).toBe(false);
    expect(todoState.fetchingHasFailed).toBe(false);
    expect(todoState.todos).toBe(todos);
  });
});
```

In the above example, we used the `jest.mock` function to create mocked versions of the `store` and `todoService` modules.
Another way to handle mocking with Jest is using `jest.fn()`, which creates a mocked function. Let's assume that the `fetchTodos` function is changed so
that it takes a `store` and `todoService` as its arguments:

{title: "fetchTodos.ts"}
```ts
// ...

export default async function fetchTodos(
  store: Store,
  todoService: TodoService
): Promise<void> {
  // Same code here as in above example...
}
```

Now the mocking would look like the following:

{title: "fetchTodos.test.ts"}
```
import fetchTodos from 'fetchTodos';
// ...

const store = {
   getState: jest.fn()
};

const todoService = {
   tryFetchTodos: jest.fn();
}

describe('fetchTodos', async () => {
  it('should fetch todos from todo service', async () => {
    // GIVEN
    // Same code as in earlier example...

    // WHEN
    await fetchTodos(store as any, todoService as any);

    // THEN
    // Same code as in earlier example...
  });
});
```

Let's have an example with C++ and Google Test unit testing framework. In C++, you can define a mock class
by extending a pure virtual base class ("interface") and using Google Mock macros to define mocked methods.
Below is the definition of a `detectedAnomalies` method that we want to unit test:

{title: "AnomalyDetectionEngine.h"}
```cpp
class AnomalyDetectionEngine
{
public:
  virtual ~AnomalyDetectionEngine() = default;

  virtual void detectAnomalies() = 0;
};
```

{title: "AnomalyDetectionEngineImpl.h"}
```cpp
#include <memory>
#include "AnomalyDetectionEngine.h"
#include "Configuration.h"


class AnomalyDetectionEngineImpl : public AnomalyDetectionEngine
{
public:
  explicit AnomalyDetectionEngineImpl(
    std::shared_ptr<Configuration> configuration
  );

  void detectAnomalies() override;

private:
  void detectAnomalies(
    const std::shared_ptr<AnomalyDetectionRule>& anomalyDetectionRule
  );

  std::shared_ptr<Configuration> m_configuration;
};
```

{title: "AnomalyDetectionEngineImpl.cpp"}
```cpp
#include <algorithm>
#include <execution>
#include "AnomalyDetectionEngineImpl.h"


AnomalyDetectionEngineImpl::AnomalyDetectionEngineImpl(
  std::shared_ptr<Configuration> configuration
) : m_configuration(std::move(configuration))
{}

void AnomalyDetectionEngineImpl::detectAnomalies()
{
  const auto anomalyDetectionRules =
    m_configuration->getAnomalyDetectionRules();

   std::for_each(std::execution::par,
                 anomalyDetectionRules->cbegin(),
                 anomalyDetectionRules->cend(),
                 [this](const auto& anomalyDetectionRule)
                 {
                   detectAnomalies(anomalyDetectionRule);
                 });
}

void AnomalyDetectionEngineImpl::detectAnomalies(
  const std::shared_ptr<AnomalyDetectionRule>& anomalyDetectionRule
)
{
  const auto anomalyIndicators = anomalyDetectionRule->detectAnomalies();

  std::ranges::for_each(*anomalyIndicators,
                        [](const auto& anomalyIndicator)
                        {
                          anomalyIndicator->publish();
                        });
}
```

Let's create a `Configuration` class and a `ConfigurationMock` class for mocks:

{title: "Configuration.h"}
```cpp
#include <memory>
#include <vector>
#include "AnomalyDetectionRule.h"


class Configuration
{
public:
  virtual ~Configuration() = default;

  virtual std::shared_ptr<AnomalyDetectionRules>
  getAnomalyDetectionRules() const = 0;
};
```

{title: "ConfigurationMock.h"}
```cpp
#include <gmock/gmock.h>
#include "Configuration.h"


class ConfigurationMock : public Configuration
{
public:
  MOCK_METHOD(std::shared_ptr<AnomalyDetectionRules>,
              getAnomalyDetectionRules, (), (const)
  );
};
```

Let's create an `AnomalyDetectionRule` class and a respective mock class, `AnomalyDetectionRuleMock`:

{title: "AnomalyDetectionRule.h"}
```cpp
#include "AnomalyIndicator.h"


class AnomalyDetectionRule
{
public:
  virtual ~AnomalyDetectionRule() = default;

  virtual std::shared_ptr<AnomalyIndicators>
  detectAnomalies() = 0;
};

using AnomalyDetectionRules =
  std::vector<std::shared_ptr<AnomalyDetectionRule>>;
```

{title: "AnomalyDetectionRuleMock.h"}
```cpp
#include <gmock/gmock.h>
#include "AnomalyDetectionRule.h"


class AnomalyDetectionRuleMock : public AnomalyDetectionRule
{
  public:
    MOCK_METHOD(std::shared_ptr<AnomalyIndicators>,
                detectAnomalies, ());
};
```

Let's create an `AnomalyIndicator` class and a mock class, `AnomalyIndicatorMock`:

{title: "AnomalyIndicator.h"}
```cpp
#include <memory>
#include <vector>


class AnomalyIndicator
{
public:
  virtual ~AnomalyIndicator() = default;

  virtual void publish() = 0;
};

using AnomalyIndicators =
  std::vector<std::shared_ptr<AnomalyIndicator>>;
```

{title: "AnomalyIndicatorMock.h"}
```cpp
#include <gmock/gmock.h>
#include "AnomalyIndicator.h"


class AnomalyIndicatorMock : public AnomalyIndicator
{
public:
  MOCK_METHOD(void, publish, ());
};
```

Let's create a unit test for the `detectAnomalies` method in the `AnomalyDetectionEngineImpl` class:

{title: "AnomalyDetectionEngineImplTests.h"}
```cpp
#include <gtest/gtest.h>
#include "ConfigurationMock.h"
#include "AnomalyDetectionRuleMock.h"
#include "AnomalyIndicatorMock.h"


class AnomalyDetectionEngineImplTests : public testing::Test
{
protected:
  void SetUp() override {
    m_anomalyDetectionRules->push_back(m_anomalyDetectionRuleMock);
    m_anomalyIndicators->push_back(m_anomalyIndicatorMock);
  }

  std::shared_ptr<ConfigurationMock> m_configurationMock{
    std::make_shared<ConfigurationMock>()
  };

  std::shared_ptr<AnomalyDetectionRuleMock> m_anomalyDetectionRuleMock{
    std::make_shared<AnomalyDetectionRuleMock>()
  };

  std::shared_ptr<AnomalyDetectionRules> m_anomalyDetectionRules{
    std::make_shared<AnomalyDetectionRules>()
  };

  std::shared_ptr<AnomalyIndicatorMock> m_anomalyIndicatorMock{
    std::make_shared<AnomalyIndicatorMock>()
  };

  std::shared_ptr<AnomalyIndicators> m_anomalyIndicators{
    std::make_shared<AnomalyIndicators>()
  }
};
```

{title: "AnomalyDetectionEngineImplTests.cpp"}
```cpp
#include "../src/AnomalyDetectionEngineImpl.h"
#include "AnomalyDetectionEngineImplTests.h"


using testing::Return;

TEST_F(AnomalyDetectionEngineImplTests, testDetectAnomalies)
{
  // GIVEN
  AnomalyDetectionEngineImpl anomalyDetectionEngine{m_configurationMock};

  // EXPECTATIONS
  EXPECT_CALL(*m_configurationMock, getAnomalyDetectionRules)
    .Times(1)
    .WillOnce(Return(m_anomalyDetectionRules));

  EXPECT_CALL(*m_anomalyDetectionRuleMock, detectAnomalies)
    .Times(1)
    .WillOnce(Return(m_anomalyIndicators));

  EXPECT_CALL(*m_anomalyIndicatorMock, publish).Times(1);

  // WHEN
  anomalyDetectionEngine.detectAnomalies();
}
```

The above example did not contain dependency injection, so let's have another example in C++ where
dependency injection is used. First, we define a generic base class for singletons:

{title: "Singleton.h"}
```cpp
#include <memory>


template<typename T>
class Singleton
{
public:
  Singleton() = default;

  virtual ~Singleton()
  {
    m_instance.reset();
  };

  static inline std::shared_ptr<T>& getInstance()
  {
    return m_instance;
  }

  static void setInstance(const std::shared_ptr<T>& instance)
  {
    m_instance = instance;
  }

private:
  static inline std::shared_ptr<T> m_instance;
};
```

Next, we implement a configuration parser that we will later unit test:

{title: "ConfigParserImpl.h"}
```cpp
#include <memory>
#include "Configuration.h"


class ConfigParserImpl {
public:
  std::shared_ptr<Configuration> parse();
};
```

{title: "ConfigParserImpl.cpp"}
```cpp
#include "AnomalyDetectionRulesParser.h"
#include "Configuration.h"
#include "ConfigFactory.h"
#include "ConfigParserImpl.h"
#include "MeasurementDataSourcesParser.h"


std::shared_ptr<Configuration>
ConfigParserImpl::parse(...)
{
  const auto measurementDataSources =
    MeasurementDataSourcesParser::getInstance()->parse(...);

  const auto anomalyDetectionRules =
    AnomalyDetectionRulesParser::getInstance()->parse(...);

  return ConfigFactory::getInstance()
    ->createConfig(anomalyDetectionRules);
}
```

Next, we define `MeasurementDataSource`, `MeasurementDataSourcesParser`, and
`MeasurementDataSourcesParserImpl` classes:

{title: "MeasurementDataSource.h"}
```cpp
#include <memory>
#include <vector>


class MeasurementDataSource {
  // ...
};

using MeasurementDataSources =
  std::vector<std::shared_ptr<MeasurementDataSource>>;
```

{title: "MeasurementDataSourcesParser.h"}
```cpp
#include "Singleton.h"
#include "MeasurementDataSource.h"


class MeasurementDataSourcesParser :
  public Singleton<MeasurementDataSourcesParser>
{
public:
  virtual std::shared_ptr<MeasurementDataSources> parse(...) = 0;
};
```

{title: "MeasurementDataSourcesParserImpl.h"}
```cpp
#include "MeasurementDataSourcesParser.h"


class MeasurementDataSourcesParserImpl :
  public MeasurementDataSourcesParser
{
public:
  std::shared_ptr<MeasurementDataSources> parse(...) override {
    // ...
  }
};
```

Next, we define `AnomalyDetectionRulesParser` and `AnomalyDetectionRulesParserImpl`
classes:

{title: "AnomalyDetectionRulesParser.h"}
```cpp
#include "Singleton.h"
#include "AnomalyDetectionRule.h"


class AnomalyDetectionRulesParser :
  public Singleton<AnomalyDetectionRulesParser>
{
public:
  virtual std::shared_ptr<AnomalyDetectionRules> parse(...) = 0;
};
```

{title: "AnomalyDetectionRulesParserImpl.h"}
```cpp
#include "AnomalyDetectionRulesParser.h"


class AnomalyDetectionRulesParserImpl :
  public AnomalyDetectionRulesParser
{
public:
  std::shared_ptr<AnomalyDetectionRules> parse(...) override {
   // ...
  }
};
```

Next, we define `ConfigFactory` and `ConfigFactoryImpl` classes:

{title: "ConfigFactory.h"
```cpp
#include "Singleton.h"
#include "Configuration.h"


class ConfigFactory :
  public Singleton<ConfigFactory>
{
public:
  virtual std::shared_ptr<Configuration>
  createConfig(
    const std::shared_ptr<AnomalyDetectionRules>& rules
  ) = 0;
};
```

{title: "ConfigFactoryImpl.h"}
```cpp
#include "ConfigFactory.h"


class ConfigFactoryImpl : public ConfigFactory
{
public:
  std::shared_ptr<Configuration>
  createConfig(
    const std::shared_ptr<AnomalyDetectionRules>& rules
  ) override {
    // ...
  }
};
```

Then we define a dependency injector class:

{title: "DependencyInjector.h"}
```cpp
#include "AnomalyDetectionRulesParserImpl.h"
#include "ConfigFactoryImpl.h"
#include "MeasurementDataSourcesParserImpl.h"


class DependencyInjector final
{
public:
  static void injectDependencies()
  {
    AnomalyDetectionRulesParser::setInstance(
      std::make_shared<AnomalyDetectionRulesParserImpl>()
    );

    ConfigFactory::setInstance(
      std::make_shared<ConfigFactoryImpl>()
    );

    MeasurementDataSourcesParser::setInstance(
      std::make_shared<MeasurementDataSourcesParserImpl>()
    );
  }

private:
  DependencyInjector() = default;
};
```

We inject dependencies upon application startup using the dependency injector:

{title: "main.cpp"}
```cpp
#include "DependencyInjector.h"


int main()
{
  DependencyInjector::injectDependencies();

  // Initialize and start application...
}
```

Let's define a unit test class for the `ConfigParserImpl` class:

{title: "ConfigParserImplTests.h"}
```cpp
#include "MockDependenciesInjectedTest.h"


class ConfigParserImplTests :
  public MockDependenciesInjectedTest
{};
```

All unit test classes should inherit from a base class that injects mock dependencies. When tests are completed,
the mock dependencies will be removed. The Google Test framework requires this removal because it only validates
expectations on a mock upon the mock object destruction.

{title: "MockDependenciesInjectedTest.h"}
```cpp
#include <gtest/gtest.h>
#include "MockDependencyInjector.h"


class MockDependenciesInjectedTest :
  public testing::Test
{
protected:
  void SetUp() override
  {
    m_mockDependencyInjector.injectMockDependencies();
  }

  void TearDown() override
  {
    m_mockDependencyInjector.removeMockDependencies();
  }

  MockDependencyInjector m_mockDependencyInjector;
};
```

Below are all the mock classes defined:

{title: "AnomalyDetectionRulesParserMock.h"}
```cpp
#include <gmock/gmock.h>
#include "AnomalyDetectionRulesParser.h"


class AnomalyDetectionRulesParserMock :
  public AnomalyDetectionRulesParser
{
public:
  MOCK_METHOD(std::shared_ptr<AnomalyDetectionRules>, parse, (...));
};
```

{title: "ConfigFactoryMock.h"}
```cpp
#include <gmock/gmock.h>
#include "ConfigFactory.h"


class ConfigFactoryMock : public ConfigFactory
{
public:
  MOCK_METHOD(
    std::shared_ptr<Configuration>,
    createConfig,
    (const std::shared_ptr<AnomalyDetectionRules>& rules)
  );
};
```

{title: "MeasurementDataSourcesParserMock.h"}
```cpp
#include <gmock/gmock.h>
#include "MeasurementDataSourcesParser.h"


class MeasurementDataSourcesParserMock :
  public MeasurementDataSourcesParser
{
public:
  MOCK_METHOD(std::shared_ptr<MeasurementDataSources>, parse, (...));
};
```

Below is the `MockDependencyInjector` class defined:

{title: "MockDependencyInjector.h"}
```cpp
#include "AnomalyDetectionRulesParserMock.h"
#include "ConfigFactoryMock.h"
#include "MeasurementDataSourcesParserMock.h"


class MockDependencyInjector final
{
public:
  std::shared_ptr<AnomalyDetectionRulesParserMock>
  m_anomalyDetectionRulesParserMock{
    std::make_shared<AnomalyDetectionRulesParserMock>()
  };

  std::shared_ptr<ConfigFactoryMock> m_configFactoryMock{
    std::make_shared<ConfigFactoryMock>()
  };

  std::shared_ptr<MeasurementDataSourcesParserMock>
  m_measurementDataSourcesParserMock{
    std::make_shared<MeasurementDataSourcesParserMock>()
  };

  void injectMockDependencies() const
  {
    AnomalyDetectionRulesParser::setInstance(
      m_anomalyDetectionRulesParserMock
    );

    ConfigFactory::setInstance(
      m_configFactoryMock
    );

    MeasurementDataSourcesParser::setInstance(
      m_measurementDataSourcesParserMock
    );
  }

  void removeMockDependencies() const {
    AnomalyDetectionRulesParser::setInstance({nullptr});
    ConfigFactory::setInstance({nullptr});
    MeasurementDataSourcesParser::setInstance({nullptr});
  }
};
```

Below is the unit test implementation that uses the mocks:

{title: "ConfigParserImplTests.cpp"}
```cpp
#include "ConfigParserImplTests.h"
#include "ConfigParserImpl.h"


using testing::Eq;
using testing::Return;

TEST_F(ConfigParserImplTests, testParseConfig)
{
  // GIVEN
  ConfigParserImpl configParser;

  // EXPECTATIONS
  EXPECT_CALL(
    *m_mockDependencyInjector.m_anomalyDetectionRulesParserMock,
    parse
  ).Times(1)
   .WillOnce(Return(m_anomalyDetectionRules));

  EXPECT_CALL(
    *m_mockDependencyInjector.m_measurementDataSourcesParserMock,
    parse
  ).Times(1)
   .WillOnce(Return(m_measurementDataSources));

  EXPECT_CALL(
    *m_mockDependencyInjector.m_configFactoryMock,
    createConfig(Eq(m_anomalyDetectionRules))
  ).Times(1)
   .WillOnce(Return(m_configMock));

  // WHEN
  const auto configuration = configParser.parse();

  // THEN
  ASSERT_EQ(configuration, m_configMock);
}
```

You can also make sure that implementation class instances can be created only in the `DependencyInjector` class by declaring
implementation class constructors private and making the `DependencyInjector` class a friend of the implementation classes.
In this way, no one can accidentally create an instance of an implementation class. Instances of implementation classes should be created
by the dependency injector only. Below is a implementation class where the constructor is made private, and the dependency
injector is made a friend of the class:

{title: "AnomalyDetectionRulesParserImpl.h"}
```cpp
#include "AnomalyDetectionRulesParser.h"

class AnomalyDetectionRulesParserImpl :
  public AnomalyDetectionRulesParser
{
  friend class DependencyInjector;

public:
  std::shared_ptr<AnomalyDetectionRules> parse() override;

private:
  AnomalyDetectionRulesParserImpl() = default;
};
```

#### Web UI Component Testing

UI component testing differs from regular unit testing because you cannot necessarily test the functions of a
UI component in isolation if you have, for example, a React functional component. You must conduct UI component testing
by mounting the component to the DOM and then perform tests by triggering events, for example. This way, you can test
the event handler functions of a UI component. The rendering part should also be tested. It can be tested by producing a snapshot of
the rendered component and storing that in version control. Further rendering tests should compare the rendered result to the snapshot stored in the version control.

Below is an example of testing the rendering of a React component, `NumberInput`:

{title: "NumberInput.test.jsx"}
```js
import renderer from 'react-test-renderer';
// ...

describe('NumberInput') () => {
  // ...

  describe('render', () => {
    it('renders with buttons on left and right"', () => {
      const numberInputAsJson =
        renderer
          .create(<NumberInput buttonPlacement="leftAndRight"/>)
          .toJSON();

      expect(numberInputAsJson).toMatchSnapshot();
    });

    it('renders with buttons on right', () => {
      const numberInputAsJson =
        renderer
          .create(<NumberInput buttonPlacement="right"/>)
          .toJSON();

      expect(numberInputAsJson).toMatchSnapshot();
    });
  });
});
```

Below is an example unit test for the number input's decrement button's click event handler function,
`decrementValue`:

{title: "NumberInput.test.jsx"}
```js
import { render, fireEvent, screen } from '@testing-library/react'
// ...

describe('NumberInput') () => {
  // ...

  describe('decrementValue', () => {
    it('should decrement value by given step amount', () => {
      render(<NumberInput value="3" stepAmount={2} />);
      fireEvent.click(screen.getByText('-'));
      const numberInputElement = screen.getByDisplayValue('1');
      expect(numberInputElement).toBeTruthy();
    });
  });
});
```

In the above example, we used the [testing-library](https://testing-library.com/), which has implementations for all the common UI
frameworks: React, Vue, and Angular. It means you can use mostly the same testing API regardless
of your UI framework. There are tiny differences, basically only in the syntax of the `render` method.
If you had implemented some UI components and unit tests for them with React, and you would like to reimplement them with Vue, you don't need to reimplement all the unit tests. You only
need to modify them slightly (e.g., make changes to the `render` function calls). Otherwise, the existing
unit tests should work because the behavior of the UI component
did not change, only its internal implementation technology from React to Vue.

### Software Component Integration Testing Principle

> ***Integration testing aims to test that a software component works against actual dependencies and that its public methods***
> ***correctly understand the purpose and signature of other public methods they are using.***

The target of software component integration testing is that all public functions of the software component should be touched by at least one integration test.
Not all functionality of the public functions should be tested because that has already been done in the unit testing phase. This is
why there are fewer integration tests than unit tests. The term *integration testing* sometimes refers to integrating
a complete software system or a product. However, it should be used to describe software
component integration only. When testing a product or a software system, instead of *product integration* or *system integration*, the term *end-to-end testing* should be preferred to avoid
confusion and misunderstandings.

The best way to do the integration testing is using [black-box testing](https://en.wikipedia.org/wiki/Black-box_testing).
The software component is treated as a black box with inputs and outputs. Test automation developers can use any programming language and testing
framework to develop the tests. Integration tests do not depend on the source code. It can be changed or completely rewritten in a different programming language without the need to modify the integration tests.
Test automation engineers can also start writing integration tests immediately and don't have to wait for the implementation to be ready.

The best way to define integration tests is by using [behavior-driven development](https://en.wikipedia.org/wiki/Behavior-driven_development) (BDD).
and [acceptance test-driven development](https://en.wikipedia.org/wiki/Acceptance_test-driven_development) ATDD. BDD and ATDD encourage teams to
use domain-driven design and concrete examples to formalize a shared understanding of how a software component
should behave. In BDD and ATDD, behavioral specifications are the root of the integration tests. A development team should create behavioral
specifications for each backlog feature. The specifications are the basis for integration tests that also serve as acceptance tests for the feature.
When the team demonstrates a complete feature in a system demo, they should also demonstrate the passing acceptance tests.
This practice will shift the integration testing to the left, meaning writing the integration tests can start early and proceed in parallel with the actual implementation. The development
team writes a failing integration test and only after that implement enough source code to make that test pass.
BDD and ATDD also ensure that it is less likely to forget to test some functionality because the functionality is first formally specified as tests before any implementation begins.

When writing behavioral specifications, happy-path scenarios and the main error scenarios should be covered. The idea is not to test every possible
error that can occur.

One widely used way to write behavioral specifications is the [Gherkin](https://cucumber.io/docs/gherkin/) language. However, it is not the only way. So, we cannot say that BDD equals Gherkin.
You can even write integration tests using a unit testing framework if you prefer. An example of that approach is available in *Steve Freeman*'s and *Nat Pryce*'s book *Growing Object-Oriented Software, Guided by Tests*.
The problem with using a unit testing framework for writing integration tests has been how to test the external dependencies, like a database. Finally, a clean and easy solution to this problem is available.
It is called [testcontainers](https://testcontainers.com/).
Testcontainers allow you to programmatically start and stop containerized external dependencies, like a database in test cases with only one or two lines of code.

You can also write integration tests in two parts: component tests and external integration tests, where the component tests test internal integration (of unit-tested modules with faked external dependencies) and the external integration tests
test integration to external services. The component tests could be written using a unit test framework, and external integration tests could be written by a test automation developer using tools like Behave and Docker Compose.
I have also worked with the approach of creating just a handful of fast-executing component tests to act as smoke tests (focusing on the most essential part of business logic), and all integration tests are written using Behave and Docker Compose.
I can execute those smoke tests together with unit tests to quickly see if something is broken.
The Behave + Docker Compose integration tests take longer to start and run, so they are not executed as often.
Remember that if you want to be able to execute component tests with fake external dependencies, you need to follow the *clean microservice design principle (architecture)* so that you can create fake input/output interface adapter classes used in the tests.

When using the Gherkin language, the behavior of a software component is described as features. There should be a separate
file for each feature. These files have the *.feature* extension. Each feature file
describes one feature and one or more scenarios for that feature. The first scenario should be
the so-called "happy path" scenario, and other possible scenarios should handle additional happy paths, failures, and edge cases
that need to be tested. Remember that you don't have to test every failure and edge case because those were already tested in
the unit testing phase.

When the integration tests are black-box tests, the Gherkin features should be end-to-end testable (from software component input to output);
otherwise, writing integration tests would be challenging. For example, suppose you have a backlog feature for the data exporter
microservice for consuming Avro binary messages from Kafka. In that case, you cannot write an integration test because it is not end-to-end testable.
You can't verify that an Avro binary message was successfully read from Kafka because there is no output in the feature to compare the input with.
If you cannot write integration tests for a backlog feature, then you cannot prove and demonstrate to relevant stakeholders that the feature is completed by executing the integration (i.e., acceptance) tests,
e.g., in a SAFe system demo. For this reason, it is recommended to make all backlog features such that they can be demonstrated with an end-to-end integration (=acceptance) test case.

Let's consider the data exporter microservice. If we start implementing it from scratch, we should define features in such an order that we first build
capability to test end-to-end, for example:

- Export a message from Apache Kafka to Apache Pulsar
- Export single field Avro binary message from Apache Kafka to Apache Pulsar with copy transformation and field type *x*
  - Repeat the above feature for all possible Avro field types (primitive and complex)
- Message filtering
- Type conversion transformations from type x to type y
- Expression transformations
- Output field filtering
- Pulsar export with TLS,
- Kafka export,
- Kafka export with TLS,
- CSV export
- JSON export

The first feature in the above list builds the capability for black-box/E2E integration tests (from software component input to output).
This process is also called creating a *walking skeleton* of the software component first. After you have a walking skeleton, you
can start adding some "flesh" (other features) around the bones.

Below is a simplified example of one feature in a *data-visualization-configuration-service*. We assume that the service
is a REST API. The feature is for creating a new chart. (In a real-life scenario, a chart contains more properties like
the chart's data source and what measure(s) and dimension(s) are shown in the chart, for example). In our simplified example,
a chart contains the following properties: layout id, type, number of x-axis categories shown, and how many rows of chart data
should be fetched from the chart's data source.

{title: "createChart.feature"}
```gherkin
Feature: Create chart
  Creates a new chart

  Scenario: Creates a new chart successfully
    Given chart layout id is "1"
    And chart type is "line"
    And X-axis categories shown count is 10
    And fetchedRowCount is 1000

    When I create a new chart

    Then I should get the chart given above
         with response code 201 "Created"
```

The above example shows how the feature's name is given after the `Feature` keyword. You can add free-form text below the feature's name
to describe the feature in more detail. Next, a scenario is defined after the `Scenario` keyword.
First, the name of the scenario is given. Then comes the steps of the scenario. Each step is defined using
one of the following keywords: `Given`, `When`, `Then`, `And`, and `But`. A scenario should follow this pattern:

- Steps to describe initial context/setup (Given/And steps)
- Steps to describe an event (When step)
- Steps to describe the expected outcome for the event (Then/And steps)

We can add another scenario to the above example:

{title: "createChart.feature"}
```gherkin
Feature: Create chart
  Creates a new chart

  Scenario: Creates a new chart successfully
    Given chart layout id is "1"
    And chart type is "line"
    And X-axis categories shown count is 10
    And fetchedRowCount is 1000

    When I create a new chart

    Then I should get the chart given above
         with status code 201 "Created"

  Scenario: Chart creation fails due to missing mandatory parameter
    When I create a new chart

    Then I should get a response with status code 400 "Bad Request"
    And response body should contain "is mandatory field" entry
        for following fields
      | layoutId                  |
      | fetchedRowCount           |
      | xAxisCategoriesShownCount |
      | type                      |
```

Now we have one feature with two scenarios specified. Next, we shall implement the scenarios. Our
_data-visualization-configuration-service_ is implemented in Java, and we want to implement also the integration
tests in Java. [Cucumber](https://cucumber.io/docs/installation/) has BDD tools for various programming languages.
We will be using the [Cucumber-JVM](https://cucumber.io/docs/installation/java/) library.

We place integration test code into the source code repository's _src/test_ directory.
The feature files are in the _src/test/resources/features_ directory. Feature directories should be organized
into subdirectories in the same way source code is organized into subdirectories: using
domain-driven design and creating subdirectories for subdomains. We can put the above _createChart.feature_
file to the _src/test/resources/features/chart_ directory.

Next, we need to provide an implementation for each step in the scenarios. Let's start with the first scenario. We shall
create a file _TestContext.java_ for the test context and a _CreateChartStepDefs.java_ file for the step definitions:

{title: "TestContext.java"}
```java
public class TestContext {
  public io.restassured.response.Response response;
}
```

{title: "CreateChartStepDefs.java"}
```java
import integrationtests.TestContext;
import com.silensoft.dataviz.configuration.service.chart.Chart;
import io.cucumber.java.en.Given;
import io.cucumber.java.en.Then;
import io.cucumber.java.en.When;
import io.restassured.http.ContentType;
import io.restassured.mapper.ObjectMapperType;

import static io.restassured.RestAssured.given;
import static org.hamcrest.Matchers.equalTo;
import static org.hamcrest.Matchers.greaterThan;


public class CreateChartStepDefs {
  private static final String BASE_URL =
    "http://localhost:8080/data-visualization-configuration-service/";

  private final TestContext testContext;
  private final Chart chart = new Chart();

  public CreateChartStepDefs(final TestContext testContext) {
    this.testContext = testContext;
  }

  @Given("chart layout id is {string}")
  public void setChartLayoutId(final String layoutId) {
    chart.setLayoutId(layoutId);
  }

  @Given("chart type is {string}")
  public void setChartType(final String chartType) {
    chart.setType(chartType);
  }

  @Given("X-axis categories shown count is {int}")
  public void setXAxisCategoriesShownCount(
    final Integer xAxisCategoriesShownCount
  ) {
    chart
      .setxAxisCategoriesShownCount(xAxisCategoriesShownCount);
  }

  @Given("fetchedRowCount is {int}")
  public void setFetchedRowCount(final Integer fetchedRowCount) {
    chart.setFetchedRowCount(fetchedRowCount);
  }

  @When("I create a new chart")
  public void createNewChart() {
    testContext.response = given()
      .contentType("application/json")
      .body(chart, ObjectMapperType.GSON)
      .when()
      .post(Constants.BASE_URL + "charts");
  }

   @Then("I should get the chart given above with status code {int} {string}")
   public void iShouldGetTheChartGivenAbove(
     final int statusCode,
     final String statusCodeName
   ) {
     testContext.response.then()
       .assertThat()
       .statusCode(statusCode)
       .body("id", greaterThan(0))
       .body("layoutId", equalTo(chart.getLayoutId()))
       .body("type", equalTo(chart.getType()))
       .body("xAxisCategoriesShownCount",
             equalTo(chart.getXAxisCategoriesShownCount()))
       .body("fetchedRowCount",
              equalTo(chart.getFetchedRowCount()));
 }
}
```

The above implementation contains a function for each step. Each function is annotated with an annotation
for a specific Gherkin keyword: `@Given`, `@When`, and `@Then`. Note that a step in a scenario
can be templated. For example, the step `Given chart layout id is "1"` is templated and defined in the function
`@Given("chart layout id is {string}")`  `public void setChartLayoutId(final String layoutId)`
where the actual layout id is given as a parameter to the function. You can use this templated step
in different scenarios that can give a different value for the layout id, for example:
`Given chart layout id is "8"`.

The `createNewChart` method uses [REST-assured](https://rest-assured.io/) for submitting an HTTP POST request
to the _data-visualization-configuration-service_. And the `iShouldGetTheChartGivenAbove` function takes
the HTTP POST response and validates the status code and the properties in the response body.

The second scenario is a common failure scenario where you create something with missing parameters. Because
this scenario is common (i.e., we can use the same steps in other features), we put the step definitions
in a file named _CommonStepDefs.java_ in the _common_ subdirectory of the _src/test/java/integrationtests_ directory.

Here are the step definitions:

{title: "CommonStepDefs.java"}
```java
import integrationtests.TestContext;
import io.cucumber.java.en.And;
import io.cucumber.java.en.Then;

import java.util.List;

import static org.hamcrest.Matchers.hasItems;

public class CommonStepDefs {
  private final TestContext testContext;

  public CommonStepDefs(final TestContext testContext) {
    this.testContext = testContext;
  }

  @Then("I should get a response with status code {int} {string}")
  public void iShouldGetAResponseWithResponseCode(
    final int statusCode,
    final String statusCodeName
  ) {
      testContext.response.then()
        .assertThat()
        .statusCode(statusCode);
  }

  @And("response body should contain {string} entry for following fields")
  public void responseBodyShouldContainEntryForFollowingFields(
    final String entry,
    final List<String> fields
  ) {
    testContext.response.then()
      .assertThat()
      .body("", hasItems(fields
                           .stream()
                           .map(field -> field + ' ' + entry)
                           .toArray()));
  }
}
```

Cucumber is available in many other languages in addition to Java. It is available for JavaScript
with the ([Cucumber.js](https://cucumber.io/docs/installation/javascript/)) library and for Python with the ([Behave](https://behave.readthedocs.io/en/stable/)) library.
Integration tests can be written in a different language than the language used for implementation
and unit test code. For example, I am currently developing a microservice in C++. Our team has a
test automation developer working with integration tests using the Gherkin language for feature definitions
and Python and Behave for implementing the steps.

Some frameworks offer their way of creating integration tests. For example, the Python Django and Spring Boot web frameworks offer their own
ways of doing integration tests. There are two reasons why I don't recommend using framework-specific testing tools.
The first reason is that your integration tests are coupled to the framework, and if you decide to reimplement your
microservice using a different language or framework, you also need to reimplement the integration tests.
When you use a generic BDD tool like Behave, your integration tests are not coupled to any
microservice implementation programming language or framework. The second reason is that there is less learning and information
burden for QA/test engineers when they don't have to master multiple framework-specific integration testing tools. If you
use a single tool like Behave in all the microservices in a software system, it will be easier for
QA/test engineers to work with different microservices.

Even though I don't recommend framework specific integration testing tools, I will give you one example with Spring Boot
and the [MockMVC](https://docs.spring.io/spring-framework/reference/testing/spring-mvc-test-framework.html) to test a simple
*sales-item-service* REST API with some CRUD operations:

{title: "SalesItemControllerTests.java"}
```java
import com.fasterxml.jackson.databind.ObjectMapper;
import org.junit.jupiter.api.MethodOrderer;
import org.junit.jupiter.api.Order;
import org.junit.jupiter.api.Test;
import org.junit.jupiter.api.TestMethodOrder;
import org.junit.jupiter.api.extension.ExtendWith;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.boot.test.autoconfigure.web.servlet.AutoConfigureMockMvc;
import org.springframework.boot.test.context.SpringBootTest;
import org.springframework.http.MediaType;
import org.springframework.test.context.junit.jupiter.SpringExtension;
import org.springframework.test.web.servlet.MockMvc;

import static org.springframework.test.web.servlet.request.MockMvcRequestBuilders.delete;
import static org.springframework.test.web.servlet.request.MockMvcRequestBuilders.get;
import static org.springframework.test.web.servlet.request.MockMvcRequestBuilders.post;
import static org.springframework.test.web.servlet.request.MockMvcRequestBuilders.put;
import static org.springframework.test.web.servlet.result.MockMvcResultHandlers.print;
import static org.springframework.test.web.servlet.result.MockMvcResultMatchers.jsonPath;
import static org.springframework.test.web.servlet.result.MockMvcResultMatchers.status;


@SpringBootTest
@AutoConfigureMockMvc
@ExtendWith(SpringExtension.class)
@TestMethodOrder(MethodOrderer.OrderAnnotation.class)
class SalesItemControllerTests {
  private static final long SALES_ITEM_USER_ACCOUNT_ID = 1L;
  private static final String SALES_ITEM_NAME = "Test sales item";
  private static final int SALES_ITEM_PRICE = 10;
  private static final int UPDATED_SALES_ITEM_PRICE = 20;

  private static final String UPDATED_SALES_ITEM_NAME =
    "Updated test sales item";

  @Autowired
  private MockMvc mockMvc;

  @Test
  @Order(1)
  final void testCreateSalesItem() throws Exception {
    // GIVEN
    final var salesItemArg =
      new SalesItemArg(SALES_ITEM_USER_ACCOUNT_ID,
                       SALES_ITEM_NAME,
                       SALES_ITEM_PRICE);

    final var salesItemArgJson =
      new ObjectMapper().writeValueAsString(salesItemArg);

    // WHEN
    mockMvc
      .perform(post(SalesItemController.API_ENDPOINT)
                 .contentType(MediaType.APPLICATION_JSON)
                 .content(salesItemArgJson))
      .andDo(print())
      // THEN
      .andExpect(jsonPath("$.id").value(1))
      .andExpect(jsonPath("$.name").value(SALES_ITEM_NAME))
      .andExpect(jsonPath("$.price").value(SALES_ITEM_PRICE))
      .andExpect(status().isCreated());
  }

  @Test
  @Order(2)
  final void testGetSalesItems() throws Exception {
    // WHEN
    mockMvc
      .perform(get(SalesItemController.API_ENDPOINT))
      .andDo(print())
      // THEN
      .andExpect(jsonPath("$[0].id").value(1))
      .andExpect(jsonPath("$[0].name").value(SALES_ITEM_NAME))
      .andExpect(status().isOk());
  }

  @Test
  @Order(3)
  final void testGetSalesItemById() throws Exception {
    // WHEN
    mockMvc
      .perform(get(SalesItemController.API_ENDPOINT + "/1"))
      .andDo(print())
      // THEN
      .andExpect(jsonPath("$.name").value(SALES_ITEM_NAME))
      .andExpect(status().isOk());
  }

  @Test
  @Order(4)
  final void testGetSalesItemsByUserAccountId() throws Exception {
    // GIVEN
    final var url = SalesItemController.API_ENDPOINT +
                    "?userAccountId=" + SALES_ITEM_USER_ACCOUNT_ID;

    // WHEN
    mockMvc
      .perform(get(url))
      .andDo(print())
      // THEN
      .andExpect(jsonPath("$[0].name").value(SALES_ITEM_NAME))
      .andExpect(status().isOk());
  }

  @Test
  @Order(5)
  final void testUpdateSalesItem() throws Exception {
    // GIVEN
    final var salesItemArg =
      new SalesItemArg(SALES_ITEM_USER_ACCOUNT_ID,
                       UPDATED_SALES_ITEM_NAME,
                       UPDATED_SALES_ITEM_PRICE);

    final var salesItemArgJson =
      new ObjectMapper().writeValueAsString(salesItemArg);

    // WHEN
    mockMvc
      .perform(put(SalesItemController.API_ENDPOINT + "/1")
                 .contentType(MediaType.APPLICATION_JSON)
                 .content(salesItemArgJson))
      .andDo(print());

    // THEN
    mockMvc
      .perform(get(SalesItemController.API_ENDPOINT + "/1"))
      .andDo(print())
      .andExpect(jsonPath("$.name").value(UPDATED_SALES_ITEM_NAME))
      .andExpect(jsonPath("$.price").value(UPDATED_SALES_ITEM_PRICE))
      .andExpect(status().isOk());
  }

  @Test
  @Order(6)
  final void testDeleteSalesItemById() throws Exception {
    // WHEN
    mockMvc
      .perform(delete(SalesItemController.API_ENDPOINT + "/1"))
      .andDo(print());

    // THEN
    mockMvc
      .perform(get(SalesItemController.API_ENDPOINT + "/1"))
      .andDo(print())
      .andExpect(status().isNotFound());
  }

  @Test
  @Order(7)
  final void testDeleteSalesItems() throws Exception {
    // GIVEN
    final var salesItemArg =
      new SalesItemArg(SALES_ITEM_USER_ACCOUNT_ID,
                       SALES_ITEM_NAME,
                       SALES_ITEM_PRICE);

    final var salesItemArgJson =
      new ObjectMapper().writeValueAsString(salesItemArg);

    mockMvc
      .perform(post(SalesItemController.API_ENDPOINT)
                 .contentType(MediaType.APPLICATION_JSON)
                 .content(salesItemArgJson))
      .andDo(print());

    // WHEN
    mockMvc
      .perform(delete(SalesItemController.API_ENDPOINT))
      .andDo(print());

    // THEN
    mockMvc
      .perform(get(SalesItemController.API_ENDPOINT))
      .andDo(print())
      .andExpect(jsonPath("$").isEmpty())
      .andExpect(status().isOk());
  }
}
```

For API microservices, one more alternative to implement integration tests is an API development platform like
[Postman](https://www.postman.com/). Postman can be used to write integration tests using JavaScript.

Suppose we have an API microservice called *sales-item-service* that offers CRUD operations on sales items.
Below is an example API request for creating a new sales item. You can define this in Postman as a new request:

```http
POST http://localhost:3000/sales-item-service/sales-items HTTP/1.1
Content-Type: application/json

{
  "name": "Test sales item",
  "price": 10,
}
```

Here is a Postman test case to validate the response to the above request:

```javascript
pm.test("Status code is 201 Created", function () {
  pm.response.to.have.status(201);
});

const salesItem = pm.response.json();
pm.collectionVariables.set("salesItemId", salesItem.id)

pm.test("Sales item name", function () {
  return pm.expect(salesItem.name).to.eql("Test sales item");
})

pm.test("Sales item price", function () {
  return pm.expect(salesItem.price).to.eql(10);
})
```

In the above test case, the response status code is verified first, and then the `salesItem` object is parsed
from the response body. Value for the variable `salesItemId` is set. This variable will be used
in subsequent test cases. Finally, the values of the `name` and `price` properties are checked.

Next, a new API request could be created in Postman to retrieve the just created sales item:

```http
GET http://localhost:3000/sales-item-service/sales-items/{{salesItemId}} HTTP/1.1
```

We used the value stored earlier in the `salesItemId` variable in the request URL. Variables can be used in the URL
and request body using the following notation: `{{<variable-name>}}`. Let's create a test case for
the above request:

```javascript
pm.test("Status code is 200 OK", function () {
  pm.response.to.have.status(200);
});

const salesItem = pm.response.json();

pm.test("Sales item name", function () {
  return pm.expect(salesItem.name).to.eql("Test sales item");
})

pm.test("Sales item price", function () {
  return pm.expect(salesItem.price).to.eql(10);
})
```

API integration tests written in Postman can be utilized in a CI pipeline. An easy way to do that is to export
a Postman collection to a file that contains all the API requests and related tests. A Postman collection
file is a JSON file. Postman offers a Node.js command-line utility called [Newman](https://learning.postman.com/docs/running-collections/using-newman-cli/installing-running-newman/).
It can run API requests and related tests from an exported Postman collection file.

YYou can run integration tests stored in an exported Postman collection file with the below command in a CI pipeline:

```bash
newman run integrationtests/integrationTestsPostmanCollection.json
```

In the above example, we assume that a file named
*integrationTestsPostmanCollection.json* has been exported from the Postman to the *integrationtests* directory in the source code repository.

#### Web UI Integration Testing

You can also use the Gherkin language when specifying UI features. For example, the [TestCafe](https://testcafe.io/) UI testing tool can be
used with the [gherkin-testcafe](https://www.npmjs.com/package/gherkin-testcafe) tool to make TestCafe support the Gherkin syntax. Let's create a simple UI feature:

```gherkin
Feature: Greet user
  Entering user name and clicking submit button
  displays a greeting for the user

  Scenario: Greet user successfully
    Given there is "John Doe" entered in the input field
    When I press the submit button
    Then I am greeted with text "Hello, John Doe"
```

Next, we can implement the above steps in JavaScript using the TestCafe testing API:

```javascript
// Imports...

// 'Before' hook runs before the first step of each scenario.
// 't' is the TestCafe test controller object
Before('Navigate to application URL', async (t) => {
  // Navigate browser to application URL
  await t.navigateTo('...');
});

Given('there is {string} entered in the input field',
      async (t, [userName]) => {
  // Finds an HTML element with CSS id selector and
  // enters text to it
  await t.typeText('#user-name', userName);
});

When('I press the submit button', async (t) => {
  // Finds an HTML element with CSS id selector and clicks it
  await t.click('#submit-button');
});

When('I am greeted with text {string}', async (t, [greeting]) => {
  // Finds an HTML element with CSS id selector
  // and compares its inner text
  await t.expect(Selector('#greeting').innerText).eql(greeting);
});
```

Another tool similar to TestCafe is [Cypress](https://www.cypress.io/). You can also use Gherkin with Cypress with the
[cypress-cucumber-preprocessor](https://github.com/badeball/cypress-cucumber-preprocessor) library. Then, you can write your UI integration tests like this:

```gherkin
Feature: Visit duckduckgo.com website

  Scenario: Visit duckduckgo.com website successfully
    When I visit duckduckgo.com
    Then I should see the search bar
```

```javascript
import { When, Then } from
  '@badeball/cypress-cucumber-preprocessor';

When("I visit duckduckgo.com", () => {
  cy.visit("https://www.duckduckgo.com");
});

Then("I should see the search bar", () => {
  cy.get("input").should(
    "have.attr",
    "placeholder",
    "Search the web without being tracked"
  );
});
```

#### Setting Up Integration Testing Environment

If you have implemented integration tests as black-box tests outside the microservice, an integration testing environment must be set up before integration tests can be run. An integration testing
environment is where the tested microservice and all its dependencies are running. The easiest way to
set up an integration testing environment for a containerized microservice is to use [Docker Compose](https://docs.docker.com/compose/),
a simple container orchestration tool for a single host. If you have implemented integration tests using a unit testing framework, i.e., inside the microservice,
you don't need to set up a testing environment (so this section does not apply).
You can set up the needed environment (external dependencies) in each integration test separately by using testcontainers, for example.

When a developer needs to debug a failing test in integration tests using Docker Compose, they must attach
the debugger to the software component's running container, which is a bit more work compared to normally debugging source code.
Another possibility is to introduce temporary debug-level logging in the microservice source code. That logging can be removed after the bug is found and corrected.
However, if you cannot debug the microservice in a customer's production environment, it is good practice to have some debug-level logging in the source code to enable troubleshooting of a customer's problems that cannot be reproduced in a local environment.
To enable viewing logs, the integration tests should write logs to the console in case of a test failure. This can be done, e.g., by running the `docker logs` command for the application container.
It should be noted that when the application development has been done using low-level (unit/function-level) BDD (or TDD), the debugging need at the integration testing level is significantly reduced.

Let's create a *docker-compose.yml* file for the *sales-item-service* microservice, which has a MySQL database as a dependency.
The microservice uses the database to store sales items.

{title: "docker-compose.yaml"}
```yaml
version: "3.8"

services:
  wait-for-services-ready:
    image: dokku/wait
  sales-item-service:
    restart: always
    build:
      context: .
    env_file: .env.ci
    ports:
      - "3000:3000"
    depends_on:
      - mysql
  mysql:
    image: mysql:8.0.22
    command: --default-authentication-plugin=mysql_native_password
    restart: always
    cap_add:
      - SYS_NICE
    environment:
      MYSQL_ROOT_PASSWORD: ${MYSQL_PASSWORD}
    ports:
      - "3306:3306"
```

In the above example, we first define a service *wait-for-services-ready*, which we will use later.
Next, we define our microservice, *sales-item-service*. We ask Docker Compose to build a container image
for the *sales-item-service* using the *Dockerfile* in the current directory. Then, we define the environment for
the microservice to be read from an *.env.ci* file. We expose port 3000 and tell that our microservice
depends on the *mysql* service.

Next, we define the `mysql` service. We tell what image to use, give a command-line parameter, and
define the environment and expose a port.

Before we can run the integration tests, we must spin the integration testing environment up using
the `docker-compose up` command:

```bash
docker-compose up --env-file .env.ci --build -d
```

We tell the `docker-compose` command to read environment variables from an *.env.ci* file that should contain
an environment variable named `MYSQL_PASSWORD`. We ask Docker Compose to always build the *sales-item-service* by specifying the
`--build` flag. The `-d` flag tells the `docker-compose` command to run in the background.

Before we can run the integration tests, we must wait until all services defined in the *docker-compose.yml* are
up and running. We use the *wait-for-services-ready* service provided by the
[dokku/wait](https://hub.docker.com/r/dokku/wait) image. We can wait for the services to be ready by issuing the following
command:

```bash
docker-compose
  --env-file .env.ci
  run wait-for-services-ready
  -c mysql:3306,sales-item-service:3000
  -t 600
```

The above command will finish after *mysql* service's port 3306 and *sales-item-service's* port 3000 can be
connected (as specified with the `-c` flag, the `-t` flag specifies a timeout for waiting). After the above command is finished,
you can run the integration tests. In the below example, we run the integration
tests using the *newman* CLI tool:

```bash
newman run integrationtests/integrationTestsPostmanCollection.json
```

If your integration tests are implemented using Behave, you can run them by going to the *integrationtests* directory
and run the `behave` command there. Instead of using the *dokku/wait* image for waiting services to be ready, you can do the waiting in
Behave's `before_all` function. Just make a loop that tries to make a TCP connection to `mysql:3306` and `sales-item-service:3000`.
When both connections succeed, break the loop to start the tests.

After integration tests are completed, you can shut down the integration testing environment:

```bash
docker-compose down
```

If you need other dependencies in your integration testing environment, you can add them to the *docker-compose.yml*
file. If you need to add other microservices with dependencies, you must also add transitive dependencies. For example,
if you needed to add another microservice that uses a PostgreSQL database, you would have to add the other microservice and
PostgreSQL database to the *docker-compose.yml* file as new services.

Let's say the *sales-item-service* depends on Apache Kafka 2.x
that depends on a Zookeeper service. The *sales-item-service's* *docker-compose.yml* looks
like the below after adding Kafka and Zookeeper:

{title: "docker-compose.yaml"}
```yaml
version: "3.8"

services:
  wait-for-services-ready:
    image: dokku/wait
  sales-item-service:
    restart: always
    build:
      context: .
    env_file: .env.ci
    ports:
      - 3000:3000
    depends_on:
      - mysql
      - kafka
  mysql:
    image: mysql:8.0.22
    command: --default-authentication-plugin=mysql_native_password
    restart: always
    cap_add:
      - SYS_NICE
    environment:
      MYSQL_ROOT_PASSWORD: ${MYSQL_PASSWORD}
    ports:
      - "3306:3306"
  zookeeper:
    image: bitnami/zookeeper:3.7
    volumes:
      - "zookeeper_data:/bitnami"
    ports:
      - 2181:2181"
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
  kafka:
    image: bitnami/kafka:2.8.1
    volumes:
      - "kafka_data:/bitnami"
    ports:
      - "9092:9092"
    environment:
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - ALLOW_PLAINTEXT_LISTENER=yes
    depends_on:
      - zookeeper

volumes:
  zookeeper_data:
    driver: local
  kafka_data:
    driver: local
```

### Complete Example with BDD, ATDD, DDD, OOD and TDD

Let's have a complete example using the following design principles: BDD, ATDD, DDD, OOD, and TDD. We will implement a *gossiping bus drivers* application, which some of you might be familiar with. Product management gives us the following user story:

> *Each bus driver drives a bus along a specified circular route. A route consists of one or more bus stops. Bus drivers drive the route and stop at each bus stop. Bus drivers have a set of rumors. At the bus stop, drivers gossip (share rumors) with other drivers stopped at the same bus stop. The application stops when all rumors are shared, or bus drivers have driven for a maximum number of bus stops. Upon exit, the application should inform the user whether all rumors were successfully shared.*

We start with BDD and ATDD and write a formal behavioral specification for the above informal description:

```gherkin
Feature: Gossiping bus drivers

  Scenario: Bus drivers successfully share all rumors
    Given maximum number of bus stops driven is 100
    Given bus drivers with the following routes and rumors
      | Route                                   | Rumors                 |
      | stop-a, stop-b, stop-c                  | rumor1, rumor2         |
      | stop-d, stop-b, stop-e                  | rumor1, rumor3, rumor4 |
      | stop-f, stop-g, stop-h, stop-i, stop-e  | rumor1, rumor5, rumor6 |

    When bus drivers have completed driving
    Then all rumors are successfully shared


  Scenario: Bus drivers fails to share all rumors due to driving maximum number of stops
    Given maximum number of bus stops driven is 5
    Given bus drivers with the following routes and rumors
      | Route                                   | Rumors                 |
      | stop-a, stop-b, stop-c                  | rumor1, rumor2         |
      | stop-d, stop-b, stop-e                  | rumor1, rumor3, rumor4 |
      | stop-f, stop-g, stop-h, stop-i, stop-e  | rumor1, rumor5, rumor6 |

      When bus drivers have completed driving
      Then all rumors are not shared


  Scenario: Bus drivers fail to share all rumors because bus routes never cross
     Given maximum number of bus stops driven is 100
     Given bus drivers with the following bus routes and rumors
       | Route                    | Rumors                 |
       | stop-a, stop-b, stop-c   | rumor1, rumor2         |
       | stop-d, stop-e, stop-f   | rumor1, rumor3, rumor4 |

     When bus drivers have completed driving
     Then all rumors are not shared
```

Next, we add a task to the team's backlog for write integration (acceptance) tests for the above scenarios.
The implementation of the integration tests can start parallel to the actual
implementation of the user story. The user story description provided by product management does not specify how below things should be exactly implemented:

- Upon application exit, inform the user whether all rumors were successfully shared or not
- How bus drivers are supplied to the application

The team should discuss the above two topics and consult the product manager for specific requirements. If there is no feedback from the product manager, the team can
decide how to implement the above things. For example, the team could specify the following:

- If all rumors were successfully shared, the application should exit with an exit code zero; otherwise, exit with a non-zero exit code.
- Application gets the maximum number of driven bus stops as the first command line parameter
- Application gets drivers as subsequent command line parameters
- Each driver is specified with a string in the following format: *<bus route as a comma-separated list of bus stops>;<comma-separated list of rumors>*, e.g., `bus-stop-a,bus-stop-b;rumor-1,rumor-2`

We will use Python and Behave to implement the integration tests. Below are the step implementations for the above Gherkin feature specification:

```python
import subprocess

from behave import given, then, when
from behave.runner import Context


@given(
    'maximum number of bus stops driven is {max_driven_bus_stop_count:d}'
)
def step_impl(context: Context, max_driven_bus_stop_count: int):
    context.max_driven_bus_stop_count = max_driven_bus_stop_count


@given('bus drivers with the following routes and rumors')
def step_impl2(context: Context):
    context.drivers = []

    for driver in context.table:
        bus_route = ','.join(
            bus_stop.strip() for bus_stop in driver['Route'].split(',')
        )

        rumors = ','.join(
            rumor.strip() for rumor in driver['Rumors'].split(',')
        )

        context.drivers.append(f'{bus_route};{rumors}')


@when('bus drivers have completed driving')
def step_impl3(context: Context):
    context.exit_code = subprocess.run(
        [
            'python main.py',
            str(context.max_driven_bus_stop_count),
            ' '.join(context.drivers),
        ]
    ).returncode


@then('all rumors are successfully shared')
def step_impl4(context: Context):
    assert context.exit_code == 0


@then('all rumors are not shared')
def step_impl5(context: Context):
    assert context.exit_code != 0
```

In the `@given` steps, we store information in the `context.` We store driver definitions as strings to the `drivers` attribute of
the `context`. In the `@when` step, the application is launched with command line arguments, and in the `@then` steps,
the exit code of the sub-process is examined to be either 0 (successful sharing of all rumors) or non-zero (sharing of
all rumors failed).

Before starting the implementation using TDD, we must first design our application using DDD and then OOD. Let's continue
with the DDD phase. We can start with event storming and define the *domain events* first:

- The maximum number of driven bus stops is parsed from the command line
- Bus drivers are parsed from the command line
- The bus driver has driven to the next bus stop according to the bus route
- Rumors are shared with the drivers at the bus stop
- Bus drivers have driven until all rumors have been shared

Let's introduce the *actors*, *commands* and *entities* related to the above domain events:

- The maximum number of driven bus stops is parsed from the command line
    - Actor: MaxDrivenStopCountParser, Command: parse

- Bus drivers are parsed from the command line
   - Actor: BusDriversParser, Command: parse, Entities: BusDriver

- Bus drivers have driven until all rumors have been shared
    - Actor: GossipingBusDrivers, Command: drive_until_all_rumors_shared, Entities: Rumor, BusDriver

- The bus driver has driven to the next bus stop according to the bus route
    - Actor: BusDriver, Command: drive_to_next_bus_stop, Entities: BusStop
    - Actor: BusRoute, Command: get_next_bus_stop, Entities: BusStop

- Rumors are shared with the drivers at the bus stop
    - Actor: BusStop, Command: share_rumors_with_drivers, Entities BusDriver, Rumor

Based on the above output of the DDD phase, we can design our classes using OOD. We design actor classes and put public
behavior to them and design the `Rumor` entity class with no behavior.
Below is the class diagram:

![Class diagram](resources/chapter4/images/gbd.drawio.svg)

We should not forget the *program against interfaces principle* (dependency inversion principle). So, let's add interfaces
to the class diagram:

![Class diagram with interfaces](resources/chapter4/images/gbd2.drawio.svg)

Now that we have our design done, we can add the following tasks to the team's backlog:

- GossipingBusDrivers class implementation
- BusDriverImpl class implementation
- CircularBusRoute class implementation
- BusStopImpl class implementation
- Rumor class implementation
- MaxDrivenStopCountParser class implementation
- BusDriversParser class implementation

We already had the integration tests implementation task added to the backlog earlier. There are eight tasks in the backlog, and each
can be implemented (either entirely or at least partially) in parallel. If the team has eight team members, each of them can pick a task for themselves and
proceed in parallel to complete the user story as fast as possible.

We can start implementing classes one public method at a time. Let's start with the most straightforward class, `Rumor,` which does not have behavior at all.
Our implementation will be in Java.

```java
public class Rumor {
}
```

Next, we can implement the `CircularBusRoute` class using TDD. Let's start with the simplest case, which is also a failure scenario:
If the bus route has no bus stops, a `ValueError` with an informative message should be raised. Let's implement a unit test for that scenario:

```java
class CircularBusRouteTests {
  @Test
  void testConstructor_whenNoBusStops() {
    try {
      // WHEN
      final var busStops = List.of();
      new CircularBusRoute(busStops);

      fail("'IllegalArgumentException should have been raised");
    } catch(final IllegalArgumentException exception) {
      # THEN
      assertEquals(
        "'Bus route must have at least one bus stop",
        exception.getMessage()
      );
    }
  }
}
```

Let's implement the constructor of the `CircularBusRoute` class to make the above test pass:

```java
public interface BusStop {
}


public interface BusRoute {
}


public class CircularBusRoute implements BusRoute {
  public CircularBusRoute(final List<BusStop> busStops) {
    if (busStops.isEmpty()) {
      throw IllegalArgumentException("Bus route must have at least one bus stop");
    }
  }
}
```

The unit test for the next scenario is the following: if there is only one stop in the bus route, the `getNextBusStop` method should always
return that bus stop (because the bus route is circular).

```java
public class CircularBusRouteTests {
  @Test
  void testGetNextBusStop_whenOneBusStop() {
    // GIVEN
    final var busStop = new BusStopImpl();
    final var busRoute = new CircularBusRoute(List.of(bus_stop));

    // WHEN
    final var nextBusStop = busRoute.getNextBusStop(busStop);

    // THEN
    assertEquals(busStop, nextBusStop);
}
```

Let's implement the `getNextBusStop` method to make the above test to pass:

```java
public interface BusRoute {
  BusStop getNextBusStop(final BusStop currentBusStop);
}


class CircularBusRoute implements BusRoute:
  private final List<BusStop> busStops;

  public CircularBusRoute(final List<BusStop> busStops) {
    if (busStops.isEmpty()) {
      throw IllegalArgumentException("Bus route must have at least one bus stop");
    }

    // ImmutableList is from Google Guava library
    busStops = new ImmutableList.copyOf(busStops);
  }

  public BusStop getNextBusStop(final BusStop currentBusStop) {
    return bus_stops.get(0);
  }
```

Let's implement a unit test for the following scenario: If the `getNextBusStop` method's argument is a bus stop not belonging
to the bus route, an `IllegalArgumentException` with an informative message should be raised.

```java
class CircularBusRouteTests {
  @Test
  void testGetNextBusStop_whenBusStopDoesNotBelongToRoute() {
     // GIVEN
     final var busStopA = new BusStopImpl();
     final var busRoute = new CircularBusRoute(List.of(bus_stop_a));
     final var busStopB = new BusStopImpl();

    try {
      // WHEN
      busRoute.getNextBusStop(busStopB);

      fail('IllegalArgumentException should have been raised');
    } catch (final IllegalArgumentException exception) {
      // THEN
      assertEquals(
        "Bus stop does not belong to bus route",
        exception.getMessage()
      );
    }
  }
}
```

Let's modify the `getNextBusStop` implementation:

```java
public class CircularBusRoute implements BusRoute {
  // ...

  public BusStop getNextBusStop(final BusStop currentBusStop) {
    if (!busStops.contains(currentBusStop) {
      throw new IllegalArgumentException("Bus stop does not belong to bus route");
    }

    return busStop.get(0);
  }
}
```

Next, we specify two scenarios of how the `getNextBusStop` method should behave when there is more than one stop in the bus route:

> *When a current bus stop is given, the `getNextBusStop` method should return the next bus stop in a list of bus stops for the route.*

Let's write a failing unit test for the above scenario:

```java
class CircularBusRouteTests {
  @Test
  void testGetNextBusStop_whenNextBusStopExists() {
    // GIVEN
    final var busStopA = new BusStopImpl();
    final var busStopB = new BusStopImpl();
    final var busRoute = new CircularBusRoute(List.of(bus_stop_a, bus_stop_b));

    // WHEN
    final var nextBusStop = busRoute.getNextBusStop(busStopA);

    // THEN
    self.assertEqual(busStopB, nextBusStop);
  }
}
```

Let's modify the source code to make the above test pass:

```java
public class CircularBusRoute implements BusRoute {
  // ...

  public BusStop getNextBusStop(final BusStop currentBusStop) {
    if (!busStops.contains(currentBusStop) {
      throw new IllegalArgumentError("Bus stop does not belong to bus route";
    }

    if (busStops.size() == 1) {
      return busStops.get(0);
    }

    final var currBusStopIndex = busStops.indexOf(currentBusStop);
    return busStops.get(currBusStopIndex + 1);
  }
}
```

Let's add a test for the following scenario:

> *If there is no next bus stop in the list of the route's bus stops, the `getNextBusStop` method should return the first bus stop (due to the route being circular).*

```java
class CircularBusRouteTests {
  @Test
  void testGetNextBusStop_whenNoNextBusStop() {
    // GIVEN
    final var busStopA = new BusStopImpl();
    final var busStopB = new BusStopImpl();
    final var busRoute = new CircularBusRoute(List.of(bus_stop_a, bus_stop_b));

    // WHEN
    final var nextBusStop = busRoute.getNextBusStop(busStopB);

    // THEN
    assertEquals(busStopA, nextBusStop);
  }
}
```

Let's make the above test pass:

```java
public class CircularBusRoute implements BusRoute {
  // ...

  public BusStop getNextBusStop(final BusStop currentBusStop) {
    if (!busStops.contains(currentBusStop) {
      throw new IllegalArgumentError("Bus stop does not belong to bus route";
    }

    final busStopCount = busStops.length();

    if (busStopCount == 1) {
      return busStops.get(0);
    }

    final var currBusStopIndex = busStops.indexOf(currentBusStop);
    final var nextBusStopIndex = currBusStopIndex + 1;

    if (nextBusStopIndex <= busStopCount - 1) {
      return bus_stops.get(nextBusStopIndex);
    } else {
      return bus_stops.get(0);
    }
}
```

Our code could benefit from refactoring. With the help of existing
unit tests, we can safely refactor the code to the following:

```java
public class CircularBusRoute implements BusRoute {
  private final List<BusStop> busStops;
  private final int busStopCount;

  public CircularBusRoute(final List<BusStop> busStops) {
    if (busStops.isEmpty()) {
      throw IllegalArgumentException("Bus route must have at least one bus stop");
    }

    busStops = new ImmutableList.copyOf(busStops);
    busStopCount = busStops.size();
  }

  public BusStop getNextBusStop(final BusStop currentBusStop) {
    final currBusStopIndex = busStops.indexOf(currentBusStop);

    if (currBusStopIndex == -1) {
      throw new IllegalArgumentException("Bus stop does not belong to bus route");
    }

    final var nextBusStopIndex = (currBusStopIndex + 1) % busStopCount;
    return bus_stops.get(nextBusStopIndex);
  }
}
```

Next, we shall implement the `BusStopImpl` class and the `shareRumorsWithDrivers` method. Let's start by specifying
what that method should do: After the execution of the method is completed, all the drivers at the bus stop should have the same set of rumors
that is a union of all the rumors that the drivers at the bus stop have.

Let's create a test for the above specification. We will have three bus drivers at a bus stop. Because bus drivers
are implemented in a separate class, we will use mocks for the drivers.

```java
class BusStopImplTests {
  @Mock
  private BusDriver mockBusDriver1;

  @Mock
  private BusDriver mockBusDriver2;

  @Mock
  private BusDriver mockBusDriver3;

  @Test
  void testShareRumorsWithDrivers():
    // GIVEN
    final var busDrivers = List.of(mockBusDriver1, mockBusDriver2, mockBusDriver3);

    final var rumor1 = new Rumor();
    final var rumor2 = new Rumor();
    final var rumor3 = new Rumor();
    final var allRumors = Set.of(rumor1, rumor2, rumor3);

    when(mockBusDriver1.getRumors()).thenReturn(Set.of(rumor1, rumor2));
    when(mockBusDriver2.getRumors()).thenReturn(Set.of(rumor2));
    when(mockBusDriver3.getRumors()).thenReturn(Set.of(rumor2, rumor3));

    final var busStop = new BusStopImpl();
    busStop.add(mockBusDriver1);
    busStop.add(mockBusDriver2);
    busStop.add(mockBusDriver3);

    // WHEN
    busStop.shareRumorsWithDrivers();

    // THEN
    assertRumorsAreSet(allRumors, busDrivers);

  private void assertRumorsAreSet(
    final Set<Rumor> rumors,
    final List<BusDriver> mockBusDrivers
  ) {
    for (final var mockBusDriver : mockBusDrivers) {
      verify(mockBusDriver).setRumors(all_rumors);
    }
}
```

Let's implement the `BusStopImpl` class to make the above test pass:

```java
public interface BusDriver {
  Set<Rumor> getRumors();
  void setRumors(final Set<Rumor> rumors);
}


public interface BusStop {
  void shareRumorsWithDrivers();
  void addBusDriver(final BusDriver busDriver);
}


public class BusStopImpl implement BusStop {
  private final Set<BusDriver> busDrivers = Set.of();

  public void share_rumors_with_drivers() {
    final Set<Rumor> allRumors = new HashSet<>();

    for (final var busDriver : busDrivers) {
      for (final var rumor : busDriver.getRumors()) {
        allRumors.add(rumor);
    }

    for (final var busDriver in busDrivers) {
      busDriver.setRumors(allRumors);
    }
  }

  public void add(final BusDriver busDriver) {
    busDrivers.add(busDriver);
  }
}
```

Let's implement the `BusDriverImpl` class next. As shown above, we needed to add two methods, `getRumors` and `setRumors`, to the `BusDriver` interface.
Let's make a unit test for the `getRumors` method, which returns the rumors given for the driver in the constructor:

```python
class BusDriveImplTests {
  @Mock
  private BusRoute mockBusRoute;

  private final Rumor rumor1 = new Rumor();
  private final Rumor rumor2 = new Rumor();

  @Test
  void testGetRumors() {
    // GIVEN
    final busDriver = new BusDriverImpl(mockBusRoute, Set.of(self.rumor1, self.rumor2));

    // WHEN
    final var rumors = busDriver.getRumors();

    // THEN
    assertEqual(Set.of(self.rumor1, self.rumor2), rumors);
  }
}
```

Now we can implement the `getRumors` method to make the above test pass:

```python
public class BusDriverImpl implements BusDriver {
  private final Set<Rumor> rumors;

  public BusDriverImpl(final Set<Rumor> rumors):
    rumors = new ImmutableSet.copyOf(rumors);

  public Set<Rumor> getRumors() {
    return rumors;
  }
}
```

Let's make a unit test for the `setRumors` method, which should override the rumors given in the constructor:

```java
class BusDriveImplTests {
  // ...

  @Test
  void testSetRumors() {
    // GIVEN
    final var rumor3 = new Rumor();
    final var rumor4 = new Rumor();

    final var busDriver = new BusDriverImpl(mockBusRoute, Set.of(self.rumor1, self.rumor2));

    // WHEN
    busDriver.setRumors(Set.of(rumor3, rumor4));

    // THEN
    assertEquals({rumor3, rumor4}, busDriver.getRumors());
  }
}
```

Now we can implement the `getRumors` method to make the above test pass:

```java
public class BusDriverImpl implements BusDriver {
  // ...

  public void setRumors(final Set<Rumor> rumors) {
    rumors = new ImmutableSet.copyOf(rumors);
  }
}
```

We need to implement the `driveToNextBusSstop` method in the `BusDriverImpl` class: A bus driver
has a current bus stop that is initially the first bus stop of the route. The driver drives to the next bus stop according to its route. When the driver
arrives at the next bus stop, the driver is added to the bus stop. The driver is removed from the current bus stop, which is changed to the next bus stop.

```java
class BusDriverImplTests {
  @Mock
  private BusStop mockBusStopA;

  @Mock
  private BusStop mockBusStopB;

  @Mock
  private BusRoute mockBusRoute;

  @Test
  void testDriveToNextBusStop():
    // GIVEN
    when(mockBusRoute.getFirstBusStop()).thenReturn(mockBusStopA);
    when(mockBusRute.getNextBusStop()).thenReturn(mockBusStopB);
    final var busDriver = new BusDriverImpl(busRouteMock, Set.of());

    // WHEN
    busDriver.driveToNextBusStop();

    // THEN
    verify(mockBusStopA).remove(busDriver);
    verify(mockBusStopB).add(busDriver);
    assertEqual(mockBusStopB, busDriver.getCurrentBusStop());
  }
}
```

Let's implement the `BusDriverImpl` class to make the above test pass. Before that, we must add a test for the new `getFirstBusStop` method in the `BusRouteImpl` class.

```java
class CircularBusRouteTests {
  // ...

  @Test
  void testGetFirstBusStop() {
    // GIVEN
    final var busStopA = new BusStopImpl();
    final var busStopB = new BusStopImpl();
    final var busRoute = new CircularBusRoute(List.of(bus_stop_a, bus_stop_b));

    // WHEN
    final var firstBusStop = busRoute.getFirstBusStop();

    // THEN
    assertEquals(busStopA, firstBusStop);
  }
}
```

Let's modify the `CircularBusRoute` class to make the above test pass:

```java
public interface BusRoute {
  // ...

  BusStop getFirstBusStop();
}


public class CircularBusRoute implements BusRoute {
  // ...

  public BusStop getFirstBus_stop() {
    return bus_stops.get(0);
  }
}
```

Here is the implementation of the `driveToNextBusStop` method in the `BusDriverImpl` class:

```java
public class BusDriverImpl implements BusDriver {
  private final BusRoute busRoute;
  private final Rumors rumors;
  private BusStop currentBusStop;

  public BusDriverImpl(final BusRoute busRoute, final Set<Rumor> rumors) {
    this.busRoute = busRoute;
    rumors = new ImmutableSet.copyOf(rumors);
    currentBusStop = busRoute.getFirstBusSstop();
    currentBusStop.add(self);
  }

  public BusStop driveToNextBusStop() {
    currentBusStop.remove(self);
    currentBusStop = busRoute.getNextBusStop(currentBusStop);
    currentBusStop.add(self);
    return currentBusStop;
  }

  public BusStop getCurrentBusStop() {
    return currentBusStop;
  }

  // ...
}
```

Finally, we should implement the `GossipingBusDrivers` class and its `driveUntilAllRumorsShared`.
Let's write a unit test for the first scenario when all rumors are shared after driving from one bus stop to the next.
The `driveUntilAllRumorsShared` method
makes drivers drive to the next bus stop (the same for both drivers) and share rumors there.

```java
class GossipingBusDriversTests {
  @Mock
  private BusStop mockBusStop;

  @Mock
  private BusDriver mockBusDriver1;

  @Mock
  private BusDriver mockBusDriver2;

  private final Rumor rumor1 = new Rumor();
  private final Rumor rumor2 = new Rumor();
  private final Set<Rumor> allRumors = Set.of(rumor1, rumor2);

  @Test
  void testDriveUntilAllRumorsShared_afterOneStop() {
    // GIVEN
    when(mockBusDriver1.driveToNextBusStop()).thenReturn(mockBusStop);
    when(mockBusDriver2.driveToNextBusStop()).thenReturn(mockBusStop);
    when(mockBusDriver1.getRumors()).thenReturn(Set.of(rumor1), allRumors);
    when(mockBusDriver2.getRumors()).thenReturn(Set.of(rumor2), allRumors);

    final var gossipingBusDrivers = new GossipingBusDrivers(
        List.of(mockBusDriver1, mockBusDriver2)
    );

    // WHEN
    final var allRumorsWereShared =
        gossipingBusDrivers.driveUntilAllRumorsShared();

    // THEN
    assertTrue(allRumorsWereShared);
    verify(mockBusStop).shareRumorsWithDrivers();
  }
}
```

Let's make the above test pass with the below code.

```java
public class GossipingBusDrivers {
  private final List<BusDriver> busDrivers;
  private final Set<Rumor> allRumors;

  public GossipingBusDrivers(final List<BusDriver> busDrivers) {
    busDrivers = new ImmutableList.copyOf(busDrivers);
    allRumors = getAllRumors();
  }

  public boolean driveUntilAllRumorsShared() {
    while(true) {
      for (final var busDriver : busDrivers) {
        final var busStop = busDriver.driveToNextBusStop()
        busStop.shareRumorsWithDrivers();
      }

      if (allRumorsAreShared()) {
        return true;
      }
    }

    return false;
  }

  private Set<Rumor> getAllRumors() {
    final Set<Rumor> allRumors = new HashSet<>();

    for(final var busDriver : busDrivers) {
      for(final var rumors : busDriver.getRumors()) {
          allRumors.addAll(rumors);
      }
    }

    return allRumors;
  }

  private boolean allRumorsAreSshared {
    return busDrivers.stream()
      .allMatch(busDriver -> busDriver.getRumors().equals(allRumors));
  }
```

Let's add a test for a scenario where two bus drivers drive from their starting bus stops to two different bus stops and must continue driving
because all rumors were not shared at the first bus stop. Rumors are shared when drivers continue driving to their next bus stop, which is the same for both drivers.

```java
class GossipingBusDriversTests {
   @Mock
   private BusStop mockBusStop1;

   @Mock
   private BusStop mockBusStop2;

   @Mock
   private BusStop mockBusStop3;

   @Mock
   private BusDriver mockBusDriver1;

   @Mock
   private BusDriver mockBusDriver2;

   @Test
   void testDriveUntilAllRumorsShared_afterTwoStops() {
     // GIVEN
     final var mockBusStops = List.of(mockBusStop1, mockBusStop2, mockBusStop3);

     when(mockBusDriver1.driveToNextBusStop()).thenReturn(
       mockBusStop1,
       mockBusStop3
     );

     when(mockBusDriver2.driveToNextBusStop()).thenReturn(
       mockBusStop2,
       mockBusStop3
     );

    when(mockBusDriver1.getRumors).thenReturn(
      Set.of(rumor1),
      Set.of(rumor1),
      allRumors
    );

    when(mockBusDriver2.getRumors).thenReturn(
      Set.of(rumor2),
      Set.of(rumor2),
      allRumors
    );

    final var gossipingBusDrivers = new GossipingBusDrivers(
        List.of(mockBusDriver1, mockBusDriver2]
    )

    // WHEN
    final var allRumorsWereShared =
        gossipingBusDrivers.driveUntilAllRumorsShared();

    // THEN
    assertTrue(all_rumors_were_shared)

    for (final var mockBusStop : mockBusStops) {
      verify(mockBusStop).shareRumorsWithDrivers();
    }
  }
}
```

Let's modify the implementation:

```java
public class GossipingBusDrivers {
  // ...

  public boolean driveUntilAllRumorsShared() {
    while(true) {
      final var Set<BusStop> busStops = new HashSet<>();

      for (final var busDriver : busDrivers) {
        final var busStop = busDriver.driveToNextBusStop();
        busStops.add(busStop);
      }

      for (final var busStop : busStops) {
        busStop.shareRumorsWithDrivers();
      }

      if (allRumorsAreShared()) {
        return true;
      }
    }

    return false;
  }
}
```

Next, we should implement a test where drivers don't have common bus stops and they have driven the maximum number of bus stops:

```python
class GossipingBusDriversTests {
  // ...

  @Test
  def testDriveUntilAllRumorsShared_whenBusRoutesDontCross(
    // GIVEN
    final var mockBusStops = List.of(mockBusStop1, mockBusStop2);

    when(mockBusDriver1.driveToNextBusStop()).thenReturn(
      mockBusStop1
    );

    when(mockBusDriver2.driveToNextBusStop()).thenReturn(
      mockBusStop2
    );

    when(mockBusDriver1.getRumors).thenReturn(Set.of(self.rumor1));
    when(mockBusDriver2.getRumors).thenReturn(Set.of(self.rumor2));

    final var gossipingBusDrivers = new GossipingBusDrivers(
      List.of(mockBusDriver1, mockBusDriver2)
    );

    final var MAX_DRIVEN_STOP_COUNT = 2;

    // WHEN
    final var allRumorsWereShared =
        gossipingBusDrivers.driveUntilAllRumorsShared(
          MAX_DRIVEN_STOP_COUNT
        );

    // THEN
    assertFalse(allRumorsWereShared);

    for (final var mockBusStop : busStopMocks) {
      verify(mockBusStop, times(2)).shareRumorsWithDrivers();
    }
}
```

Let' modify the implementation to make the above test to pass:

```java
public class GossipingBusDrivers {
  private final List<BusDriver> busDrivers;
  private final Set<Rumor> allRumors;
  private int drivenStopCount = 0;

  public GossipingBusDrivers(final List<BusDriver> busDrivers) {
    busDrivers = new ImmutableList.copyOf(busDrivers);
    allRumors = getAllRumors();
  }

  public boolean driveUntilAllRumorsShared(final int maxDrivenStopCount) {
    while(true) {
      final var Set<BusStop> busStops = new HashSet<>();

      for (final var busDriver : busDrivers) {
        final var busStop = busDriver.driveToNextBusStop();
        busStops.add(busStop);
      }

      drivenStopCount++;

      for (final var busStop : busStops) {
        busStop.shareRumorsWithDrivers();
      }

      if (allRumorsAreShared()) {
        return true;
      } else if (drivenStopCount == maxDrivenStopCount) {
        return false;
      }
    }
  }
}
```

Let's refactor the method slightly so that it is not too long (max 5-9 statements):

```java
public class GossipingBusDrivers {
  // ...

  public boolean driveUntilAllRumorsShared(final int maxDrivenStopCount) {
    while(true) {
      final var busStops = busDriversDriveToNextBusStop();
      drivenStopCount++;
      shareRumors(busStops);

      if (allRumorsAreShared()) {
        return true;
      } else if (drivenStopCount == maxDrivenStopCount) {
        return false;
      }
    }
  }

  private Set<BusStop> busDriversDriveToNextBusStop() {
    final var Set<BusStop> busStops = new HashSet<>();

    for (final var busDriver : busDrivers) {
      final var busStop = busDriver.driveToNextBusStop();
      busStops.add(busStop);
    }

    return busStops;
  }

  private void shareRumors(final Set<BusStop> busStops) {
    for (final var busStop : busStops) {
      busStop.shareRumorsWithDrivers();
    }
  }
}
```

Next, we shall implement the `MaxDrivenStopCountParser` and its 'parse' method. Let's create a failing test:

```java
class MaxDrivenStopCountParserTests {
  @Test
  void testParse_whenSuccessful() {
    // GIVEN
    final var MAX_DRIVEN_STOP_COUNT_STR = "2";

    // WHEN
    maxDrivenStopCount = new MaxDrivenStopCountParserImpl().parse(
        MAX_DRIVEN_STOP_COUNT_STR
    )

    // THEN
    assertEqual(2, maxDrivenStopCount);
  }
}
```

Now we can implement the class to make the above test to pass:

```python
public interface MaxDrivenStopCountParser {
  int parse(final String maxDrivenStopCountStr);
}


public class MaxDrivenStopCountParserImpl extends MaxDrivenStopCountParser {
  public int parse(final String maxDrivenStopCountStr) {
    return Integer.parseInt(maxDrivenStopCountStr);
  }
```

Next, we specify that the `parse` method should throw a `NumberFormatException` if the parsing fails:

```java
class MaxDrivenStopCountParserTests {
  @Test
  void testParse_whenItFails() {
    // GIVEN
    final var MAX_DRIVEN_STOP_COUNT_STR = "invalid";

    // WHEN + THEN
    assertThrows(
      NumberFormatException.class,
      () -> new MaxDrivenStopCountParserImpl().parse(
        MAX_DRIVEN_STOP_COUNT_STR
      )
    );
  }
}
```

The above test will pass without modification to the implementation.

Next, we shall implement the `BusDriversParser` and its 'parse' method. To keep the example shorter, we will skip the failure scenarios,
like when a bus driver
specification does not contain at least one bus stop and one rumor. Let's first create
a failing test for a scenario where we have only one driver with one bus stop and one rumor:

```java
class BusDriversParserImplTests {
  @Test
  void testParse_withOneDriverThatHasOneBusStopAndOneRumor() {
    // GIVEN
    final var busDriverSpec = "bus-stop-a;rumor1";

    // WHEN
    final var busDrivers =
      new BusDriversParserImpl().parse(List.of(busDriverSpec));

    // THEN
    assertEquals(1, busDrivers.size());
    busDriver = busDrivers.get(0);
    assertHasCircularBusRouteWithOneStop(busDriver);
    assertEquals(1, busDriver.getRumors().size());
  }

  private void assertHasCircularBusRouteWithOneStop(
    final BusDriver bus_driver
  ) {
    final var currentbusStop = busDriver.getCurrentBusStop();
    final var nextBusStop = busDriver.driveToNextBusStop();
    assertEquals(currentBusStop, nextBusStop);
  }
}
```

Let's implement the `parse` method to make the above test pass:

```java
public interface BusDriversParser(Protocol) {
  List<BusDriver> parse(final List<String> busDriverSpecs);
}


public class BusDriversParserImpl implements BusDriversParser:
  public List<BusDriver> parse(final List<String> busDriverSpecs) {
    return busDriverSpecs.stream().map(busDriverSpec ->
      new BusDriverImpl(
        new CircularBusRoute(List.of(new BusStopImpl()), Set.of(new Rumor()))
      )
    ).toList();
  }
```

Let's create a test for a scenario where we have multiple drivers with one bus stop and one rumor each; both the bus stops
and rumors for drivers are different:

```java
class BusDriversParserImplTests {
  @Test
  def testParse_withMultipleDriversWithDifferentBusStopAndRumor() {
    // GIVEN
    final var busDriverSpecs = List.of("bus-stop-a;rumor1", "bus-stop-b;rumor2");

    // WHEN
    final var busDrivers = new BusDriversParserImpl().parse(busDriverSpecs);

    // THEN
    assertEqual(2, busDrivers.size());
    assertBusStopsAreDifferent(bus_drivers);
    assertRumorsAreDifferent(bus_drivers);
  }

  private void assertBusStopsAreDifferent(final List<BusDriver> busDrivers) {
    assertNotEqual(
      busDrivers.get(0).getCurrentBusStop(),
      busDrivers.get(1).getCurrentBusStop()
    );
  }

  private void assertRumorsAreDifferent(final List<BusDriver> busDrivers) {
    assertNotEqual(
      busDrivers.get(0).getRunors(),
      busDrivers.get(1).getRumors()
    );
  }
}
```

The above test will pass without modifications to the implementation.

Let's create a test for a scenario where we have multiple drivers with one bus stop and one rumor each; the bus stops
are the same, but rumors are different:

```python
class BusDriversParserImplTests {
  @Test
  def testParse_withMultipleDriversWithCommonBusStop() {
    // GIVEN
    final var busDriverSpecs = List.of("bus-stop-a;rumor1", "bus-stop-a;rumor2");

    // WHEN
    final var busDrivers = new BusDriversParserImpl().parse([bus_driver_specs]);

    // THEN
    assertEquals(2, busDrivers.size());
    assertBusStopsAreSame(bus_drivers);
    assertRumorsAreDifferent(bus_drivers);
  }

  private void assertBusStopsAreSame(final List<BusDriver> busDrivers) {
    assertEquals(
       busDriver.get(0).getCurrentBusStop(),
       busDriver.get(1).getCurrentBusStop()
    )
  }
}
```

Let's modify the implementation to make the above test pass:

```java
public class BusDriversParserImpl implements BusDriversParser {
  private final Map<String, BusStop> busStopNameToBusStop = new HashMap<>();

  public List<BusDriver> parse(final List<String> busDriverSpecs) {
    return busDriverSpecs.stream()
      .map(busDriverSpec -> parseBusDriver(busDriverSpec))
      .toList();
  }

  private BusDriver parseBusDriver(final String busDriverSpec) {
    final var busDriverSpecParts = busDriverSpec.split(";");
    final var busStopName = busDriverSpecParts[0];

    if (busStopNameToBusStop.get(busStopName) == null) {
      busStopNameToBusStop.put(busStopName, new BusStopImpl());
    }

    return mew BusDriverImpl(
        CircularBusRoute(List.of(busStopNameToBusStop.get(busStopName))),
        Set.of(Rumor())
    );
  }
}
```

Let's create a test for a scenario where we have multiple drivers with one rumor each. This time the rumors are the same:

```java
class BusDriversParserImplTests {
  @Test
  void testParse_withMultipleDriversAndCommonRumor() {
    // GIVEN
    final var busDiverSpecs = List.of("bus-stop-a;rumor1", "bus-stop-b;rumor1");

    // WHEN
    final var busDrivers = new BusDriversParserImpl().parse(busDriverSpecs);

    // THEN
    assertRumorsAreSame(busDrivers);
  }

  private void assertRumorsAreSame(final List<BusDriver> busDrivers) {
    assertEquals(
        busDrivers.get(0).getRumors(), busDrivers.get(1).getRumors()
    )
  }
}
```

Let's modify the implementation to make the test pass:

```java
public class BusDriversParserImpl implements BusDriversParser {
  private final Map<String, BusStop> busStopNameToBusStop = new HashMap<>();
  private final Map<String, Rumor> rumorNameToRumor = new HashMap<>();

  public List<BusDriver> parse(final List<String> busDriverSpecs) {
    return busDriverSpecs.stream()
      .map(busDriverSpec -> parseBusDriver(busDriverSpec))
      .toList();
  }

  private BusDriver parseBusDriver(final String busDriverSpec) {
    final var busDriverSpecParts = busDriverSpec.split(";");
    final var busStopName = busDriverSpecParts[0];
    final var rumorName = busDriverSpecParts[1];

    if (busStopNameToBusStop.get(busStopName) == null) {
      busStopNameToBusStop.put(busStopName, new BusStopImpl());
    }

    if (rumorNameToRumor.get(rumorName) == null) {
      rumorNameToRumor.put(rumorName, new Rumor());
    }

    return mew BusDriverImpl(
        CircularBusRoute(List.of(busStopNameToBusStop.get(busStopName))),
        Set.of(rumorNameToRumor.get(rumorName))
    );
  }
}
```

Let's create a test for a scenario where we have multiple drivers with multiple bus stops (the first bus stop is the same):

```java
class BusDriversParserImplTests {
  @Test
  void testParse_withMultipleDriversAndMultipleBusStopsWhereOneIsSame() {
    // GIVEN
    final var busDriverSpecs = List.of(
        "bus-stop-a,bus-stop-b;rumor1",
        "bus-stop-a,bus-stop-c;rumor2",
    );

    // WHEN
    final var busDrivers = new BusDriversParserImpl().parse(busDriverSpecs);

    // THEN
    assertOnlyOneBusStopIsSame(busDrivers);
  }

  private void assertOnlyOneBusStopIsSame(final List<BusDriver> busDrivers) {
    assertEquals(
      busDrivers.get(0).getCurrentBusStop(),
      busDrivers.get(1).getCurrentBusStop()
    );

    assertNotEqual(
      busDrivers.get(0).driveToNextBusStop(),
      busDrivers.get(1).driveToNextBusStop()
    );
  }
}
```

Let's make the above test pass:

```java
public class BusDriversParserImpl implements BusDriversParser {
  // ...

  private BusDriver parseBusDriver(final String busDriverSpec) {
    final var busDriverSpecParts = busDriverSpec.split(";");
    final var busRouteSpec = busDriverSpecParts[0];
    final var rumorName = busDriverSpecParts[1];
    final var busStopNames = busRouteSpec.split(",");
    final List<BusStop> busStops = new ArrayList<>();

    for (final var busStopName : busStopNames) {
      var busStop = busStopNameToBusStop.get(busStopName);

      if (busStop == null) {
        busStop = new BusStopImpl();
        busStopNameToBusStop.put(busStopName, busStop);
      }

      busStops.add(busStop);
    }

    if (rumorNameToRumor.get(rumorName) == null) {
      rumorNameToRumor.put(rumorName, new Rumor());
    }

    return mew BusDriverImpl(
        CircularBusRoute(busStops)),
        Set.of(rumorNameToRumor.get(rumorName))
    );
  }
}
```

Let's create the final test for a scenario where we have multiple drivers with multiple rumors (one of which is the same):

```java
class BusDriversParserImplTests {
  @Test
  void testParse_withMultipleDriversAndMultipleRumorsWhereOneIsSame() {
    // GIVEN
    final var busDriverSpecs = List.of(
        "bus-stop-a;rumor1,rumor2,rumor3",
        "bus-stop-b;rumor1,rumor3",
    );

    // WHEN
    final var busDrivers = new BusDriversParserImpl().parse(busDriverSpecs);

    // THEN
    assertBusStopsAreSame(busDrivers);
    assertRumorsDifferByOne(busDrivers);
  }

  private void assertRumorsDifferByOne(final List<BusDriver> busDrivers) {
    assertEquals(3, busDrivers.get(0).getRumors().size());
    assertEquals(2, busDrivers.get(1).getRumors().size());

    // Sets.difference is from Google Guava library
    final var rumorDifference = Sets.difference(
      busDrivers.get(0).getRumors(),
      busDrivers.get(1).getRumors(),
    );

    assertEqual(1, rumorDifference.size());
  }
}
```

Let's modify the implementation to make the above test pass:

```java
public class BusDriversParserImpl implements BusDriversParser {
  // ...

  private BusDriver parseBusDriver(final String busDriverSpec) {
    final var busDriverSpecParts = busDriverSpec.split(";");
    final var busRouteSpec = busDriverSpecParts[0];
    final var busStopNames = busRouteSpec.split(",");
    final List<BusStop> busStops = new ArrayList<>();

    for (final var busStopName : busStopNames) {
      var busStop = busStopNameToBusStop.get(busStopName);

      if (busStop == null) {
        busStop = new BusStopImpl();
        busStopNameToBusStop.put(busStopName, busStop);
      }

      busStops.add(busStop);
    }

    final var rumorsSpec = busDriverSpecParts[1];
    final var rumorNames = rumorsSpec.split(",");
    final Set<Rumor> rumors = new HashSet<>();

    for (final var rumorName : rumorNames) {
      var rumor = rumorNameToRumor.get(rumorName);

      if (rumor == null) {
        rumor = new Rumor();
        rumorNameToRumor.put(rumorName, rumor);
      }

      rumors.add(rumor);
    }

    return mew BusDriverImpl(CircularBusRoute(busStops), rumors));
  }
}
```

Now that we have implemented all tests, we can refactor the `parse` to the following:

```java
public class BusDriversParserImpl implements BusDriversParser {
  // ...

  private BusDriver parseBusDriver(final String busDriverSpec) {
    final var busDriverSpecParts = busDriverSpec.split(";");
    final var busRouteSpec = busDriverSpecParts[0];
    final var busStopNames = busRouteSpec.split(",");
    final var busStops = parseBusStops(busStopNames);
    final var rumorsSpec = busDriverSpecParts[1];
    final var rumorNames = rumorsSpec.split(",");
    final var rumors = parseRumors(rumorNames);
    return mew BusDriverImpl(CircularBusRoute(busStops), rumors));
  }

  private List<BusStop> parseBusStops(final List<String> busStopNames) {
    final List<BusStop> busStops = new ArrayList<>();

    for (final var busStopName : busStopNames) {
      var busStop = busStopNameToBusStop.get(busStopName);

      if (busStop == null) {
        busStop = new BusStopImpl();
        busStopNameToBusStop.put(busStopName, busStop);
      }

      busStops.add(busStop);
    }

    return busStops;
  }

  private Set<Rumor> parseRumors(final List<String> rumorNames) {
    final Set<Rumor> rumors = new HashSet<>();

    for (final var rumorName : rumorNames) {
      var rumor = rumorNameToRumor.get(rumorName);

      if (rumor == null) {
        rumor = new Rumor();
        rumorNameToRumor.put(rumorName, rumor);
      }

      rumors.add(rumor);
    }
  }
}
```

Finally we need to implement the *main.py*:

```java
public class Main {
  public static void main(String[] args):
    final var maxDrivenStopCount =
      new MaxDrivenStopCountParserImpl().parse(args[1]);

    final var busDriverSpecs = Arrays.stream(args, 2, args.length())
      .toArray(String[]::new);

    final var busDrivers = new BusDriversParserImpl().parse(busDriverSpecs);

    final var allRumorsWereShared = new GossipingBusDrivers(
        busDrivers
    ).driveUntilAllRumorsShared(maxDrivenStopCount)

    System.exit(allRumorsWereShared ? 0 : 1);
}
```

Let's say that product management wants a new feature and puts the following user story on the backlog:
- Support a back-and-forth bus route in addition to the circular bus route.

Because we used the *program against interfaces principle* earlier, we can implement this new feature using the
*open-closed principle* by implementing a new `BackNForthBusRoute` class that implements the `BusRoute` interface.
How about integrating that new class with existing code? Can we also do it by following the *open-closed principle*?
For the most part, yes. As I have mentioned earlier, it is challenging and often impossible to 100% follow
the *open-closed principle* principle. And that is not the goal. However, we should use it as much as we can. In the above code,
the `CircularBusRoute` class was hardcoded in the `BusDriversParserImpl` class. We should create a 
bus route factory that creates either circular or back-and-forth bus routes. Here, we again follow the *open-closed principle*.
Then, we use that factory in the `BusDriversParserImpl` class instead of the hard-coded `CircularBusRoute` constructor.
The `BusDriversParserImpl` class's `parse` method should get the bus route type as a parameter and forward it to the bus route factory.
These last two modifications are not following the *open-closed principle* because we were obliged to modify an existing class.

Similarly, we could later introduce other new features using the *open-closed principle*:

- Quick bus stops where drivers don't have time to share rumors could be implemented in a new `QuickBusStop` class implementing
  the `BusStop` interface
- Forgetful bus drivers that remember others' rumors only, e.g., for a certain number of bus stops, could be implemented with
  a new `ForgetfulBusDriver` class that implements the `BusDriver` interface

As another example, let's consider implementing a simple API containing CRUD operations. Product management has defined the following feature (or epic)
in the backlog:

> *Sales item API creates, reads, updates, and deletes sales items. Sales items are stored in a persistent storage. Each sales item consists of the following attributes: unique ID, name, and price. Sales items can be created, updated, and deleted only by administrators.*

First, the architecture team should provide technical guidance on implementing the backlog feature (or epic). The guidance could be the
following: API should be a REST API, MySQL database should be used as persistent storage, and Keycloak should be used as the IAM system.
Next, the development team should perform threat modeling (facilitated by the product security lead if needed). Threat modelling
should result in additional security-related user stories that should be added to the backlog feature (or epic) and implemented
by the team.

{aside}
Consider implementing the above-specified sales item API in a real-life scenario as two separate APIs for improved security: One public internet-facing API for reading sales items and another private API for administrator-related operations. The private admin API
is accessible only from the company intranet and should not be accessible from the public internet directly, but access from the internet should require a company VPN connection (and proper authorization, of course), for example.
{/aside}

The development team should split the backlog feature into user stories in the [PI planning](https://scaledagileframework.com/pi-planning/) (or before it in the [IP iteration](https://scaledagileframework.com/innovation-and-planning-iteration/)).
The team will come up with the following user stories:

1) As an admin user, I want to create a new sales item in a persistent storage. A sales item contains the following attributes: id, name, and price
2) As a user, I want to read sales items from the persistent storage
3) As an admin user, I want to update a sales item in the persistent storage
4) As an admin user, I want to delete a sales item from the persistent storage

Next, the team should continue by applying BDD to each user story:

User story 1:

{title: "create_sales_item.feature"}
```gherkin
Feature: Create sales item

  Background: Database is available and a clean table exists
    Given database is available
    And table is empty
    And auto increment is reset


  Scenario: Successfully create sales item as admin user
    When the following sales items are created as admin user
       | name         | price |
       | Sales item 1 | 10    |
       | Sales item 2 | 20    |
       | Sales item 3 | 30    |

    Then a response with status code 201 is received
    And the following sales items are received as response
      | name         | price | Id |
      | Sales item 1 | 10    | 1  |
      | Sales item 2 | 20    | 2  |
      | Sales item 3 | 30    | 3  |


  Scenario: Try to create a sales item with invalid data
    When the following sales items are created as admin user
      | name         | price |
      | Sales item 1 | aa    |

    Then a response with status code 400 is received


  Scenario: Try to create new sales when database is unavailable
    Given database is unavailable

    When the following sales items are created as admin user
      | name         | price |
      | Sales item 1 | 10    |

    Then a response with status code 503 is received


  Scenario: Try to create a sales item as normal user
    When the following sales items are created as normal user
      | name         | price |
      | Sales item 1 | 10    |

    Then a response with status code 403 is received


  Scenario: Try to create a sales item unauthenticated
    When the following sales items are created unauthenticated
      | aame         | price |
      | Sales item 1 | aa    |

    Then a response with status code 401 is received
```

User story 2:

{title: "read_sales_item.feature"}
```gherkin
Feature: Read sales items

  Background: Database is available and a clean table exists
    Given database is available
    And table is empty
    And auto increment is reset


  Scenario: Successfully read sales items
    Given the following sales items are created
      | name         | price |
      | Sales item 1 | 10    |
      | Sales item 2 | 20    |
      | Sales item 3 | 30    |

    When sales items are read

    Then a response with status code 200 is received
    And the following sales items are received as response
      | name         | price | Id |
      | Sales item 1 | 10    | 1  |
      | Sales item 2 | 20    | 2  |
      | Sales item 3 | 30    | 3  |


  Scenario: Try to read sales items when database is unavailable
    Given database is unavailable
    When sales items are read
    Then a response with status code 503 is received
```

User story 3:

{title: "update_sales_item.feature"}
```gherkin
Feature: Update sales item

  Background: Database is available and a clean table exists
    Given database is available
    And table is empty
    And auto increment is reset


  Scenario: Successfully update sales item as admin user
    Given the following sales items are created as admin user
      | name         | price |
      | Sales item 1 | 10    |

    When the created sales item is updated to the following as admin user
       | name         | price |
       | Sales item X | 100   |

    Then reading the sales item should provide the following response
      | name         | price |
      | Sales item X | 100   |


  Scenario: Try to update sales item with invalid data
    When sales item with id 1 is updated to the following
      | name         | price |
      | Sales item X | aa    |

    Then a response with status code 400 is received


  Scenario: Sales item update fails because sales item is not found
    When sales item with id 999 is updated to the following
      | name         | price |
      | Sales item X | 100   |

    Then a response with status code 404 is received


  Scenario: Try to update sales item when database is unavailable
    Given database is unavailable

    When sales item with id 1 is updated to the following
      | name         | price  |
      | Sales item X | 100    |

    Then a response with status code 503 is received


  Scenario: Try to update sales as normal user
    When sales item with id 1 is updated to the following as normal user
      | name         | price |
      | Sales item X | aa    |

    Then a response with status code 403 is received


  Scenario: Try to update sales unauthenticated
    When sales item with id 1 is updated to the following unauthenticated
      | name         | price |
      | Sales item X | aa    |

    Then a response with status code 401 is received
```

User story 4:

{title: "delete_sales_item.feature"}
```gherkin
Feature: Delete sales item

  Background: Database is available and clean table exists
    Given database is available
    And table is empty
    And auto increment is reset


  Scenario: Successfully delete sales item as admin user
    Given the following sales items are created as admin user
      | name         | price |
      | Sales item 1 | 10    |

    When the created sales item is deleted as admin user

    Then a response with status code 204 is received
    And reading sales items should provide an empty array as response


  Scenario: Try to delete a non-existent sales with
    When sales item with id 9999 is deleted as admin user
    Then a response with status code 204 is received


  Scenario: Try to delete a sales item when database is unavailable
    Given database is unavailable
    When sales item with id 1 is deleted as admin user
    Then a response with status code 503 is received


  Scenario: Try to delete a sales as normal user
    When sales item with id 1 is deleted as normal user
    Then a response with status code 403 is received


  Scenario: Try to delete a sales unauthenticated
    When sales item with id 1 is deleted unauthenticated
    Then a response with status code 401 is received
```

In the above features, we have considered the main failure scenarios in addition to the happy path scenario. Remember that you
should also test the most common failure scenarios as part of integration testing.

All of the above features include a `Background` section defining steps executed before each scenario.
In the `Background` section, we first ensure the database is available. This is needed because some scenarios make
the database unavailable on purpose. Then, we clean the sales items table in the database. This can be done by connecting
to the database and executing SQL statements like `DELETE FROM sales_items` and `ALTER TABLE sales_items AUTO_INCREMENT=1`.
Here, I am assuming a MySQL database is used.
The database availability
can be toggled by issuing `docker pause` and `docker unpause` commands for the database server container using `subprocess.run` or
`subprocess.Popen`. You should put the step implementations for the `Background` section into a common `background_steps.py` file
because the same `Background` section is used in all features. Before being able to execute integration tests, a *docker-compose.yml*
file must be created. The file should define the microservice itself and its dependencies, the database (MySQL), and the IAM system (Keycloak).

Additionally, we must configure the IAM system before executing the integration tests. This can be done in the *environment.py* file
in the `before_all` function. This function is executed by the `behave` command before executing the integration tests.
Let's assume we are using Keycloak as the IAM system. To configure Keycloak, we can use the Keycloak's Admin REST API.
We need to perform the following (Change the hardcoded version number in the URLs to the newest):

- Create a client ([https://www.keycloak.org/docs-api/23.0.2/rest-api/index.html#_clients](https://www.keycloak.org/docs-api/23.0.2/rest-api/index.html#_clients))
- Create an admin role for the client ([https://www.keycloak.org/docs-api/23.0.2/rest-api/index.html#_roles](https://www.keycloak.org/docs-api/23.0.2/rest-api/index.html#_roles))
- Create an admin user with the admin role ([https://www.keycloak.org/docs-api/23.0.2/rest-api/index.html#_users](https://www.keycloak.org/docs-api/23.0.2/rest-api/index.html#_users))
- Create a regular user without the admin role

An alternative method to test authorization is to use a configurable mock OAuth2 server instead of a real IAM system.

Let's implement the steps for the first scenario of the first user story (creating a sales item) using Python and Behave:

{title: "create_sales_item_steps.py"}
```python
import requests
from behave import then, when
from behave.runner import Context
from environment import SALES_ITEM_API_URL


@when('the following sales items are created {user_type}')
def step_impl(context: Context, user_type: str):
    # Obtain an 'access token' for an admin or normal user based on 'user_type'
    # from the IAM system using OAuth2 Resource Owner Password Flow
    # https://auth0.com/docs/get-started/authentication-and-authorization-flow/resource-owner-password-flow
    # Using this flow, you exchange client_id, client_secret,
    # username and password for an access token

    auth_header = (
        None
        if user_type == 'unauthenticated'
        else f'Bearer {access_token}'
    )

    context.received_sales_items = []
    context.status_codes = []

    for row in context.table:
        sales_item = {key: row[key] for key in row.keys()}

        response = requests.post(
            SALES_ITEM_API_URL,
            sales_item,
            headers={'Authorization': auth_header},
        )

        context.status_codes.append(response.status_code)
        received_sales_item = response.json()
        context.received_sales_items.append(received_sales_item)


@then('a response with status code {expected_status_code:d} is received')
def step_impl(context: Context, expected_status_code: int):
    for received_status_code in context.status_codes:
        assert received_status_code == expected_status_code


@then('the following sales items are received as response')
def step_impl(context: Context):
    recv_and_expected_sales_items = zip(
        context.received_sales_items,
        context.table,
    )

    for (
        recv_sales_item,
        expected_sales_item,
    ) in recv_and_expected_sales_items:
        for key in expected_sales_item.keys():
            assert recv_sales_item[key] == expected_sales_item[key]
```

To make the rest of the scenarios for the first feature pass, we need to add the following:

{title: "common_steps.py"}
```python
@given('database is unavailable')
def step_impl(context: Context):
    # Use subprocess.run or subprocess.Popen to execute
    # 'docker pause' command for the
    # database container
```

We will skip implementing the rest of the steps because they are similar to those we implemented above. The main difference is using different methods of the `request` library: `get` for reading,
`put` for updating and `delete` for deleting.

Next, the development team should perform DDD for the user stories. The first user story is comprised of the following
domain events:

- user is authorized
- sales item is validated
- sales item is created
- sales item is persisted

From the above domain events, we can infer the following:

- Actor: UserAuthorizer, Command: authorize
- Actor: InputSalesItemValidator, Command: validate, Entity: InputSalesItem
- Actor: CreateSalesItemService: Command: create_sales_item, Entities: InputSalesItem, OutputSalesItem
- Actor: CreateSalesItemRepository, Command: save, Entity: SalesItem

Let's conduct DDD for the second user story. We will have the following domain events:

- sales items are read from the persistent store
- sales items are converted to output format

From the above domain events, we can infer the following:

- Actor: ReadSalesItemsRepository, Command: read_all, Entity: SalesItem
- Actor: ReadSalesItemsService: Command: read_sales_items, Entities: OutputSalesItem

The team decides to use the *clean microservice design principle*. Thus, interfaces and controller classes should be added for both user stories. The source code directory should look like the following:

```
sales-item-service
└── src/main/java
    └── sales-items
        ├── common
        │   ├── dtos
        │   │   ├── InputSalesItem.java
        │   │   └── OutputSalesItem.java
        │   ├── entities
        │   │   └── SalesItem.java
        │   └── validator
        │       └── InputSalesItemValidator.java
        ├── create
        │   ├── CreateSalesItemRepository.java
        │   ├── CreateSalesItemService.java
        │   ├── CreateSalesItemServiceImpl.java
        │   ├── RestCreateSalesItemController.java
        │   └── SqlCreateSalesItemRepository.java
        └── read
            ├── ReadSalesItemsRepository.java
            ├── ReadSalesItemsService.java
            ├── ReadSalesItemsServiceImpl.java
            ├── RestReadSalesItemsController.java
            └── SqlReadSalesItemsRepository.java
```

When we continue with DDD for the rest of the user stories, we should eventually have subdirectories for `update` and `delete` features
like those for `create` and `read` features. What we ended up having is called *vertical slice design or architecture*, which
we presented in the second chapter. A different team member can implement each user story, and each team
member has their own subdirectory (a vertical slice) to work on to minimize merge conflicts. Things common to all features are put into
the `common` subdirectory. The development continues with each team member conducting TDD for the public methods in the classes.
This kind of parallel development provides better agility when the whole team (instead of just one team member) can focus on the
same feature (or epic) to complete it as fast as possible, and only after that proceed to the next feature (or epic) on the prioritized backlog.
The implementation details of the classes are not shown here, but the coming chapters about API design and databases show
details how to create a REST controller, DTOs, a service class, and repositories like ORM, SQL, and MongoDB.

As I mentioned earlier, Gherkin is not the only syntax, and Behave is not the only tool to conduct BDD. I want to give you an example where
[Robot Framework](https://robotframework.org/) (RF) is used as an alternative to Gherkin and Behave. You don't have to completely say goodbye to
the Gherkin syntax because the Robot framework also supports a Gherkin-style way to define test cases. The below example is for the first user story we defined earlier:
creating a sales item. The example shows how to test that user story's first two scenarios (or *test cases* in RF vocabulary).
The below *.robot* file resembles the Gherkin *.feature* file. Each test case in the *.robot* file contains a list of steps that are *keywords*, defined in the *.resource* files.
Each keyword defines code (a list of functions) to execute. The functions are implemented in the *libraries*.

{title: "create_sales_item.robot"}
```robot
*** Settings ***
Documentation   Create sales item
Resource        database_setup.resource
Resource        create_sales_item.resource
Test Setup      Database is available and a clean table exists


*** Test Cases ***
Successfully create sales item as admin user
    When a sales item is created as admin user              salesitem1  ${10}
    Then a response with status code is received            ${201}
    And the following sales item is received as response    salesitem1  ${10}


Try to create a sales item with invalid data
    When a sales item is created as admin user              salesitem   aa
    Then a response with status code is received            ${400}
```

{title: "database_setup.resource"}
```robot
*** Settings ***
Documentation
Library         ./DatabaseSetup.py


*** Keywords ***
Database is available and a clean table exists
    Start database if needed
    Clear table
    Reset auto increment
```

{title: "create_sales_item.resource"}
```robot
*** Settings ***
Documentation
Library         ./CreateSalesItem.py


*** Keywords ***
Database is available and a clean table exists
    Start database if needed
    Clear table
    Reset auto increment


A sales item is created as admin user
    [Arguments]    ${name}    ${price}
    Create sales item as admin user     ${name}     ${price}


A response with status code is received
    [Arguments]    ${status_code}
    Verify response     ${status code}


The following sales item is received as response
    [Arguments]    ${name}    ${price}
    Verify received sales item  ${name}     ${price}
```

{title: "DatabaseSetup.py"}
```python
class DatabaseSetup:
    def __init__(self):
        pass

    def start_database_if_needed(self):
        # ...

    def clear_table(self):
        # ...

    def reset_auto_increment(self):
        # ...
```

{title: "CreateSalesItem.py"}
```python
import requests
from environment import SALES_ITEM_API_URL


class CreateSalesItem:
    def __init__(self):
        self.response = None

    def create_sales_item_as_admin_user(self, name: str, price: str | int):
        # Obtain admin user access token from the IAM system
        access_token = ''
        sales_item = {'name': name, 'price': price}

        self.response = requests.post(
            SALES_ITEM_API_URL,
            sales_item,
            headers={'Authorization': f'Bearer {access_token}'},
        )

    def verify_response(self, status_code: int):
        assert self.response.status_code == status_code

    def verify_received_sales_item(self, name: str, price: int):
        sales_item = self.response.json()
        assert sales_item['name'] == name
        assert sales_item['price'] == price
```

### End-to-End (E2E) Testing Principle

> ***End-to-end (E2E) testing should test a complete software system (i.e., the integration of microservices) so that each test case is end-to-end (from the software system's south-bound interface to the software system's north-bound interface).***

As the name says, in E2E testing, test cases should be end-to-end. They should test that each microservice is deployed correctly
to the test environment and connected to its dependent services. The idea of E2E test cases
is not to test details of microservices' functionality because that has already been tested in unit and
software component integration testing. This is why there should be only a handful of E2E test cases.

Let's consider a telecom network analytics software system that consists of the following applications:

- Data ingestion
- Data correlation
- Data aggregation
- Data exporter
- Data visualization

![Telecom Network Analytics Software System](resources/chapter4/images/06-02.png)

The southbound interface of the software system is the data ingestion
application. The data visualization application provides a web client as a northbound interface. The data exporter application also provides another northbound interface for the software system.

E2E tests are designed and implemented similarly to software component integration tests.
We are just integrating different things (microservices instead of functions). E2E testing starts with the specification of E2E features.
These features can be specified using, for example, the Gherkin language and put in *.feature* files.

You can start specifying and implementing E2E tests right after the architectural design for the software system is completed.
This way, you can shift the implementation of the E2E test to the left and speed up the development phase.
You should not start specifying and implementing E2E only after the whole software system is implemented.

Our example software system should have at least two happy-path E2E features. One is for testing the data flow from data ingestion
to data visualization, and another feature is to test the data flow from data ingestion to data export. Below is the specification
of the first E2E feature:

```gherkin
Feature: Visualize ingested, correlated and
         aggregated data in web UI's dashboard's charts

  Scenario: Data ingested, correlated and aggregated is visualized
            successfully in web UI's dashboard's charts

    Given southbound interface simulator is configured
          to send input messages that contain data...
    And data ingester is configured to read the input messages
        from the southbound interface
    And data correlator is configured to correlate
        the input messages
    And data aggregator is configured to calculate
        the following counters...
    And data visualization is configured with a dashboard containing
        the following charts viewing the following counters/KPIs...

    When southbound interface simulator sends the input messages
    And data aggregation period is waited
    And data content of each data visualization web UI's dasboard's
        chart is exported to a CSV file

    Then CSV export file of the first chart should
         contain the following values...
    And the CSV export file of the second chart should
         contain the following values...
    .
    .
    .
    And CSV export file of the last chart should
         contain the following values...
```

Then, we can create another feature that tests the E2E path from data ingestion to data export:

```gherkin
Feature: Export ingested, correlated and transformed data
         to Apache Pulsar

  Scenario: Data ingested, correlated and transformed is
            successfully exported to Apache Pulsar
    Given southbound interface simulator is configured to send
          input messages that contain data...
    And data ingester is configured to read the input messages
        from the southbound interface
    And data correlator is configured to correlate
       the input messages
    And data exporter is configured to export messages with
        the following transformations to Apache Pulsar...

    When southbound interface simulator sends the input messages
    And messages from Apache Pulsar are consumed

    Then first message from Apache Pulsar should have
         the following fields with following values...
    And second message from Apache Pulsar should have
        the following fields with following values...
    .
    .
    .
    And last message from Apache Pulsar should have
        the following fields with following values...
```

Next, E2E tests can be implemented. Any programming language and tool compatible with the Gherkin
syntax, like Behave with Python, can be used. If the QA/test engineers in the development teams already use Behave for integration tests, it would be a natural choice to use Behave also for the E2E tests.

The software system we want to E2E test must reside in a production-like test environment.
Usually, E2E testing is done in both the CI and the staging environment(s). Before running the E2E tests, the software needs
to be deployed to the test environment.

If we consider the first feature above, implementing the E2E test steps
can be done so that the steps in the `Given` part of the scenario are implemented using an externalized configuration. If our software
system runs in a Kubernetes cluster, we can configure the microservices by creating
the needed ConfigMaps. The southbound interface simulator can be controlled by launching a Kubernetes Job
or, if it is a microservice with an API, commanding it via its API.
After waiting for all the ingested data to be aggregated and visualized, the E2E test can launch a test tool suited for web UI testing
(like TestCafe) to export chart data from the web UI to downloaded files. Then, the E2E test compares the content
of those files with expected values.

You can run E2E tests in a CI environment after each commit to the main branch (i.e., after the microservice CI
pipeline run has finished) to test that the new commit did not break any E2E tests. Alternatively, if the E2E tests are complex and
take a long time to execute, you can run the E2E tests in the CI environment on a schedule, like hourly, but at least nightly.

You can run E2E tests in a staging environment using a separate pipeline in your CI/CD tool.

## Non-Functional Testing Principle

> ***In addition to multi-level functional testing, non-functional testing, as automated as possible, should be performed for a software system.***

The most important categories of non-functional testing are the following:

- Performance testing
- Data volume testing
- Stability testing
- Reliability testing
- Stress and scalability testing
- Security testing

### Performance Testing

The goal of performance testing is to verify the performance of a software system. This verification can be
done on different levels and in different ways, for example, by verifying each performance-critical microservice separately.

To measure the performance of a microservice, performance tests can be created to benchmark the busy loop or
loops in the microservice. If we take the data exporter microservice as an example, there is a busy loop that performs message decoding, transformation, and encoding.
We can create a performance test using a unit testing framework for this busy loop. The performance test
should execute the code in the busy loop for a certain number of rounds (like 50,000 times) and verify that the execution
duration does not exceed a specified threshold value obtained on the first run of
the performance test. The performance test aims to verify that performance remains at the same
level as it has been or is better. If the performance has worsened, the test won't pass. In this way, you cannot accidentally introduce
changes that negatively affect the performance without noticing it.
This same performance test can also be used to measure the effects of optimizations. First, you write
code for the busy loop without optimizations, measure the performance, and use that measure as a reference point. After
that, you introduce optimizations individually and see if and how they affect the performance.

The performance test's execution time threshold value must be separately specified for each
developer's computer. This can be achieved by having a different threshold value for each
computer hostname running the test.

You can also run the performance test in a CI pipeline, but you must first measure the performance in that
pipeline and set the threshold value accordingly. Also, the computing instances running CI pipelines must be
homogeneous. Otherwise, you will get different results on different CI pipeline runs.

The above-described performance test was for a unit (one public function without mocking), but performance testing can also be done
on the software component level. This is useful if the software component has external dependencies whose performance
needs to be measured. In the telecom network analytics software system, we could introduce a performance test for the *data-ingester-service*
to measure how long it takes to process a certain number of messages, like one million. After executing that test, we have a performance measurement available for reference. When we try to optimize
the microservice, we can measure the performance of the optimized microservice and compare it to the reference value.
If we make a change known to worsen the performance, we have a reference value to which we can compare
the deteriorated performance and see if it is acceptable. And, of course, this reference value will prevent a developer
from accidentally making a change that negatively impacts the microservice's performance.

You can also measure end-to-end performance. In the telecom network analytics software system, we could measure the performance
from data ingestion to data export, for example.

### Data Volume Testing

The goal of data volume testing is to measure the performance of a database by comparing an empty database to
a database with a sizeable amount of data stored in it. With data volume testing, we can measure the impact of data volume
on a software component's performance. Usually, an empty database has better performance than a database
containing a high amount of data. This depends on the database and how it scales with large amounts of data.

### Stability Testing

Stability testing aims to verify that a software system remains stable when running for an extended period
of time under load. This testing is also called *load*, *endurance*, or *soak* testing. The term "extended period" can be interpreted
differently depending on the software system. But this period should be many hours, preferably several
days, even up to one week. Stability testing aims to discover problems like sporadic bugs or memory leaks.
A sporadic bug is a bug that occurs only in certain conditions or at irregular intervals. A memory leak can be so small
that the software component must run for tens of hours after it becomes clearly visible. It is recommended that when running
the software system for a longer period, the induced load to the software system follows a natural pattern
(mimicking the production load), meaning that there are peaks and lows in the load.

Stability testing can be partly automated. The load to the system can be generated using tools created for that purpose,
like Apache JMeter, for example. Each software component can measure crash count, and those statistics can be analyzed
automatically or manually after the stability testing is completed. Analyzing memory leaks can be trickier, but crashes due
to out-of-memory and situations where a software component is scaling out due to lack of memory should be registered.

### Reliability Testing

Reliability testing aims to verify that a software system runs reliably. The software system is reliable when
it is resilient to failures and recovers from failures automatically as fast as possible. Reliability testing is also called availability,
*recovery*, or *resilience* testing.

Reliability testing involves chaos engineering to induce various failures in the software system's
environment. It should also ensure the software system stays available and can automatically recover from failures.

Suppose you have a software system deployed to a Kubernetes cluster. You can make stateless services highly available
by configuring them to run multiple pods. If one node goes down,
it will terminate one of the pods (never allow scheduling all the microservice pods on the same node). However, the service remains available and usable because
at least one other pod is still running on a different node. Also, after a short while, when Kubernetes notices that
one pod is missing, it will create a new pod on a new node, and
there will be the original number of pods running, and the recovery from the node down is successful.

Many parts of the reliability testing can be automated. You can use ready-made chaos engineering tools or create and use
your tools. Use a tool to induce failures in the environment. Then verify based on the service's business-criticality that the service remains highly available or swiftly recovers from a failure.

Considering the telecom network analytics software system, we could introduce a test case where the message broker (e.g., Kafka) is shut down. Then, we expect alerts to be triggered
after a while by the microservices that try to use the unavailable message broker. After the message broker is restarted, the alerts should
cancel automatically, and the microservices should continue normal operation.

### Stress and Scalability Testing

Stress testing aims to verify that a software system runs under high load. In stress testing, the
software system is exposed to a load higher than the system's usual load. The software system
should be designed as scalable, which means that the software system should also run under high load. Thus,
stress testing should test the scalability of the software system and see that microservices scale out when needed.
At the end of stress testing, the load is returned back to the normal level, and scaling in the microservices can also be verified.

You can specify a HorizontalPodAutoscaler (HPA) for a Kubernetes Deployment. In the HPA manifest, you must specify the minimum
number of replicas. This should be at least two if you want to make your microservice highly
available. You also need to specify the maximum number of replicas so that your microservice does not consume too many
computing resources in some weird failure case. You can make the horizontal scaling (in and out) happen
by specifying a target utilization rate for CPU and memory. Below is an example Helm chart template for defining a Kubernetes
HPA:

```yaml
{{- if eq .Values.env "production" }}
apiVersion: autoscaling/v2beta1
kind: HorizontalPodAutoscaler
metadata:
  name: {{ include "microservice.fullname" . }}
  labels:
    {{- include "microservice.labels" . | nindent 4 }}
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: {{ include "microservice.fullname" . }}
  minReplicas: {{ .Values.hpa.minReplicas }}
  maxReplicas: {{ .Values.hpa.maxReplicas }}
  metrics:
    {{- if .Values.hpa.targetCPUUtilizationPercentage }}
    - type: Resource
      resource:
        name: cpu
        targetAverageUtilization: {{ .Values.hpa.targetCPUUtilizationPercentage }}
    {{- end }}
    {{- if .Values.hpa.targetMemoryUtilizationPercentage }}
    - type: Resource
      resource:
        name: memory
        targetAverageUtilization: {{ .Values.hpa.targetMemoryUtilizationPercentage }}
    {{- end }}
{{- end }}
```

It is also possible to specify the autoscaling to use an external metric. An external metric could be Kafka consumer lag, for instance.
If the Kafka consumer lag grows too high, the HPA can scale the microservice out for more processing power for the Kafka consumer group.
When the Kafka consumer lag decreases below a defined threshold, HPA can scale in the microservice to reduce the number of pods.

### Security Testing

Security testing aims to verify that a software system is secure and does not contain security vulnerabilities.
One part of security testing is performing vulnerability scans of the software artifacts. Typically, this means
scanning the microservice containers using an automatic vulnerability scanning tool. Another essential part of security
testing is penetration testing, which simulates attacks by a malicious party. Penetration testing can be performed
using an automated tool like [ZAP](https://www.zaproxy.org/) or [Burp Suite](https://portswigger.net/).

Penetration testing tools try to find security vulnerabilities in the following categories:

- Cross-site scripting
- SQL injection
- Path disclosure
- Denial of service
- Code execution
- Memory corruption
- Cross-site request forgery (CSRF)
- Information disclosure
- Local/remote file inclusion

A complete list of possible security vulnerabilities found by the ZAP tool can be found at [ZAP Alert Details](https://www.zaproxy.org/docs/alerts/).

### Other Non-Functional Testing

Other non-functional testing is documentation testing and several UI-related non-functional testing, including
accessibility (A11Y) testing, visual testing, usability testing, and localization and internationalization (I18N) testing.

#### Visual Testing

I want to bring up visual testing here because it is important. [Backstop.js](https://github.com/garris/BackstopJS) and [cypress-plugin-snapshots](https://github.com/meinaart/cypress-plugin-snapshots) test web UI's HTML and CSS
using snapshot testing. Snapshots are screenshots taken of the web UI. Snapshots are compared to ensure that the visual look of the
application stays the same and no bugs are introduced with HTML or CSS changes.
